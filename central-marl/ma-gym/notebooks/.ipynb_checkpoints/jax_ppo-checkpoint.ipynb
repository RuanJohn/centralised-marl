{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1dcd3561-ac42-47ba-9cff-cac22b2be978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda env: centralised-agents\n",
    "\n",
    "import gym \n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "import numpy as np\n",
    "import haiku as hk\n",
    "from copy import deepcopy\n",
    "from jax import jit, grad, vmap, pmap, random\n",
    "import optax\n",
    "import chex\n",
    "import rlax\n",
    "from typing import Tuple\n",
    "import distrax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fa18407-3ddb-42bd-90ca-c12ce615de37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# global hypterparameters\n",
    "HORIZON = 100\n",
    "NUM_EPOCHS = 2\n",
    "NUM_MINIBATCHES = 2\n",
    "SEED = 2022\n",
    "LEARNING_RATE = 5e-4\n",
    "DISCOUNT = 0.99\n",
    "GAE_LAMBDA = 0.95 \n",
    "CLIPPING_EPSILON = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5382ac13-7a8b-49f2-863b-0b61fa97c9e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "# first state\n",
    "random_state = random.PRNGKey(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05e9a311-fe12-446e-b699-24d27dd31483",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CentralControllerWrapper: \n",
    "    \n",
    "    def __init__(self, ma_env):\n",
    "        \n",
    "        self.env = ma_env \n",
    "        self.num_agents = ma_env.n_agents \n",
    "        self.action_mapping = self.enumerate_agent_actions()\n",
    "        self.action_space = len(self.action_mapping)\n",
    "        self.observation_space = np.sum([len(i) for i in ma_env.reset()])\n",
    "        \n",
    "    def reset(self, ):\n",
    "        \n",
    "        obs_n = self.env.reset()\n",
    "        joint_obs = self.create_joint_obs(obs_n)\n",
    "        \n",
    "        return joint_obs\n",
    "    \n",
    "    def step(self, joint_action): \n",
    "        \n",
    "        action = self.action_mapping[joint_action]\n",
    "        obs_n, reward_n, done_n, info = self.env.step(action)\n",
    "        \n",
    "        joint_obs = self.create_joint_obs(obs_n)\n",
    "        team_reward = jnp.sum(jnp.array(reward_n))\n",
    "        team_done = all(done_n)\n",
    "        \n",
    "        return joint_obs, team_reward, team_done, info\n",
    "    \n",
    "    def random_action(self,): \n",
    "        \n",
    "        action = np.random.randint(low = 0, high = self.action_space)\n",
    "        return action \n",
    "    \n",
    "    def enumerate_agent_actions(self, ):\n",
    "        \n",
    "        agent_actions = [np.arange(self.env.action_space[i].n) for i in range(len(self.env.action_space))]\n",
    "        enumerated_actions = np.array(np.meshgrid(*agent_actions)).T.reshape(-1,self.num_agents)\n",
    "        action_mapping = {int(i): list(action) for i, action in enumerate(enumerated_actions)}\n",
    "        return action_mapping\n",
    "    \n",
    "    def create_joint_obs(self, env_obs):\n",
    "        \n",
    "        array_obs = np.array(env_obs)\n",
    "        joint_obs = np.concatenate(array_obs, axis = -1)\n",
    "        \n",
    "        return joint_obs\n",
    "    \n",
    "    def unwrapped_env(self):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e62e6420-096e-462c-84ef-7f2b858bf7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For debugging\n",
    "\n",
    "class NormalGymWrapper: \n",
    "    \n",
    "    def __init__(self, env):\n",
    "        \n",
    "        self.env = env  \n",
    "        self.action_space = env.action_space.n\n",
    "        self.observation_space = env.observation_space.shape[0]\n",
    "        \n",
    "    def reset(self, ):\n",
    "        \n",
    "        obs = self.env.reset()\n",
    "        joint_obs = np.array(obs)\n",
    "        \n",
    "        return joint_obs\n",
    "    \n",
    "    def step(self, action): \n",
    "        \n",
    "        obs, reward, done, info = self.env.step(action)\n",
    "        \n",
    "        return np.array(obs), jnp.array(reward), done, info\n",
    "    \n",
    "    def unwrapped_env(self):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a2ecc2d-11dd-48fa-a000-46f011fbff2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Getting environment details \n",
    "# env = gym.make('ma_gym:Switch2-v0')\n",
    "# env = CentralControllerWrapper(env)\n",
    "\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "env = NormalGymWrapper(env)\n",
    "num_actions     = env.action_space\n",
    "observation_dim = env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76347b0b-bb5d-460d-a426-4afdb816a617",
   "metadata": {},
   "outputs": [],
   "source": [
    "@chex.dataclass\n",
    "class BufferState: \n",
    "    states: jnp.ndarray\n",
    "    actions: jnp.ndarray \n",
    "    rewards: jnp.ndarray \n",
    "    dones: jnp.ndarray\n",
    "    log_probs: jnp.ndarray\n",
    "    values: jnp.ndarray\n",
    "    advantages: jnp.ndarray\n",
    "    returns: jnp.ndarray\n",
    "    counter: jnp.int32 \n",
    "    key: chex.PRNGKey\n",
    "    buffer_size: jnp.int32\n",
    "    gae_lambda: jnp.float32 \n",
    "    discount: jnp.float32\n",
    "    num_minibatches: jnp.int32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b030c215-c6a9-4452-914b-7589760aa794",
   "metadata": {},
   "outputs": [],
   "source": [
    "@chex.dataclass\n",
    "class MiniBatch:\n",
    "    states: jnp.ndarray\n",
    "    actions: jnp.ndarray \n",
    "    rewards: jnp.ndarray \n",
    "    dones: jnp.ndarray\n",
    "    log_probs: jnp.ndarray\n",
    "    values: jnp.ndarray\n",
    "    advantages: jnp.ndarray\n",
    "    returns: jnp.ndarray\n",
    "    key: chex.PRNGKey\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b23fc9f-85d7-4ced-b52b-19ef92071d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Very basic jax replay buffer\n",
    "\n",
    "class JaxTrajectoryBuffer: \n",
    "    \n",
    "    def create_buffer(\n",
    "        self, \n",
    "        buffer_size: int, \n",
    "        observation_dim: int,\n",
    "        gae_lambda: float, \n",
    "        discount: float, \n",
    "        num_minibatches: int,\n",
    "        buffer_key: chex.PRNGKey = random.PRNGKey(0),\n",
    "    ) -> BufferState:\n",
    "        \n",
    "        state_buffer = jnp.empty((buffer_size + 1, observation_dim), dtype=jnp.float32)\n",
    "        action_buffer = jnp.empty(buffer_size + 1, dtype=jnp.int32)\n",
    "        reward_buffer = jnp.empty(buffer_size + 1, dtype=jnp.float32)\n",
    "        done_buffer = jnp.empty(buffer_size + 1, dtype=bool) \n",
    "        log_probs_buffer = jnp.empty(buffer_size + 1, dtype=jnp.float32)\n",
    "        values_buffer = jnp.empty(buffer_size + 1, dtype=jnp.float32)\n",
    "        advantages_buffer = jnp.empty(buffer_size, dtype=jnp.float32)\n",
    "        returns_buffer = jnp.empty(buffer_size, dtype=jnp.float32)\n",
    "        \n",
    "        buffer_state = BufferState(\n",
    "            states = state_buffer, \n",
    "            actions = action_buffer, \n",
    "            rewards = reward_buffer, \n",
    "            dones = done_buffer, \n",
    "            log_probs=log_probs_buffer, \n",
    "            values=values_buffer,\n",
    "            advantages=advantages_buffer,\n",
    "            returns=returns_buffer, \n",
    "            counter = jnp.array(0, dtype=jnp.int32), \n",
    "            key = buffer_key, \n",
    "            buffer_size = jnp.array(buffer_size, dtype=jnp.int32), \n",
    "            gae_lambda = jnp.array(gae_lambda, dtype=jnp.float32), \n",
    "            discount = jnp.array(discount, dtype=jnp.float32),\n",
    "            num_minibatches = jnp.array(num_minibatches, dtype=jnp.int32),\n",
    "        )\n",
    "        \n",
    "        return buffer_state\n",
    "    \n",
    "    def add(\n",
    "        self,\n",
    "        buffer_state, \n",
    "        state, \n",
    "        action, \n",
    "        reward, \n",
    "        done, \n",
    "        log_prob,\n",
    "        value, \n",
    "    ) -> BufferState:\n",
    "        \n",
    "        index = buffer_state.counter\n",
    "        #x = x.at[idx].set(y)\n",
    "        buffer_state.states = buffer_state.states.at[index].set(state)\n",
    "        buffer_state.actions = buffer_state.actions.at[index].set(action)\n",
    "        buffer_state.rewards = buffer_state.rewards.at[index].set(reward)\n",
    "        buffer_state.dones = buffer_state.dones.at[index].set(done)\n",
    "        buffer_state.log_probs = buffer_state.log_probs.at[index].set(log_prob)\n",
    "        buffer_state.values = buffer_state.values.at[index].set(value)\n",
    "        \n",
    "        buffer_state.counter += 1\n",
    "        \n",
    "        return buffer_state\n",
    "    \n",
    "    def compute_advantages(\n",
    "        self, \n",
    "        buffer_state, \n",
    "    ) -> BufferState:\n",
    "        \n",
    "        # Returns array of length [0:k-1]\n",
    "        \n",
    "        advantages = rlax.truncated_generalized_advantage_estimation(\n",
    "            r_t = buffer_state.rewards[1: ], \n",
    "            discount_t = (1 - buffer_state.dones[1: ]) * buffer_state.discount, \n",
    "            lambda_ = buffer_state.gae_lambda, \n",
    "            values = buffer_state.values,\n",
    "        )\n",
    "        \n",
    "        # Don't have to add a zero just make other arrays shorter during training. \n",
    "        \n",
    "        buffer_state.advantages = advantages\n",
    "        \n",
    "        # can now get the returns by saying \n",
    "        # returns = advantages - values[0:k-1]  -> essentially Adv - V_{t}\n",
    "        returns = advantages + buffer_state.values[:-1]\n",
    "        buffer_state.returns = returns \n",
    "        \n",
    "        return buffer_state\n",
    "        \n",
    "        \n",
    "    def get_epoch_indices(\n",
    "        self, \n",
    "        buffer_state, \n",
    "    ) -> jnp.ndarray:\n",
    "        \n",
    "        key, sample_key = random.split(buffer_state.key)\n",
    "        \n",
    "        shuffled_idx = random.permutation(sample_key, buffer_state.buffer_size)\n",
    "        \n",
    "        buffer_state.key = key \n",
    "        \n",
    "        # Split indices into minibatches \n",
    "        # right now the type here is a list. might have to be cast into \n",
    "        # something else. \n",
    "        minibatch_idxs = jnp.split(shuffled_idx, buffer_state.num_minibatches)\n",
    "        \n",
    "        return buffer_state, minibatch_idxs\n",
    "    \n",
    "    def should_train(\n",
    "        self, \n",
    "        buffer_state \n",
    "    ) -> bool:\n",
    "        \n",
    "        return jnp.equal(buffer_state.counter, buffer_state.buffer_size + 1)\n",
    "    \n",
    "    def reset(\n",
    "        self, \n",
    "        buffer_state: BufferState\n",
    "    ) -> BufferState:\n",
    "        \n",
    "        state_buffer = jnp.empty((buffer_state.buffer_size + 1, observation_dim), dtype=jnp.float32)\n",
    "        action_buffer = jnp.empty(buffer_state.buffer_size + 1, dtype=jnp.int32)\n",
    "        reward_buffer = jnp.empty(buffer_state.buffer_size + 1, dtype=jnp.float32)\n",
    "        done_buffer = jnp.empty(buffer_state.buffer_size + 1, dtype=bool) \n",
    "        log_probs_buffer = jnp.empty(buffer_state.buffer_size + 1, dtype=jnp.float32)\n",
    "        values_buffer = jnp.empty(buffer_state.buffer_size + 1, dtype=jnp.float32)\n",
    "        advantages_buffer = jnp.empty(buffer_state.buffer_size, dtype=jnp.float32)\n",
    "        returns_buffer = jnp.empty(buffer_state.buffer_size, dtype=jnp.float32)\n",
    "        \n",
    "        buffer_state.states = state_buffer\n",
    "        buffer_state.actions = action_buffer\n",
    "        buffer_state.rewards = reward_buffer\n",
    "        buffer_state.dones = done_buffer\n",
    "        buffer_state.log_probs = log_probs_buffer\n",
    "        buffer_state.values = values_buffer\n",
    "        buffer_state.advantages = advantages_buffer\n",
    "        buffer_state.returns = returns_buffer\n",
    "        buffer_state.counter = jnp.array(0, dtype=jnp.int32)\n",
    "        \n",
    "        return buffer_state\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1953436b-895d-4171-adb5-46973eb9f7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create replay buffer \n",
    "buffer = JaxTrajectoryBuffer()\n",
    "buffer_state = buffer.create_buffer(\n",
    "        buffer_size = HORIZON,  \n",
    "        observation_dim = observation_dim,\n",
    "        gae_lambda = GAE_LAMBDA, \n",
    "        discount = DISCOUNT, \n",
    "        num_minibatches = NUM_MINIBATCHES,\n",
    ")\n",
    "jit_add = jax.jit(buffer.add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31608b73-86c1-434e-8efc-640d777d90b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create networks \n",
    "\n",
    "network_state, policy_init_state = random.split(random_state)\n",
    "network_state, value_init_state  = random.split(network_state)\n",
    "\n",
    "# Create feedforward policy and value network  \n",
    "\n",
    "def policy_fn(batch) -> jnp.ndarray:\n",
    "    \"\"\"Standard MLP network.\"\"\"\n",
    "    x = batch.astype(jnp.float32)\n",
    "    mlp = hk.Sequential([\n",
    "        hk.Linear(64), jax.nn.relu,\n",
    "        hk.Linear(64), jax.nn.relu,\n",
    "        hk.Linear(num_actions),\n",
    "    ])\n",
    "    return mlp(x)\n",
    "\n",
    "def value_fn(batch) -> jnp.ndarray:\n",
    "    \"\"\"Standard MLP network.\"\"\"\n",
    "    x = batch.astype(jnp.float32)\n",
    "    mlp = hk.Sequential([\n",
    "        hk.Linear(64), jax.nn.relu,\n",
    "        hk.Linear(64), jax.nn.relu,\n",
    "        hk.Linear(1),\n",
    "    ])\n",
    "    return mlp(x)\n",
    "\n",
    "dummy_pass_data = jnp.ones((1, observation_dim))\n",
    "\n",
    "# initialize policy and value parameters  \n",
    "policy_network = hk.without_apply_rng(hk.transform(policy_fn))\n",
    "policy_params  = policy_network.init(policy_init_state, dummy_pass_data)\n",
    "\n",
    "value_network = hk.without_apply_rng(hk.transform(value_fn))\n",
    "value_params  = value_network.init(value_init_state, dummy_pass_data)\n",
    "\n",
    "\n",
    "# Intialize optimisers and optimiser states \n",
    "policy_optimiser = optax.adam(LEARNING_RATE)\n",
    "policy_optimiser_state = policy_optimiser.init(policy_params)\n",
    "\n",
    "value_optimiser = optax.adam(LEARNING_RATE)\n",
    "value_optimiser_state = value_optimiser.init(value_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "683b04f1-7c78-4dd4-9fe9-969c2ef18208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @jit\n",
    "# @chex.assert_max_traces(n=1)\n",
    "def get_action_logprob(observation, action_key, policy_params):\n",
    "    \n",
    "    \"\"\"Given an observation, returns the action to take in the environment, \\\n",
    "       the related log probability, the state value, and the distribution entropy.\n",
    "       \"\"\"\n",
    "    \n",
    "    logits = policy_network.apply(policy_params, observation)\n",
    "    distribution = distrax.Categorical(logits=logits)\n",
    "    \n",
    "    action, logprob = distribution.sample_and_log_prob(\n",
    "        seed = action_key, \n",
    "    )\n",
    "    \n",
    "    entropy = distribution.entropy()\n",
    "    \n",
    "    return jnp.squeeze(action), jnp.squeeze(logprob), jnp.squeeze(entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34f8c50c-20bb-426b-8dcd-dc2445e44ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @jit\n",
    "# @chex.assert_max_traces(n=1)\n",
    "def get_value(observation, value_params):\n",
    "    \n",
    "    \"\"\"Given an observation, returns the action to take in the environment, \\\n",
    "       the related log probability, the state value, and the distribution entropy.\n",
    "       \"\"\"\n",
    "    \n",
    "    value = value_network.apply(value_params, observation)\n",
    "    \n",
    "    return jnp.squeeze(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4922ac0d-2379-489d-85d2-d2bbb1fc7a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_policy_loss(policy_params, minibatch: MiniBatch):\n",
    "    \n",
    "    # TODO: pass in somehow \n",
    "    CLIP_EPSILON = 0.2\n",
    "    \n",
    "    states = minibatch.states \n",
    "    actions = minibatch.actions\n",
    "    old_log_probs = minibatch.log_probs\n",
    "    old_values = minibatch.values \n",
    "    key = minibatch.key\n",
    "    advantages = minibatch.advantages\n",
    "    \n",
    "    key, train_key = random.split(key)\n",
    "    batch_train_keys = random.split(train_key, len(states))\n",
    "    \n",
    "    _, new_log_probs, entropy = vmap(get_action_logprob, in_axes = (0, 0, None))(\n",
    "        states, \n",
    "        batch_train_keys, \n",
    "        policy_params)\n",
    "    \n",
    "    ratio = jnp.exp(new_log_probs - old_log_probs)\n",
    "    \n",
    "    term_1 = ratio * advantages \n",
    "    term_2 = jnp.clip(a = ratio, a_min= 1- CLIP_EPSILON, a_max = 1 + CLIP_EPSILON) * advantages\n",
    "    \n",
    "    jax.debug.print(\"term policy loss 1 {x}:\", x=term_1.shape)\n",
    "    jax.debug.print(\"term policy loss 2 {y}:\", y=term_2.shape)\n",
    "    jax.debug.print(\"policy loss before mean {z}:\", z=jnp.minimum(term_1, term_2).shape)\n",
    "    \n",
    "    \n",
    "    # negative loss for gradient ascent \n",
    "    loss = -jnp.mean(jnp.minimum(term_1, term_2))\n",
    "    \n",
    "    # TODO maybe stop gradient for all value stuff? \n",
    "    \n",
    "    return loss \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60879c28-1b0b-4c64-aa12-68c56326eb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_value_loss(value_params, minibatch: MiniBatch):\n",
    "    \n",
    "    states = minibatch.states \n",
    "    returns = minibatch.returns \n",
    "    key = minibatch.key\n",
    "    \n",
    "    key, train_key = random.split(key)\n",
    "    batch_train_key = random.split(train_key, len(states))\n",
    "    \n",
    "    new_values = vmap(get_value, in_axes = (0, None))(\n",
    "        states,\n",
    "        value_params, \n",
    "    )\n",
    "    \n",
    "    jax.debug.print(\"values value loss {x}:\", x=new_values.shape)\n",
    "    jax.debug.print(\"returns value loss {y}:\", y=returns.shape)\n",
    "    jax.debug.print(\"loss pre mean {y}:\", y=((returns - new_values)**2).shape)\n",
    "    \n",
    "    loss = jnp.mean((returns - new_values)**2)\n",
    "    \n",
    "    return loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f756e13b-6ed9-4fd6-a34c-43b9ea6728f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @jit\n",
    "# @chex.assert_max_traces(n=1)\n",
    "def update_policy(policy_params, policy_optimiser_state, minibatch):\n",
    "    grads = jax.grad(ppo_policy_loss, argnums=0)(policy_params, minibatch)\n",
    "    updates, new_pol_optimiser_state = policy_optimiser.update(grads, policy_optimiser_state)\n",
    "    new_policy_params = optax.apply_updates(policy_params, updates)\n",
    "    return new_policy_params, new_pol_optimiser_state\n",
    "\n",
    "\n",
    "# @jit\n",
    "# @chex.assert_max_traces(n=1)\n",
    "def update_value(value_params, value_optimiser_state, minibatch):\n",
    "    grads = jax.grad(ppo_value_loss, argnums=0)(value_params, minibatch)\n",
    "    updates, new_val_optimiser_state = value_optimiser.update(grads, value_optimiser_state)\n",
    "    new_value_params = optax.apply_updates(value_params, updates)\n",
    "    return new_value_params, new_val_optimiser_state\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05416a22-d2b7-4ec8-bde3-e6eaf9582443",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d91e18e2-900f-44fe-b430-e4266b9b066f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "term policy loss 1 [ 4.954503  13.028123  13.000199   1.6912967  5.8092785  9.745354\n",
      " 13.395067  11.636981  10.648446   1.9259645 12.766047   8.793589\n",
      "  6.110169   2.0673223  8.889917   5.654637   8.766969   3.5000653\n",
      " 10.169163   1.0651058 10.932582   8.249395  10.184782  12.759099\n",
      "  0.7738145  5.592564  13.9128685  9.551195   9.944553  10.159391\n",
      " 13.120686  10.0078535  7.48395    5.805062   7.7717986  4.107291\n",
      "  4.9499145  4.239339   7.6211767  2.56833    9.645289  11.0597515\n",
      "  2.3789756  1.7513583  5.812869   2.8808389  7.0829563  4.093126\n",
      " 11.506878   8.314521 ]:\n",
      "term policy loss 2 [ 4.954503  13.028123  13.000199   1.6912967  5.8092785  9.745354\n",
      " 13.395067  11.636981  10.648446   2.0793455 12.766047   8.793589\n",
      "  6.110169   2.0673223  8.889917   5.654637   8.766969   3.5000653\n",
      " 10.169163   1.0651058 10.932582   8.249395  10.184782  12.759099\n",
      "  0.7738145  5.0887628 13.9128685  9.551195   9.944553  10.159391\n",
      " 13.120686  10.0078535  7.48395    5.805062   7.7717986  4.107291\n",
      "  4.9499145  4.239339   7.6211767  2.56833    9.645289  11.0597515\n",
      "  2.7041123  1.7513583  5.812869   2.8808389  7.0829563  4.093126\n",
      " 11.506878   8.314521 ]:\n",
      "loss before mean [ 4.954503  13.028123  13.000199   1.6912967  5.8092785  9.745354\n",
      " 13.395067  11.636981  10.648446   1.9259645 12.766047   8.793589\n",
      "  6.110169   2.0673223  8.889917   5.654637   8.766969   3.5000653\n",
      " 10.169163   1.0651058 10.932582   8.249395  10.184782  12.759099\n",
      "  0.7738145  5.0887628 13.9128685  9.551195   9.944553  10.159391\n",
      " 13.120686  10.0078535  7.48395    5.805062   7.7717986  4.107291\n",
      "  4.9499145  4.239339   7.6211767  2.56833    9.645289  11.0597515\n",
      "  2.3789756  1.7513583  5.812869   2.8808389  7.0829563  4.093126\n",
      " 11.506878   8.314521 ]:\n",
      "term policy loss 1 [ 5.0368032  12.950752   13.001072    1.653368    5.8284516   9.590199\n",
      " 13.317062   11.646913   10.762299    1.9494923  12.755963    8.737301\n",
      "  6.117104    2.0717583   8.84439     5.7442145   8.76271     3.5391445\n",
      " 10.178511    1.0707835  10.918521    8.242261   10.196868   12.753786\n",
      "  0.78496784  5.5451803  14.214932    9.600965   10.159777   10.323525\n",
      " 13.269413   10.060254    7.668633    5.7422705   7.7800183   4.100432\n",
      "  4.9530067   4.1469216   7.6616907   2.6293051   9.746689   11.059835\n",
      "  2.4148233   1.718214    5.8369803   2.8696089   7.0081277   4.2034774\n",
      " 11.479173    8.310857  ]:\n",
      "term policy loss 2 [ 5.0368032  12.950752   13.001072    1.653368    5.8284516   9.590199\n",
      " 13.317062   11.646913   10.762299    2.0793455  12.755963    8.737301\n",
      "  6.117104    2.0717583   8.84439     5.7442145   8.76271     3.5391445\n",
      " 10.178511    1.0707835  10.918521    8.242261   10.196868   12.753786\n",
      "  0.78496784  5.0887628  14.214932    9.600965   10.159777   10.323525\n",
      " 13.269413   10.060254    7.668633    5.7422705   7.7800183   4.100432\n",
      "  4.9530067   4.1469216   7.6616907   2.6293051   9.746689   11.059835\n",
      "  2.7041123   1.718214    5.8369803   2.8696089   7.0081277   4.2034774\n",
      " 11.479173    8.310857  ]:\n",
      "loss before mean [ 5.0368032  12.950752   13.001072    1.653368    5.8284516   9.590199\n",
      " 13.317062   11.646913   10.762299    1.9494923  12.755963    8.737301\n",
      "  6.117104    2.0717583   8.84439     5.7442145   8.76271     3.5391445\n",
      " 10.178511    1.0707835  10.918521    8.242261   10.196868   12.753786\n",
      "  0.78496784  5.0887628  14.214932    9.600965   10.159777   10.323525\n",
      " 13.269413   10.060254    7.668633    5.7422705   7.7800183   4.100432\n",
      "  4.9530067   4.1469216   7.6616907   2.6293051   9.746689   11.059835\n",
      "  2.4148233   1.718214    5.8369803   2.8696089   7.0081277   4.2034774\n",
      " 11.479173    8.310857  ]:\n",
      "values value loss [ 0.18546936  0.06585386  0.05851714  0.26583144  0.10155986  0.1834431\n",
      "  0.06149057  0.00895998  0.11774941  0.24704489  0.00942617  0.09797136\n",
      " -0.01004266  0.07002407  0.05589611  0.16061509  0.04384567  0.18731557\n",
      "  0.00670738  0.13698414  0.02018374  0.0565242   0.05805969  0.0174673\n",
      "  0.21289012  0.24426171  0.28332213  0.05349858  0.23325655  0.17491877\n",
      "  0.11773009  0.04993389  0.35962126  0.12539491  0.06489261  0.06503346\n",
      "  0.08261819  0.24799837  0.05936471  0.27872205  0.10522676  0.04826736\n",
      "  0.30896473  0.24446276  0.11496828  0.13280149  0.11781116  0.27771226\n",
      "  0.11784172  0.0103731 ]:\n",
      "returns value loss [ 5.2326164 13.009203  12.509452   1.9571282  5.910838   9.748782\n",
      " 13.456558  11.645941  10.9627     2.8462267 12.775473   8.814515\n",
      "  6.0323906  2.1373463  8.823782   5.940813   8.810815   3.6873808\n",
      " 10.17587    1.2020899 10.944519   8.30592   10.943608  12.768036\n",
      "  1.         4.484897  14.196191   9.753558  10.17781   10.583776\n",
      " 13.238416  10.175569   7.8435717  5.9304566  8.318482   3.8409026\n",
      "  5.2171702  4.487337   7.778154   2.847052   9.750516  10.571111\n",
      "  3.689105   1.951038   5.9278374  3.0136404  7.2007675  4.49565\n",
      " 11.62472    8.304414 ]:\n",
      "loss pre mean [ 25.473696   167.53027    155.02577      2.8604846   33.747715\n",
      "  91.49571    179.42783    135.41933    117.61295      6.7557464\n",
      " 162.97194     75.97814     36.511        4.2738214   76.875824\n",
      "  33.41069     76.85974     12.250458   103.41187      1.1344503\n",
      " 119.3411      68.05252    118.49517    162.577        0.61954194\n",
      "  17.982988   193.56792     94.09116     98.89414    108.344315\n",
      " 172.15239    102.52849     56.009518    33.698742    68.12174\n",
      "  14.257188    26.363625    17.971994    59.579704     6.596319\n",
      "  93.03161    110.73023     11.425348     2.912399    33.789448\n",
      "   8.2992325   50.16827     17.790997   132.40823     68.79111   ]:\n",
      "values value loss [ 0.2095995   0.07691383  0.0669004   0.2986581   0.10871717  0.20693612\n",
      "  0.07217321  0.01175945  0.1343228   0.27709833  0.01235595  0.10946488\n",
      " -0.0057598   0.08306202  0.06563684  0.18356097  0.05095774  0.20420262\n",
      "  0.01012101  0.15640604  0.0252379   0.06172239  0.06680707  0.02206283\n",
      "  0.23486431  0.27325448  0.31303874  0.06320734  0.26305065  0.19786137\n",
      "  0.13515846  0.05971388  0.3939589   0.14258543  0.07468841  0.07716785\n",
      "  0.09094197  0.27936605  0.06914668  0.31163323  0.12188503  0.05509863\n",
      "  0.3450072   0.27226245  0.13005535  0.15106185  0.13433361  0.3144484\n",
      "  0.13170622  0.01470373]:\n",
      "returns value loss [ 5.2326164 13.009203  12.509452   1.9571282  5.910838   9.748782\n",
      " 13.456558  11.645941  10.9627     2.8462267 12.775473   8.814515\n",
      "  6.0323906  2.1373463  8.823782   5.940813   8.810815   3.6873808\n",
      " 10.17587    1.2020899 10.944519   8.30592   10.943608  12.768036\n",
      "  1.         4.484897  14.196191   9.753558  10.17781   10.583776\n",
      " 13.238416  10.175569   7.8435717  5.9304566  8.318482   3.8409026\n",
      "  5.2171702  4.487337   7.778154   2.847052   9.750516  10.571111\n",
      "  3.689105   1.951038   5.9278374  3.0136404  7.2007675  4.49565\n",
      " 11.62472    8.304414 ]:\n",
      "loss pre mean [ 25.2307     167.2441     154.8171       2.7505229   33.66461\n",
      "  91.04683    179.14177    135.35417    117.253746     6.60042\n",
      " 162.89716     75.7779      36.45926      4.220084    76.70511\n",
      "  33.145954    76.7351      12.13253    103.342445     1.0934547\n",
      " 119.2307      67.96678    118.30481    162.45982      0.58543265\n",
      "  17.737934   192.74191     93.90289     98.30244    107.867226\n",
      " 171.69534    102.330505    55.496727    33.499454    67.960144\n",
      "  14.1657      26.278217    17.70702     59.428795     6.4283495\n",
      "  92.710526   110.58651     11.18299      2.8182874   33.614277\n",
      "   8.194356    49.934486    17.482445   132.08936     68.71929   ]:\n",
      "term policy loss 1 [10.857886    2.6769094   6.4563036   5.088386   13.056354    9.298034\n",
      "  8.3171015   3.9013994  12.07901     6.651815   11.313707   12.4374695\n",
      "  4.919215    6.998108    7.9927444  12.158912    7.2950435   2.2432842\n",
      " 11.245709   11.197127    7.63358    13.980343    7.738122    3.3843071\n",
      " 11.929699   10.083916   13.696579    1.7250042  12.244705    3.4692361\n",
      "  0.6636263   7.206653    6.4514804   2.6233475   9.186536    0.5477954\n",
      "  7.0227795   7.2061563   4.329845   11.955885    0.69993556 12.437227\n",
      "  9.297864    9.153816   13.71934     8.820284    5.6980143   7.124531\n",
      "  6.435677   10.524779  ]:\n",
      "term policy loss 2 [10.857886    2.6769094   6.4563036   5.088386   13.056354    9.298034\n",
      "  8.3171015   3.9013994  12.07901     6.651815   11.313707   12.4374695\n",
      "  4.919215    6.998108    7.9927444  12.158912    7.2950435   2.1190906\n",
      " 11.245709   11.197127    7.63358    13.980343    7.738122    3.3843071\n",
      " 11.929699   10.083916   13.696579    1.7250042  12.244705    3.4692361\n",
      "  0.6636263   7.206653    6.4514804   2.6233475   9.186536    0.5952185\n",
      "  7.0227795   7.2061563   4.329845   11.955885    0.69993556 12.437227\n",
      "  9.297864    9.153816   13.71934     8.820284    5.6980143   7.124531\n",
      "  6.435677   10.524779  ]:\n",
      "loss before mean [10.857886    2.6769094   6.4563036   5.088386   13.056354    9.298034\n",
      "  8.3171015   3.9013994  12.07901     6.651815   11.313707   12.4374695\n",
      "  4.919215    6.998108    7.9927444  12.158912    7.2950435   2.1190906\n",
      " 11.245709   11.197127    7.63358    13.980343    7.738122    3.3843071\n",
      " 11.929699   10.083916   13.696579    1.7250042  12.244705    3.4692361\n",
      "  0.6636263   7.206653    6.4514804   2.6233475   9.186536    0.5477954\n",
      "  7.0227795   7.2061563   4.329845   11.955885    0.69993556 12.437227\n",
      "  9.297864    9.153816   13.71934     8.820284    5.6980143   7.124531\n",
      "  6.435677   10.524779  ]:\n",
      "term policy loss 1 [10.879478    2.662617    6.359402    5.1319547  13.456689    9.346497\n",
      "  8.318369    3.9238875  12.035765    6.6221523  11.310297   12.423976\n",
      "  4.8238215   6.984486    8.048084   12.309693    7.295946    2.2248046\n",
      " 11.233781   11.137408    7.684745   13.99874     7.739206    3.2977946\n",
      " 11.937982   10.123266   13.841832    1.7438129  12.246814    3.5681138\n",
      "  0.68053955  7.2016325   6.514705    2.6874015   9.088369    0.555663\n",
      "  6.9376206   7.219363    4.31149    12.020198    0.6886309  12.41072\n",
      "  9.205142    9.120046   13.801764    8.820921    5.683401    7.0877438\n",
      "  6.3880677  10.525524  ]:\n",
      "term policy loss 2 [10.879478    2.662617    6.359402    5.1319547  13.456689    9.346497\n",
      "  8.318369    3.9238875  12.035765    6.6221523  11.310297   12.423976\n",
      "  4.8238215   6.984486    8.048084   12.309693    7.295946    2.1190906\n",
      " 11.233781   11.137408    7.684745   13.99874     7.739206    3.2977946\n",
      " 11.937982   10.123266   13.841832    1.7438129  12.246814    3.5681138\n",
      "  0.68053955  7.2016325   6.514705    2.6874015   9.088369    0.5952185\n",
      "  6.9376206   7.219363    4.31149    12.020198    0.6886309  12.41072\n",
      "  9.205142    9.120046   13.801764    8.820921    5.683401    7.0877438\n",
      "  6.3880677  10.525524  ]:\n",
      "loss before mean [10.879478    2.662617    6.359402    5.1319547  13.456689    9.346497\n",
      "  8.318369    3.9238875  12.035765    6.6221523  11.310297   12.423976\n",
      "  4.8238215   6.984486    8.048084   12.309693    7.295946    2.1190906\n",
      " 11.233781   11.137408    7.684745   13.99874     7.739206    3.2977946\n",
      " 11.937982   10.123266   13.841832    1.7438129  12.246814    3.5681138\n",
      "  0.68053955  7.2016325   6.514705    2.6874015   9.088369    0.555663\n",
      "  6.9376206   7.219363    4.31149    12.020198    0.6886309  12.41072\n",
      "  9.205142    9.120046   13.801764    8.820921    5.683401    7.0877438\n",
      "  6.3880677  10.525524  ]:\n",
      "values value loss [ 0.06575809  0.1665153   0.19916815  0.20170496  0.45767853  0.05945224\n",
      "  0.01181766  0.14878431  0.13718003  0.03278301  0.00554697  0.06009893\n",
      "  0.24798033  0.05781671  0.19779359  0.22284949 -0.0068585   0.21137668\n",
      "  0.05581953  0.07043111  0.12804732  0.00950995  0.00179678  0.35237488\n",
      "  0.00770411  0.12746356  0.13300888  0.23658133  0.01202179  0.38341382\n",
      "  0.3764073   0.00882431  0.12071325  0.33915582  0.15744628  0.28751165\n",
      "  0.19761023  0.05423973  0.13628116  0.06994791  0.3132935   0.07280999\n",
      "  0.12690932  0.12158231  0.0713001   0.00193173  0.07484863  0.05801921\n",
      "  0.15258327  0.00471488]:\n",
      "returns value loss [11.623381   2.8491926  6.5838017  5.230137  13.0157995  9.301492\n",
      "  8.318292   4.615588  12.231122   6.6729665 11.295338  11.938883\n",
      "  5.241905   6.5808225  9.741422  12.224737   7.277089   1.9531709\n",
      " 11.294319  11.316249   7.76203   14.030746   7.7815485  3.6914494\n",
      " 11.934633  10.170622  13.6589365  1.9547149 12.245205   3.6989286\n",
      "  1.         7.1917276  6.5952854  2.8538375  9.294902   1.\n",
      "  7.1804686  7.7640657  4.4772544 11.953451   1.        12.517074\n",
      "  9.301534   9.288742  13.851946   8.826745   5.3474336  7.208032\n",
      "  6.564171  10.572595 ]:\n",
      "loss pre mean [133.57864      7.1967573   40.763546    25.285128   157.70639\n",
      "  85.4153      68.997505    19.95234    146.26343     44.092037\n",
      " 127.45938    141.10551     24.939287    42.549606    91.08083\n",
      " 144.0453      53.05589      3.033847   126.30387    126.46842\n",
      "  58.27769    196.59508     60.524536    11.149419   142.25165\n",
      " 100.865036   182.95071      2.951983   149.65076     10.992639\n",
      "   0.38886788  51.5941      41.920086     6.3236237   83.4931\n",
      "   0.50763965  48.760307    59.441418    18.84405    141.21764\n",
      "   0.47156587 154.8597      84.17374     84.03682    189.90619\n",
      "  77.87732     27.800152    51.122684    41.108456   111.680084  ]:\n",
      "values value loss [ 0.07436329  0.17696151  0.22231118  0.22386031  0.5053372   0.06889836\n",
      "  0.01478032  0.16657655  0.15151156  0.04210389  0.00873007  0.06762955\n",
      "  0.27744117  0.06603137  0.21723412  0.24924612 -0.00260769  0.23526041\n",
      "  0.06286167  0.07999413  0.14058344  0.01349394  0.00493136  0.39063486\n",
      "  0.01101422  0.14022575  0.1500767   0.25525188  0.01489211  0.42683342\n",
      "  0.41637066  0.0126739   0.13671799  0.37702736  0.1747825   0.31878257\n",
      "  0.21723792  0.06178854  0.14508273  0.07951289  0.33919996  0.08143684\n",
      "  0.14303334  0.13479549  0.08190275  0.00507869  0.08601075  0.06744338\n",
      "  0.1663241   0.00791466]:\n",
      "returns value loss [11.623381   2.8491926  6.5838017  5.230137  13.0157995  9.301492\n",
      "  8.318292   4.615588  12.231122   6.6729665 11.295338  11.938883\n",
      "  5.241905   6.5808225  9.741422  12.224737   7.277089   1.9531709\n",
      " 11.294319  11.316249   7.76203   14.030746   7.7815485  3.6914494\n",
      " 11.934633  10.170622  13.6589365  1.9547149 12.245205   3.6989286\n",
      "  1.         7.1917276  6.5952854  2.8538375  9.294902   1.\n",
      "  7.1804686  7.7640657  4.4772544 11.953451   1.        12.517074\n",
      "  9.301534   9.288742  13.851946   8.826745   5.3474336  7.208032\n",
      "  6.564171  10.572595 ]:\n",
      "loss pre mean [133.37979      7.1408195   40.468563    25.062805   156.51167\n",
      "  85.24078     68.9483      19.793707   145.91699     43.96834\n",
      " 127.38751    140.92665     24.645905    42.4425      90.71016\n",
      " 143.41237     52.993988     2.9512167  126.145645   126.25342\n",
      "  58.08645    196.48338     60.475773    10.895377   142.1727\n",
      " 100.60886    182.48929      2.8881745  149.58054     10.706607\n",
      "   0.34062323  51.538815    41.713097     6.1345887   83.176575\n",
      "   0.46405718  48.48658     59.325073    18.76771    140.99042\n",
      "   0.43665668 154.64505     83.878136    83.79475    189.61409\n",
      "  77.8218      27.682568    50.988007    40.93244    111.612465  ]:\n",
      "term policy loss 1 [13.0275755 10.525524   0.7290075 13.999745  14.475849   8.248974\n",
      " 12.218098   3.5594406  0.723283   5.607817   1.0783496  4.3273454\n",
      " 10.356589   6.9376206  9.864379  12.754795   6.706711  12.035765\n",
      "  7.219363  11.598391  12.462301   6.361534   7.538843   2.2248046\n",
      "  5.492901  15.427805  10.897619   1.6883996  6.5243    10.869839\n",
      "  7.2327833 11.242866   2.6841023  4.8238215  9.843696  13.627947\n",
      "  2.4536276  7.703273   9.463087   1.6964244 11.438367   6.6092935\n",
      "  9.25117    2.4865463 12.1892605 11.938225  11.5317335 11.291096\n",
      "  2.499926  12.739054 ]:\n",
      "term policy loss 2 [13.0275755 10.525524   0.7290075 13.999745  14.475849   8.248974\n",
      " 12.218098   3.5594406  0.723283   5.607817   1.0783496  4.3273454\n",
      " 10.356589   6.9376206  9.864379  12.754795   6.706711  12.035765\n",
      "  7.219363  11.598391  12.462301   6.361534   7.538843   2.1190906\n",
      "  5.0887628 14.434344  10.897619   1.6883996  6.5243    10.869839\n",
      "  7.2327833 11.242866   2.6841023  4.8238215  9.843696  13.627947\n",
      "  2.7041123  7.703273   9.463087   1.6964244 11.438367   6.6092935\n",
      "  9.25117    2.4865463 12.1892605 11.938225  11.5317335 11.291096\n",
      "  2.499926  12.739054 ]:\n",
      "loss before mean [13.0275755 10.525524   0.7290075 13.999745  14.475849   8.248974\n",
      " 12.218098   3.5594406  0.723283   5.607817   1.0783496  4.3273454\n",
      " 10.356589   6.9376206  9.864379  12.754795   6.706711  12.035765\n",
      "  7.219363  11.598391  12.462301   6.361534   7.538843   2.1190906\n",
      "  5.0887628 14.434344  10.897619   1.6883996  6.5243    10.869839\n",
      "  7.2327833 11.242866   2.6841023  4.8238215  9.843696  13.627947\n",
      "  2.4536276  7.703273   9.463087   1.6964244 11.438367   6.6092935\n",
      "  9.25117    2.4865463 12.1892605 11.938225  11.5317335 11.291096\n",
      "  2.499926  12.739054 ]:\n",
      "term policy loss 1 [12.910191  10.536421   0.7259748 13.97859   14.69391    8.257495\n",
      " 11.899013   3.6296675  0.7330901  5.539479   1.0814271  4.344943\n",
      " 10.517175   6.8666883  9.983465  12.769406   6.7331033 12.024928\n",
      "  7.227313  11.61029   12.468153   6.3084207  7.4926558  2.2166898\n",
      "  5.4764867 15.359269  10.87645    1.6634873  6.5677924 10.960694\n",
      "  7.237308  11.295804   2.7284827  4.746138   9.925309  13.556179\n",
      "  2.466204   7.7397814  9.447969   1.679793  11.427468   6.6889796\n",
      "  9.208538   2.434921  12.203217  11.924122  11.524151  11.304217\n",
      "  2.5055883 12.721503 ]:\n",
      "term policy loss 2 [12.910191  10.536421   0.7259748 13.97859   14.69391    8.257495\n",
      " 11.899013   3.6296675  0.7330901  5.539479   1.0814271  4.344943\n",
      " 10.517175   6.8666883  9.983465  12.769406   6.7331033 12.024928\n",
      "  7.227313  11.61029   12.468153   6.3084207  7.4926558  2.1190906\n",
      "  5.0887628 14.434344  10.87645    1.6634873  6.5677924 10.960694\n",
      "  7.237308  11.295804   2.7284827  4.746138   9.925309  13.556179\n",
      "  2.7041123  7.7397814  9.447969   1.679793  11.427468   6.6889796\n",
      "  9.208538   2.434921  12.203217  11.924122  11.524151  11.304217\n",
      "  2.5055883 12.721503 ]:\n",
      "loss before mean [12.910191  10.536421   0.7259748 13.97859   14.69391    8.257495\n",
      " 11.899013   3.6296675  0.7330901  5.539479   1.0814271  4.344943\n",
      " 10.517175   6.8666883  9.983465  12.769406   6.7331033 12.024928\n",
      "  7.227313  11.61029   12.468153   6.3084207  7.4926558  2.1190906\n",
      "  5.0887628 14.434344  10.87645    1.6634873  6.5677924 10.960694\n",
      "  7.237308  11.295804   2.7284827  4.746138   9.925309  13.556179\n",
      "  2.466204   7.7397814  9.447969   1.679793  11.427468   6.6889796\n",
      "  9.208538   2.434921  12.203217  11.924122  11.524151  11.304217\n",
      "  2.5055883 12.721503 ]:\n",
      "values value loss [ 0.15240985  0.00791466  0.31878257  0.01349394  0.34241483  0.01478032\n",
      "  0.5053372   0.39063486  0.33919996  0.2062466   0.17583813  0.14508273\n",
      "  0.29254562  0.21723792  0.23019855  0.01531152  0.04210389  0.15151156\n",
      "  0.06178854  0.01460331  0.07510401  0.13671799  0.14058344  0.23526041\n",
      "  0.3024197   0.24924612  0.0302779   0.29952148  0.16632411  0.15033327\n",
      " -0.00260769  0.07999413  0.34418303  0.27744117  0.13816324  0.08190275\n",
      "  0.38124856  0.0787227   0.21723412  0.25525188  0.14535664  0.22231118\n",
      "  0.06889836  0.37702736  0.01489211  0.01101422  0.07436329  0.00873007\n",
      "  0.16898146  0.02658325]:\n",
      "returns value loss [13.238416  10.572595   1.        14.030746  14.196191   8.318292\n",
      " 13.0157995  3.6914494  1.         5.940813   1.2020899  4.4772544\n",
      " 10.17781    7.1804686  9.748782  12.775473   6.6729665 12.231122\n",
      "  7.7640657 11.645941  12.509452   6.5952854  7.76203    1.9531709\n",
      "  4.484897  12.224737  10.944519   1.951038   6.564171  10.9627\n",
      "  7.277089  11.316249   2.847052   5.241905   9.750516  13.851946\n",
      "  3.689105   7.778154   9.741422   1.9547149 11.62472    6.5838017\n",
      "  9.301492   2.8538375 12.245205  11.934633  11.623381  11.295338\n",
      "  3.0136404 12.768036 ]:\n",
      "loss pre mean [171.24356    111.612465     0.46405718 196.48338    191.92711\n",
      "  68.9483     156.51167     10.895377     0.43665668  32.885254\n",
      "   1.0531927   18.76771     97.71845     48.48658     90.60343\n",
      " 162.82172     43.96834    145.91699     59.325073   135.28801\n",
      " 154.613       41.713097    58.08645      2.9512167   17.493118\n",
      " 143.41237    119.12065      2.7275069   40.93244    116.90727\n",
      "  52.993988   126.25342      6.2643538   24.645905    92.397316\n",
      " 189.61409     10.941915    59.281242    90.71016      2.8881745\n",
      " 131.77579     40.468563    85.24078      6.1345887  149.58054\n",
      " 142.1727     133.37979    127.38751      8.092084   162.3446    ]:\n",
      "values value loss [0.16983147 0.01108366 0.3503325  0.01762102 0.3721336  0.01773892\n",
      " 0.552755   0.4293082  0.36618525 0.22916088 0.19541605 0.15398592\n",
      " 0.32234755 0.23709239 0.25370067 0.01830046 0.05153326 0.16597873\n",
      " 0.06938503 0.01753085 0.08337402 0.15293579 0.15347685 0.25930566\n",
      " 0.33155575 0.27578127 0.03538601 0.32709697 0.18024473 0.1665052\n",
      " 0.00169189 0.08945265 0.37681648 0.30720398 0.15433145 0.09246419\n",
      " 0.41742516 0.08848302 0.23673698 0.2741497  0.15930201 0.24569188\n",
      " 0.07864182 0.4153027  0.01785369 0.01429112 0.08315971 0.01197298\n",
      " 0.18700792 0.03136889]:\n",
      "returns value loss [13.238416  10.572595   1.        14.030746  14.196191   8.318292\n",
      " 13.0157995  3.6914494  1.         5.940813   1.2020899  4.4772544\n",
      " 10.17781    7.1804686  9.748782  12.775473   6.6729665 12.231122\n",
      "  7.7640657 11.645941  12.509452   6.5952854  7.76203    1.9531709\n",
      "  4.484897  12.224737  10.944519   1.951038   6.564171  10.9627\n",
      "  7.277089  11.316249   2.847052   5.241905   9.750516  13.851946\n",
      "  3.689105   7.778154   9.741422   1.9547149 11.62472    6.5838017\n",
      "  9.301492   2.8538375 12.245205  11.934633  11.623381  11.295338\n",
      "  3.0136404 12.768036 ]:\n",
      "loss pre mean [170.7879     111.54552      0.42206785 196.36769    191.10457\n",
      "  68.89917    155.32747     10.641565     0.40172115  32.62297\n",
      "   1.0133922   18.690649    97.130135    48.210472    90.15658\n",
      " 162.74545     43.84338    145.56769     59.20811    135.21992\n",
      " 154.40741     41.50387     57.890083     2.8691797   17.250244\n",
      " 142.77754    119.00918      2.6371846   40.75451    116.55783\n",
      "  52.931404   126.040955     6.1020637   24.351278    92.08676\n",
      " 189.32333     10.703889    59.131042    90.33903      2.8242996\n",
      " 131.45581     40.17164     85.06096      5.9464517  149.50812\n",
      " 142.09456    133.17671    127.31431      7.9898515  162.22267   ]:\n",
      "term policy loss 1 [11.219651    0.80321246 13.166659    7.192554    8.167421   10.171263\n",
      "  5.1757994   0.6147449   8.321311    9.007576    9.967904    5.676691\n",
      " 10.228447   10.724877    2.5411913   7.802309    8.211298    6.59295\n",
      "  6.558727   11.04294    13.963371    8.867317    8.901634    9.4593725\n",
      "  3.647876    2.6490345   5.123902    5.799661    3.9267595   7.191034\n",
      "  9.972876    3.451547    9.6977     12.399223    3.9932766   4.440225\n",
      "  6.113055   13.169462    5.1467767   8.63329     7.9650106   7.7836676\n",
      " 11.905368    8.219611    2.0835042   6.8771625   5.8985825   1.591036\n",
      " 11.853445    3.799557  ]:\n",
      "term policy loss 2 [11.219651    0.80321246 13.166659    7.192554    8.167421   10.171263\n",
      "  5.1757994   0.6147449   8.321311    9.007576    9.967904    5.676691\n",
      " 10.228447   10.724877    2.5411913   7.802309    8.211298    6.59295\n",
      "  6.558727   11.04294    13.963371    8.867317    8.901634    9.4593725\n",
      "  3.647876    2.6490345   5.123902    5.799661    3.9267595   7.191034\n",
      "  9.972876    3.451547    9.6977     12.399223    3.9932766   4.440225\n",
      "  6.113055   13.169462    5.1467767   8.63329     7.9650106   7.7836676\n",
      " 11.905368    8.219611    2.0835042   6.8771625   5.8985825   1.591036\n",
      " 11.853445    3.799557  ]:\n",
      "loss before mean [11.219651    0.80321246 13.166659    7.192554    8.167421   10.171263\n",
      "  5.1757994   0.6147449   8.321311    9.007576    9.967904    5.676691\n",
      " 10.228447   10.724877    2.5411913   7.802309    8.211298    6.59295\n",
      "  6.558727   11.04294    13.963371    8.867317    8.901634    9.4593725\n",
      "  3.647876    2.6490345   5.123902    5.799661    3.9267595   7.191034\n",
      "  9.972876    3.451547    9.6977     12.399223    3.9932766   4.440225\n",
      "  6.113055   13.169462    5.1467767   8.63329     7.9650106   7.7836676\n",
      " 11.905368    8.219611    2.0835042   6.8771625   5.8985825   1.591036\n",
      " 11.853445    3.799557  ]:\n",
      "term policy loss 1 [11.205723    0.8098992  13.217818    7.2169914   8.179677   10.166243\n",
      "  5.2256937   0.60429054  8.331307    8.944371    9.868555    5.6710644\n",
      " 10.236838   10.730603    2.531851    7.8085823   8.218733    6.581223\n",
      "  6.566061   11.037256   14.058912    8.866536    8.932903    9.520745\n",
      "  3.7116873   2.6398733   5.123659    5.788509    3.8620448   7.1836786\n",
      "  9.937379    3.4292998   9.732193   12.388931    3.93818     4.432716\n",
      "  6.1124516  13.117014    5.160816    8.596521    8.072789    7.782248\n",
      " 11.91079     8.226613    2.0867438   6.830378    5.9377823   1.5691588\n",
      " 11.8110285   3.8042667 ]:\n",
      "term policy loss 2 [11.205723    0.8098992  13.217818    7.2169914   8.179677   10.166243\n",
      "  5.2256937   0.60429054  8.331307    8.944371    9.868555    5.6710644\n",
      " 10.236838   10.730603    2.531851    7.8085823   8.218733    6.581223\n",
      "  6.566061   11.037256   14.058912    8.866536    8.932903    9.520745\n",
      "  3.7116873   2.6398733   5.123659    5.788509    3.8620448   7.1836786\n",
      "  9.937379    3.4292998   9.732193   12.388931    3.93818     4.432716\n",
      "  6.1124516  13.117014    5.160816    8.596521    8.072789    7.782248\n",
      " 11.91079     8.226613    2.0867438   6.830378    5.9377823   1.5691588\n",
      " 11.8110285   3.8042667 ]:\n",
      "loss before mean [11.205723    0.8098992  13.217818    7.2169914   8.179677   10.166243\n",
      "  5.2256937   0.60429054  8.331307    8.944371    9.868555    5.6710644\n",
      " 10.236838   10.730603    2.531851    7.8085823   8.218733    6.581223\n",
      "  6.566061   11.037256   14.058912    8.866536    8.932903    9.520745\n",
      "  3.7116873   2.6398733   5.123659    5.788509    3.8620448   7.1836786\n",
      "  9.937379    3.4292998   9.732193   12.388931    3.93818     4.432716\n",
      "  6.1124516  13.117014    5.160816    8.596521    8.072789    7.782248\n",
      " 11.91079     8.226613    2.0867438   6.830378    5.9377823   1.5691588\n",
      " 11.8110285   3.8042667 ]:\n",
      "values value loss [0.15325464 0.2785746  0.0989558  0.07718702 0.14813353 0.01654425\n",
      " 0.2578197  0.4559815  0.02330344 0.19229884 0.24350058 0.09736927\n",
      " 0.08459388 0.06989101 0.33697164 0.09406215 0.06526637 0.16038378\n",
      " 0.07424708 0.06900785 0.16731253 0.00822034 0.0848731  0.15905431\n",
      " 0.4707067  0.18740536 0.10650022 0.1242387  0.38755268 0.0165851\n",
      " 0.0793021  0.23793675 0.08259575 0.09000698 0.34180164 0.18406197\n",
      " 0.0029678  0.09354529 0.24599287 0.13232417 0.4628592  0.00805118\n",
      " 0.07531364 0.07170309 0.1085972  0.16723071 0.17681886 0.3633796\n",
      " 0.08912805 0.1012556 ]:\n",
      "returns value loss [10.170622   1.        13.009203   7.208032   9.288742  10.17587\n",
      "  5.2326164  1.         8.304414   9.294902  10.583776   5.3474336\n",
      " 10.943608  11.294319   2.8462267  8.318482   8.810815   5.9278374\n",
      "  6.5808225 10.571111  13.6589365  8.826745   8.823782   9.301534\n",
      "  3.6989286  2.8491926  5.2171702  5.910838   4.49565    7.1917276\n",
      " 10.175569   3.6873808  9.753558  12.517074   4.487337   4.615588\n",
      "  6.0323906 13.456558   5.230137   8.814515   7.8435717  7.7815485\n",
      " 11.938883   8.30592    2.1373463  7.2007675  5.9304566  1.9571282\n",
      " 11.953451   3.8409026]:\n",
      "loss pre mean [100.34765      0.52045465 166.67447     50.848953    83.55073\n",
      " 103.2119      24.748604     0.29595613  68.5768      82.85738\n",
      " 106.9213      27.563175   117.9182     125.987785     6.2963605\n",
      "  67.64109     76.48461     33.263523    42.335526   110.29417\n",
      " 182.02391     77.76637     76.36852     83.58492     10.421416\n",
      "   7.0851116   26.118948    33.484734    16.876461    51.482674\n",
      " 101.9346      11.898664    93.52751    154.43199     17.185465\n",
      "  19.638424    36.35394    178.57013     24.841694    75.38044\n",
      "  54.47492     60.427258   140.74428     67.80232      4.1158223\n",
      "  49.470642    33.104347     2.5400345  140.76215     13.984959  ]:\n",
      "values value loss [0.16685356 0.30031106 0.11026447 0.08681068 0.16175455 0.0197314\n",
      " 0.2818181  0.49576727 0.02754903 0.2094083  0.26621988 0.10864207\n",
      " 0.09351294 0.07711546 0.36705193 0.10400398 0.07249089 0.17561124\n",
      " 0.08261009 0.07614647 0.18478794 0.01145144 0.09487638 0.17536747\n",
      " 0.51451206 0.19816594 0.11443926 0.13211906 0.42440793 0.02054433\n",
      " 0.08918309 0.25490463 0.09243752 0.09859072 0.37286735 0.20174193\n",
      " 0.00734897 0.10449062 0.26814204 0.14375179 0.49755788 0.01132064\n",
      " 0.08307312 0.07748812 0.12085798 0.18391053 0.19386269 0.39534307\n",
      " 0.09848425 0.11279762]:\n",
      "returns value loss [10.170622   1.        13.009203   7.208032   9.288742  10.17587\n",
      "  5.2326164  1.         8.304414   9.294902  10.583776   5.3474336\n",
      " 10.943608  11.294319   2.8462267  8.318482   8.810815   5.9278374\n",
      "  6.5808225 10.571111  13.6589365  8.826745   8.823782   9.301534\n",
      "  3.6989286  2.8491926  5.2171702  5.910838   4.49565    7.1917276\n",
      " 10.175569   3.6873808  9.753558  12.517074   4.487337   4.615588\n",
      "  6.0323906 13.456558   5.230137   8.814515   7.8435717  7.7815485\n",
      " 11.938883   8.30592    2.1373463  7.2007675  5.9304566  1.9571282\n",
      " 11.953451   3.8409026]:\n",
      "loss pre mean [100.07537      0.48956457 166.3826      50.711796    83.3019\n",
      " 103.14715     24.510406     0.25425068  68.50649     82.54619\n",
      " 106.45197     27.444937   117.72458    125.82567      6.146308\n",
      "  67.47765     76.35831     33.088108    42.226765   110.14428\n",
      " 181.55269     77.7094      76.193794    83.28691     10.140509\n",
      "   7.0279427   26.03786     33.393593    16.57501     51.42587\n",
      " 101.735176    11.781892    93.33725    154.21872     16.92886\n",
      "  19.482037    36.301125   178.27773     24.621391    75.18213\n",
      "  53.96392     60.376442   140.56024     67.707085     4.066225\n",
      "  49.236282    32.90851      2.4391727  140.54025     13.898767  ]:\n",
      "term policy loss 1 [ 6.561446  13.809758   9.773157   6.57595    8.899241  13.026243\n",
      "  8.472172  12.966706   5.0217257 11.59444    0.8434017  6.8183227\n",
      "  5.8903294  2.6043525 10.118362   6.6649523  6.7916203 12.033914\n",
      " 11.313789  13.932082  10.38409    5.596956   5.3544316  0.647797\n",
      "  0.5846709  4.0745463 11.594588   4.5973206 14.162269   8.882829\n",
      "  6.345809   6.104598  10.588612  12.748202  14.531519   1.5832855\n",
      "  5.159393  12.989896   9.249132   0.7154153  7.509148  12.51936\n",
      "  1.5653758  3.6070125 14.196327   8.987927  11.767419   2.6678452\n",
      " 11.299283   6.910161 ]:\n",
      "term policy loss 2 [ 6.561446  13.809758   9.773157   6.57595    8.899241  13.026243\n",
      "  8.472172  12.966706   5.0217257 11.59444    0.8434017  6.8183227\n",
      "  5.8903294  2.6043525 10.118362   6.6649523  6.7916203 12.033914\n",
      " 11.313789  13.932082  10.38409    5.596956   5.3544316  0.647797\n",
      "  0.5846709  4.0745463 11.594588   4.5973206 14.162269   8.882829\n",
      "  6.345809   6.104598  10.588612  12.748202  14.531519   1.5832855\n",
      "  5.159393  12.989896   9.249132   0.7154153  7.509148  12.51936\n",
      "  1.5653758  3.6070125 14.196327   8.987927  11.767419   2.6678452\n",
      " 11.299283   6.910161 ]:\n",
      "loss before mean [ 6.561446  13.809758   9.773157   6.57595    8.899241  13.026243\n",
      "  8.472172  12.966706   5.0217257 11.59444    0.8434017  6.8183227\n",
      "  5.8903294  2.6043525 10.118362   6.6649523  6.7916203 12.033914\n",
      " 11.313789  13.932082  10.38409    5.596956   5.3544316  0.647797\n",
      "  0.5846709  4.0745463 11.594588   4.5973206 14.162269   8.882829\n",
      "  6.345809   6.104598  10.588612  12.748202  14.531519   1.5832855\n",
      "  5.159393  12.989896   9.249132   0.7154153  7.509148  12.51936\n",
      "  1.5653758  3.6070125 14.196327   8.987927  11.767419   2.6678452\n",
      " 11.299283   6.910161 ]:\n",
      "term policy loss 1 [ 6.5628943 13.812728   9.768049   6.726769   9.004837  13.02429\n",
      "  8.507081  13.061236   5.0097756 11.6329975  0.8424186  6.8804965\n",
      "  5.884582   2.6246722 10.1642885  6.6815615  6.8448277 12.162659\n",
      " 11.311181  13.951092  10.360091   5.66933    5.397515   0.6465619\n",
      "  0.5961277  4.2988863 11.60943    4.595509  14.214043   8.879684\n",
      "  6.341008   6.112117  10.7275915 12.693475  14.527371   1.5870388\n",
      "  5.182166  12.991572   9.291418   0.7086539  7.5163116 12.519631\n",
      "  1.5371653  3.6044052 14.192203   9.009914  11.716146   2.6639116\n",
      " 11.293582   6.9960623]:\n",
      "term policy loss 2 [ 6.5628943 13.812728   9.768049   6.726769   9.004837  13.02429\n",
      "  8.507081  13.061236   5.0097756 11.6329975  0.8424186  6.8804965\n",
      "  5.884582   2.6246722 10.1642885  6.6815615  6.8448277 12.162659\n",
      " 11.311181  13.951092  10.360091   5.66933    5.397515   0.6465619\n",
      "  0.5961277  4.2988863 11.60943    4.595509  14.214043   8.879684\n",
      "  6.341008   6.112117  10.7275915 12.693475  14.527371   1.5870388\n",
      "  5.182166  12.991572   9.291418   0.7086539  7.5163116 12.519631\n",
      "  1.5371653  3.6044052 14.192203   9.009914  11.716146   2.6639116\n",
      " 11.293582   6.9960623]:\n",
      "loss before mean [ 6.5628943 13.812728   9.768049   6.726769   9.004837  13.02429\n",
      "  8.507081  13.061236   5.0097756 11.6329975  0.8424186  6.8804965\n",
      "  5.884582   2.6246722 10.1642885  6.6815615  6.8448277 12.162659\n",
      " 11.311181  13.951092  10.360091   5.66933    5.397515   0.6465619\n",
      "  0.5961277  4.2988863 11.60943    4.595509  14.214043   8.879684\n",
      "  6.341008   6.112117  10.7275915 12.693475  14.527371   1.5870388\n",
      "  5.182166  12.991572   9.291418   0.7086539  7.5163116 12.519631\n",
      "  1.5371653  3.6044052 14.192203   9.009914  11.716146   2.6639116\n",
      " 11.293582   6.9960623]:\n",
      "values value loss [0.00290609 0.01314734 0.02214847 0.6000331  0.27410388 0.02460757\n",
      " 0.09373559 0.15299235 0.19241613 0.06775135 0.10994191 0.23349743\n",
      " 0.01730471 0.24311334 0.09320605 0.27503124 0.179219   0.23285292\n",
      " 0.01791524 0.1069625  0.2179094  0.32702643 0.1925307  0.24135168\n",
      " 0.497709   0.20770381 0.12754327 0.00893042 0.48816064 0.01964679\n",
      " 0.14537828 0.10046342 0.38962442 0.08519247 0.00899235 0.3715484\n",
      " 0.05578302 0.01428735 0.09554599 0.28458467 0.09386963 0.01347571\n",
      " 0.3997609  0.08987896 0.01268916 0.30390176 0.0880284  0.18967973\n",
      " 0.03557147 0.29712272]:\n",
      "returns value loss [ 6.5665493 13.863257   9.795305   7.1759834  8.347417  13.050851\n",
      "  8.348192  12.54842    5.214142  11.662191   1.0096625  6.5876937\n",
      "  5.9076343  2.8474658 10.211569   5.9739833  6.604846  12.266767\n",
      " 11.331704  14.039044  10.602      5.923982   5.248235   1.\n",
      "  1.         4.4775515 10.981968   4.481375  14.65043    8.871563\n",
      "  7.169953   5.913425  10.978236  13.257217  14.513539   1.9548339\n",
      "  5.2151756 13.031908   9.344678   1.         7.246688  12.532835\n",
      "  1.9651368  3.6968915 14.209017   9.291829  12.257817   2.8575249\n",
      " 11.334854   7.207284 ]:\n",
      "loss pre mean [ 43.08141   191.82555    95.5146     43.243122   65.178375  169.68301\n",
      "  68.136055  153.64662    25.217728  134.43105     0.8094972  40.37581\n",
      "  34.69598     6.782652  102.381256   32.478058   41.288685  144.81508\n",
      " 128.00183   194.10292   107.82933    31.325914   25.560148    0.5755473\n",
      "   0.2522962  18.231602  117.81853    20.002764  200.56985    78.35642\n",
      "  49.34465    33.790524  112.1187    173.50223   210.38188     2.5067928\n",
      "  26.619335  169.45845    85.54645     0.5118191  51.162807  156.73436\n",
      "   2.4504013  13.010539  201.5357     80.78284   148.10378     7.1173983\n",
      " 127.6738     47.750324 ]:\n",
      "values value loss [0.00707584 0.01746234 0.02669131 0.6466815  0.29958582 0.02887903\n",
      " 0.10246439 0.16780312 0.2097082  0.07560589 0.12126304 0.25375324\n",
      " 0.0233719  0.26038226 0.1033702  0.29699057 0.1946852  0.25466013\n",
      " 0.02187186 0.11742816 0.2358887  0.35459483 0.20883062 0.26352307\n",
      " 0.53678787 0.22333245 0.13896479 0.01560718 0.52966213 0.02345711\n",
      " 0.16025959 0.11121312 0.4170512  0.09571939 0.01346332 0.4022543\n",
      " 0.06621554 0.01862622 0.1058207  0.3039739  0.10248468 0.01754235\n",
      " 0.4314413  0.10193784 0.01728477 0.3289262  0.09880345 0.20857175\n",
      " 0.04067693 0.32318196]:\n",
      "returns value loss [ 6.5665493 13.863257   9.795305   7.1759834  8.347417  13.050851\n",
      "  8.348192  12.54842    5.214142  11.662191   1.0096625  6.5876937\n",
      "  5.9076343  2.8474658 10.211569   5.9739833  6.604846  12.266767\n",
      " 11.331704  14.039044  10.602      5.923982   5.248235   1.\n",
      "  1.         4.4775515 10.981968   4.481375  14.65043    8.871563\n",
      "  7.169953   5.913425  10.978236  13.257217  14.513539   1.9548339\n",
      "  5.2151756 13.031908   9.344678   1.         7.246688  12.532835\n",
      "  1.9651368  3.6968915 14.209017   9.291829  12.257817   2.8575249\n",
      " 11.334854   7.207284 ]:\n",
      "loss pre mean [ 43.02669    191.70602     95.42582     42.631786    64.76758\n",
      " 169.57175     67.99202    153.27968     25.044355   134.24895\n",
      "   0.78925365  40.1188      34.624546     6.6930013  102.17567\n",
      "  32.22825     41.090164   144.2907     127.912315   193.8114\n",
      " 107.456276    31.018076    25.395597     0.5423982    0.21456549\n",
      "  18.09838    117.57072     19.943083   199.39607     78.28897\n",
      "  49.1358      33.66566    111.53863    173.22504    210.25221\n",
      "   2.4105031   26.51179    169.3455      85.35648      0.4844523\n",
      "  51.03964    156.63257      2.3522217   12.923693   201.40527\n",
      "  80.33363    147.84161      7.016953   127.55843     47.390858  ]:\n",
      "term policy loss 1 [ 1.665317    5.003439    8.646418    0.77707946  6.090876    3.492872\n",
      "  7.375756   13.043183   12.121861   11.953054    7.5462923  13.104925\n",
      " 11.615667    1.9143835  11.813232    1.6340712   9.196705    1.9490889\n",
      "  3.5765288   3.466245   10.131014   10.498043    7.1499953   2.8393977\n",
      "  7.0425496  10.587785    7.42116     7.699138    9.356591   12.957518\n",
      "  3.8204195   8.75588     9.38882     1.7615122   3.536333   13.462507\n",
      "  2.5726953   3.1785388   7.566781    4.042901   10.931856   14.339128\n",
      "  1.7817506  11.220283    0.7113867   6.4961867   9.722576    7.6441426\n",
      "  5.158365    9.122542  ]:\n",
      "term policy loss 2 [ 1.665317   5.003439   8.646418   0.7330819  5.8861713  3.492872\n",
      "  7.375756  13.043183  12.121861  11.953054   7.5462923 13.104925\n",
      " 11.615667   1.959145  11.813232   1.6340712  9.196705   1.9490889\n",
      "  3.5765288  3.466245  10.131014  10.498043   7.1499953  2.8393977\n",
      "  7.0425496 10.587785   7.42116    7.699138   9.356591  12.957518\n",
      "  3.8204195  8.75588    9.38882    1.7615122  3.536333  13.462507\n",
      "  2.5726953  3.1785388  7.566781   4.042901  10.931856  14.339128\n",
      "  1.9205964 11.220283   0.7113867  6.4961867  9.722576   7.6441426\n",
      "  5.158365   9.122542 ]:\n",
      "loss before mean [ 1.665317   5.003439   8.646418   0.7330819  5.8861713  3.492872\n",
      "  7.375756  13.043183  12.121861  11.953054   7.5462923 13.104925\n",
      " 11.615667   1.9143835 11.813232   1.6340712  9.196705   1.9490889\n",
      "  3.5765288  3.466245  10.131014  10.498043   7.1499953  2.8393977\n",
      "  7.0425496 10.587785   7.42116    7.699138   9.356591  12.957518\n",
      "  3.8204195  8.75588    9.38882    1.7615122  3.536333  13.462507\n",
      "  2.5726953  3.1785388  7.566781   4.042901  10.931856  14.339128\n",
      "  1.7817506 11.220283   0.7113867  6.4961867  9.722576   7.6441426\n",
      "  5.158365   9.122542 ]:\n",
      "term policy loss 1 [ 1.661501   5.0506525  8.654914   0.7791109  6.111803   3.4991798\n",
      "  7.3729234 13.092361  12.109953  11.952054   7.6559205 13.098116\n",
      " 11.665754   1.9056363 11.848543   1.6375421  9.151932   1.949583\n",
      "  3.5647147  3.436349  10.14059   10.493776   7.1630707  2.871808\n",
      "  7.1178513 10.582768   7.4083734  7.7010217  9.431549  12.944145\n",
      "  3.7725265  8.759608   9.423887   1.7541304  3.5252519 13.463825\n",
      "  2.5709069  3.193389   7.5064874  4.0208507 10.980088  14.396514\n",
      "  1.7741599 11.139146   0.7126259  6.4957137  9.727763   7.6586585\n",
      "  5.095609   9.127442 ]:\n",
      "term policy loss 2 [ 1.661501   5.0506525  8.654914   0.7330819  5.8861713  3.4991798\n",
      "  7.3729234 13.092361  12.109953  11.952054   7.6559205 13.098116\n",
      " 11.665754   1.959145  11.848543   1.6375421  9.151932   1.949583\n",
      "  3.5647147  3.436349  10.14059   10.493776   7.1630707  2.871808\n",
      "  7.1178513 10.582768   7.4083734  7.7010217  9.431549  12.944145\n",
      "  3.7725265  8.759608   9.423887   1.7541304  3.5252519 13.463825\n",
      "  2.5709069  3.193389   7.5064874  4.0208507 10.980088  14.396514\n",
      "  1.9205964 11.139146   0.7126259  6.4957137  9.727763   7.6586585\n",
      "  5.095609   9.127442 ]:\n",
      "loss before mean [ 1.661501   5.0506525  8.654914   0.7330819  5.8861713  3.4991798\n",
      "  7.3729234 13.092361  12.109953  11.952054   7.6559205 13.098116\n",
      " 11.665754   1.9056363 11.848543   1.6375421  9.151932   1.949583\n",
      "  3.5647147  3.436349  10.14059   10.493776   7.1630707  2.871808\n",
      "  7.1178513 10.582768   7.4083734  7.7010217  9.431549  12.944145\n",
      "  3.7725265  8.759608   9.423887   1.7541304  3.5252519 13.463825\n",
      "  2.5709069  3.193389   7.5064874  4.0208507 10.980088  14.396514\n",
      "  1.7741599 11.139146   0.7126259  6.4957137  9.727763   7.6586585\n",
      "  5.095609   9.127442 ]:\n",
      "values value loss [0.3162536  0.2891784  0.10969644 0.42278314 0.39341137 0.20664294\n",
      " 0.14464897 0.08202861 0.1103465  0.01937167 0.39521763 0.17268828\n",
      " 0.102358   0.43722257 0.33197016 0.35000145 0.11653664 0.01079705\n",
      " 0.30649784 0.23213346 0.10378956 0.02557687 0.20025532 0.3300804\n",
      " 0.2799974  0.02068498 0.2346706  0.05841514 0.2069478  0.11340431\n",
      " 0.30343598 0.02505565 0.42801106 0.19696487 0.15196866 0.01634697\n",
      " 0.11295143 0.58739054 0.1905752  0.48925108 0.10717318 0.0910457\n",
      " 0.49302375 0.1647581  0.31500745 0.07152217 0.02107542 0.20600352\n",
      " 0.28697145 0.23518865]:\n",
      "returns value loss [ 1.9597603  5.221419   8.364264   1.         5.2694826  3.6751423\n",
      "  8.291905  12.809362  12.787799  11.969214   7.7931585 13.275544\n",
      " 11.658356   2.8522348 10.1932745  1.9524468  9.346216   1.9550297\n",
      "  4.465848   3.712486  10.213582  10.61055    6.6321454  2.8679993\n",
      "  7.2207294 10.608594   8.805035   7.750537   8.864313  13.67161\n",
      "  4.502233   8.856115   9.754009   1.954587   3.6900759 13.4726305\n",
      "  2.8389924  3.709507   7.8038535  4.5157347 10.980265  14.364208\n",
      "  2.856913  11.971645   1.         6.5569863  9.7934     7.8201566\n",
      "  5.9454956  9.3302555]:\n",
      "loss pre mean [  2.7011144  24.326996   68.13788     0.3331793  23.776073   12.030488\n",
      "  66.37779   161.98503   160.71779   142.79874    54.729534  171.68483\n",
      " 133.54109     5.832285   97.24532     2.567831   85.18697     3.7800407\n",
      "  17.300192   12.112855  102.20791   112.041664   41.36921     6.441033\n",
      "  48.17376   112.10381    73.45114    59.16874    74.94998   183.82494\n",
      "  17.629898   77.98761    86.97424     3.0892355  12.518202  181.07156\n",
      "   7.4312987   9.747612   57.96201    16.21257   118.22412   203.72318\n",
      "   5.587972  139.40259     0.4692148  42.061245   95.49833    57.975323\n",
      "  32.018894   82.720245 ]:\n",
      "values value loss [0.34301814 0.30991906 0.1186945  0.45667222 0.42176464 0.22396441\n",
      " 0.15818667 0.0902663  0.11997785 0.02354435 0.42717984 0.18933515\n",
      " 0.112797   0.472015   0.35666093 0.37775782 0.12790178 0.01589006\n",
      " 0.3308613  0.24898492 0.11298159 0.0300646  0.21591148 0.353915\n",
      " 0.30225387 0.02421596 0.25370604 0.06591754 0.22509658 0.12344643\n",
      " 0.32659987 0.02842394 0.45882353 0.20841083 0.16171487 0.02052071\n",
      " 0.12355575 0.6305643  0.20611373 0.52493966 0.11769554 0.10126532\n",
      " 0.530081   0.17914025 0.34125862 0.08039941 0.02448609 0.22204389\n",
      " 0.30969825 0.2565575 ]:\n",
      "returns value loss [ 1.9597603  5.221419   8.364264   1.         5.2694826  3.6751423\n",
      "  8.291905  12.809362  12.787799  11.969214   7.7931585 13.275544\n",
      " 11.658356   2.8522348 10.1932745  1.9524468  9.346216   1.9550297\n",
      "  4.465848   3.712486  10.213582  10.61055    6.6321454  2.8679993\n",
      "  7.2207294 10.608594   8.805035   7.750537   8.864313  13.67161\n",
      "  4.502233   8.856115   9.754009   1.954587   3.6900759 13.4726305\n",
      "  2.8389924  3.709507   7.8038535  4.5157347 10.980265  14.364208\n",
      "  2.856913  11.971645   1.         6.5569863  9.7934     7.8201566\n",
      "  5.9454956  9.3302555]:\n",
      "loss pre mean [  2.6138551   24.122831    67.98941      0.29520512  23.500368\n",
      "  11.910628    66.15738    161.7754     160.4737     142.69904\n",
      "  54.25764    171.24887    133.29993      5.6654468   96.758965\n",
      "   2.4796455   84.97732      3.7602625   17.098116    11.995841\n",
      " 102.022125   111.94667     41.16806      6.32062     47.865307\n",
      " 112.02906     73.12522     59.053375    74.63606    183.55273\n",
      "  17.435911    77.92813     86.40048      3.0491312   12.449332\n",
      " 180.95924      7.3735967    9.479889    57.725647    15.926446\n",
      " 117.9954     203.43155      5.4141474  139.06319      0.43394017\n",
      "  41.94618     95.43166     57.731316    31.762213    82.33199   ]:\n",
      "term policy loss 1 [ 0.70220613 14.816945   10.956376   13.295507    2.661235    8.654914\n",
      "  1.3266681   1.5109136   4.9710016  13.599589    4.5080714   7.408372\n",
      " 14.523865   10.14059     8.523762   12.637114    6.337091    3.883963\n",
      " 12.273757    3.436349    5.053238   12.20877     7.1630707   3.193389\n",
      "  1.8254137  11.665754    5.182623    6.563716   13.282608    5.739565\n",
      "  1.7741599   6.269349    1.9854577   4.0175385  13.73697     0.48020157\n",
      "  0.6065059   3.5252519   6.0996485  13.018131    0.84175164  9.033426\n",
      "  6.6995044   5.587381    8.924105    7.5228953   6.111803   11.49082\n",
      " 13.014436    7.667901  ]:\n",
      "term policy loss 2 [ 0.70220613 14.816945   10.956376   13.295507    2.661235    8.654914\n",
      "  1.3266681   1.5109136   4.9710016  13.599589    4.5080714   7.408372\n",
      " 14.523865   10.14059     8.523762   12.637114    6.337091    3.883963\n",
      " 12.273757    3.436349    5.053238   12.20877     7.1630707   3.193389\n",
      "  1.8254137  11.665754    5.2607603   6.563716   13.282608    5.739565\n",
      "  1.9205964   6.269349    1.9854577   4.0175385  13.73697     0.48020157\n",
      "  0.6065059   3.5252519   6.0996485  13.018131    0.84175164  9.033426\n",
      "  6.6995044   5.587381    8.924105    7.5228953   5.8861713  11.49082\n",
      " 13.014436    7.667901  ]:\n",
      "loss before mean [ 0.70220613 14.816945   10.956376   13.295507    2.661235    8.654914\n",
      "  1.3266681   1.5109136   4.9710016  13.599589    4.5080714   7.408372\n",
      " 14.523865   10.14059     8.523762   12.637114    6.337091    3.883963\n",
      " 12.273757    3.436349    5.053238   12.20877     7.1630707   3.193389\n",
      "  1.8254137  11.665754    5.182623    6.563716   13.282608    5.739565\n",
      "  1.7741599   6.269349    1.9854577   4.0175385  13.73697     0.48020157\n",
      "  0.6065059   3.5252519   6.0996485  13.018131    0.84175164  9.033426\n",
      "  6.6995044   5.587381    8.924105    7.5228953   5.8861713  11.49082\n",
      " 13.014436    7.667901  ]:\n",
      "term policy loss 1 [ 0.70048803 14.863396   10.920192   13.258518    2.650182    8.677654\n",
      "  1.3174361   1.5025551   4.9595394  13.634821    4.494404    7.3745737\n",
      " 14.510893   10.164657    8.502847   12.6282425   6.3182316   3.89277\n",
      " 12.285028    3.42808     5.049653   12.184699    7.191683    3.2215283\n",
      "  1.826511   11.676642    5.135925    6.569031   13.29438     5.7609525\n",
      "  1.7559291   6.2472005   1.9873507   4.010734   13.725443    0.476731\n",
      "  0.601383    3.5245755   6.0743313  13.027948    0.83940995  9.084143\n",
      "  6.7360034   5.610914    8.915131    7.541005    6.15324    11.506263\n",
      " 13.003638    7.6388927 ]:\n",
      "term policy loss 2 [ 0.70048803 14.863396   10.920192   13.258518    2.650182    8.677654\n",
      "  1.3174361   1.5025551   4.9595394  13.634821    4.494404    7.3745737\n",
      " 14.510893   10.164657    8.502847   12.6282425   6.3182316   3.89277\n",
      " 12.285028    3.42808     5.049653   12.184699    7.191683    3.2215283\n",
      "  1.826511   11.676642    5.2607603   6.569031   13.29438     5.7609525\n",
      "  1.9205964   6.2472005   1.9873507   4.010734   13.725443    0.476731\n",
      "  0.601383    3.5245755   6.0743313  13.027948    0.83940995  9.084143\n",
      "  6.7360034   5.610914    8.915131    7.541005    5.8861713  11.506263\n",
      " 13.003638    7.6388927 ]:\n",
      "loss before mean [ 0.70048803 14.863396   10.920192   13.258518    2.650182    8.677654\n",
      "  1.3174361   1.5025551   4.9595394  13.634821    4.494404    7.3745737\n",
      " 14.510893   10.164657    8.502847   12.6282425   6.3182316   3.89277\n",
      " 12.285028    3.42808     5.049653   12.184699    7.191683    3.2215283\n",
      "  1.826511   11.676642    5.135925    6.569031   13.29438     5.7609525\n",
      "  1.7559291   6.2472005   1.9873507   4.010734   13.725443    0.476731\n",
      "  0.601383    3.5245755   6.0743313  13.027948    0.83940995  9.084143\n",
      "  6.7360034   5.610914    8.915131    7.541005    5.8861713  11.506263\n",
      " 13.003638    7.6388927 ]:\n",
      "values value loss [0.32321286 0.18933515 0.275772   0.12781087 0.22778173 0.1186945\n",
      " 0.37775782 0.46234205 0.22486553 0.12344643 0.30991906 0.25370604\n",
      " 0.01795218 0.11298159 0.22509658 0.0902663  0.17532077 0.11416709\n",
      " 0.10921757 0.24898492 0.07670766 0.18226688 0.21591148 0.6305643\n",
      " 0.20841083 0.112797   0.69215363 0.01131216 0.10582853 0.3814943\n",
      " 0.530081   0.30225387 0.01589006 0.23865801 0.10126532 0.57489353\n",
      " 0.45667222 0.16171487 0.34938154 0.03311044 0.13266906 0.35424086\n",
      " 0.31900343 0.22708668 0.1159308  0.11127463 0.42176464 0.04575976\n",
      " 0.02289706 0.2565575 ]:\n",
      "returns value loss [ 1.        13.275544  12.266767  14.039044   2.8575249  8.364264\n",
      "  1.9524468  1.9651368  5.248235  13.67161    5.221419   8.805035\n",
      " 14.513539  10.213582   8.864313  12.809362   7.169953   3.6968915\n",
      " 12.257817   3.712486   5.2151756 12.54842    6.6321454  3.709507\n",
      "  1.954587  11.658356   7.1759834  6.5665493 13.257217   5.923982\n",
      "  2.856913   7.2207294  1.9550297  4.4775515 14.364208   1.\n",
      "  1.         3.6900759  7.207284  13.050851   1.0096625  9.291829\n",
      "  5.9739833  5.214142   9.344678   7.246688   5.2694826 11.334854\n",
      " 13.031908   9.3302555]:\n",
      "loss pre mean [4.58040833e-01 1.71248871e+02 1.43783951e+02 1.93522430e+02\n",
      " 6.91554880e+00 6.79894104e+01 2.47964549e+00 2.25839210e+00\n",
      " 2.52342434e+01 1.83552734e+02 2.41228313e+01 7.31252213e+01\n",
      " 2.10122055e+02 1.02022125e+02 7.46360626e+01 1.61775406e+02\n",
      " 4.89248810e+01 1.28359156e+01 1.47588470e+02 1.19958410e+01\n",
      " 2.64038506e+01 1.52921738e+02 4.11680603e+01 9.47988892e+00\n",
      " 3.04913116e+00 1.33299927e+02 4.20400505e+01 4.29711342e+01\n",
      " 1.72959030e+02 3.07191696e+01 5.41414738e+00 4.78653069e+01\n",
      " 3.76026249e+00 1.79682178e+01 2.03431549e+02 1.80715501e-01\n",
      " 2.95205116e-01 1.24493322e+01 4.70308266e+01 1.69461563e+02\n",
      " 7.69117475e-01 7.98804932e+01 3.19787960e+01 2.48707199e+01\n",
      " 8.51697769e+01 5.09141197e+01 2.35003681e+01 1.27443642e+02\n",
      " 1.69234375e+02 8.23319931e+01]:\n",
      "values value loss [0.3424868  0.20694576 0.29701772 0.13790764 0.24778265 0.12775654\n",
      " 0.40595478 0.49371025 0.24153763 0.13344601 0.3305959  0.27310386\n",
      " 0.02249767 0.12214732 0.2434738  0.09853236 0.19038904 0.12662804\n",
      " 0.11948725 0.26590714 0.08732209 0.19679713 0.23199168 0.67395663\n",
      " 0.2199361  0.12325606 0.7381023  0.0154515  0.11623513 0.40877587\n",
      " 0.56815755 0.32475218 0.02110364 0.2540602  0.11148292 0.6135778\n",
      " 0.49135298 0.17153186 0.37592182 0.03736272 0.14424485 0.38010225\n",
      " 0.3420391  0.24502647 0.12607442 0.12058751 0.45117864 0.050866\n",
      " 0.02723276 0.27805108]:\n",
      "returns value loss [ 1.        13.275544  12.266767  14.039044   2.8575249  8.364264\n",
      "  1.9524468  1.9651368  5.248235  13.67161    5.221419   8.805035\n",
      " 14.513539  10.213582   8.864313  12.809362   7.169953   3.6968915\n",
      " 12.257817   3.712486   5.2151756 12.54842    6.6321454  3.709507\n",
      "  1.954587  11.658356   7.1759834  6.5665493 13.257217   5.923982\n",
      "  2.856913   7.2207294  1.9550297  4.4775515 14.364208   1.\n",
      "  1.         3.6900759  7.207284  13.050851   1.0096625  9.291829\n",
      "  5.9739833  5.214142   9.344678   7.246688   5.2694826 11.334854\n",
      " 13.031908   9.3302555]:\n",
      "loss pre mean [4.32323605e-01 1.70788269e+02 1.43274872e+02 1.93241592e+02\n",
      " 6.81075430e+00 6.78400574e+01 2.39163780e+00 2.16509581e+00\n",
      " 2.50670223e+01 1.83281891e+02 2.39201488e+01 7.27938461e+01\n",
      " 2.09990280e+02 1.01837051e+02 7.43188705e+01 1.61565186e+02\n",
      " 4.87143097e+01 1.27467804e+01 1.47339066e+02 1.18789062e+01\n",
      " 2.62948799e+01 1.52562576e+02 4.09619675e+01 9.21456623e+00\n",
      " 3.00901365e+00 1.33058533e+02 4.14463120e+01 4.29168816e+01\n",
      " 1.72685425e+02 3.04175014e+01 5.23840141e+00 4.75545006e+01\n",
      " 3.74007010e+00 1.78378773e+01 2.03140182e+02 1.49322122e-01\n",
      " 2.58721799e-01 1.23801517e+01 4.66675110e+01 1.69350861e+02\n",
      " 7.48947740e-01 7.94188766e+01 3.17187958e+01 2.46921062e+01\n",
      " 8.49826431e+01 5.07813072e+01 2.32160549e+01 1.27328384e+02\n",
      " 1.69121567e+02 8.19423981e+01]:\n",
      "term policy loss 1 [ 6.957254   11.384045    0.76439655 10.407201    8.548507   11.810997\n",
      "  5.800184    4.473887    6.858707   10.588895    9.692334    7.3537493\n",
      " 12.077914   11.292404   10.790131    3.7580614   8.853058    6.134231\n",
      "  2.8815887   9.87098     6.506372    7.2820415   6.306764    2.6490698\n",
      "  5.076604    2.0160186  14.179418   10.219178    7.490318    7.6906343\n",
      "  7.8353558   8.7536745  10.3073     13.8273115   0.7160343   2.4852111\n",
      " 11.962462    3.5136943   9.721635    3.9783137  10.5898285  11.117983\n",
      " 13.464886    1.213482    2.747588   12.539662    9.306692    6.2853584\n",
      "  9.747447    3.5408409 ]:\n",
      "term policy loss 2 [ 6.957254   11.384045    0.76439655 10.407201    8.548507   11.810997\n",
      "  5.800184    4.473887    6.858707   10.588895    9.692334    7.3537493\n",
      " 12.077914   11.292404   10.790131    3.7580614   8.853058    6.134231\n",
      "  2.8815887   9.87098     6.506372    7.4859643   6.306764    2.6490698\n",
      "  5.076604    2.0033414  14.179418   10.219178    7.490318    7.6906343\n",
      "  7.8353558   8.7536745  11.329815   13.8273115   0.7160343   2.4852111\n",
      " 11.962462    3.5136943   9.721635    3.9783137  10.5898285  11.117983\n",
      " 13.464886    1.2666284   2.747588   12.539662    9.306692    6.2853584\n",
      "  9.747447    3.5408409 ]:\n",
      "loss before mean [ 6.957254   11.384045    0.76439655 10.407201    8.548507   11.810997\n",
      "  5.800184    4.473887    6.858707   10.588895    9.692334    7.3537493\n",
      " 12.077914   11.292404   10.790131    3.7580614   8.853058    6.134231\n",
      "  2.8815887   9.87098     6.506372    7.2820415   6.306764    2.6490698\n",
      "  5.076604    2.0033414  14.179418   10.219178    7.490318    7.6906343\n",
      "  7.8353558   8.7536745  10.3073     13.8273115   0.7160343   2.4852111\n",
      " 11.962462    3.5136943   9.721635    3.9783137  10.5898285  11.117983\n",
      " 13.464886    1.213482    2.747588   12.539662    9.306692    6.2853584\n",
      "  9.747447    3.5408409 ]:\n",
      "term policy loss 1 [ 6.962156   11.38092     0.76353025 10.404644    8.549951   11.806865\n",
      "  5.8005795   4.475003    6.8606305  10.5902195   9.69474     7.3606696\n",
      " 12.078229   11.291105   10.791421    3.754105    8.851635    6.13372\n",
      "  2.8843832   9.869055    6.5035896   7.290136    6.3029346   2.6507158\n",
      "  5.0713305   2.013366   14.184509   10.221784    7.486321    7.6936507\n",
      "  7.8274918   8.753361   10.336319   13.832237    0.71514946  2.4822524\n",
      " 11.965787    3.5119855   9.720411    3.9842832  10.587721   11.112684\n",
      " 13.460783    1.2156703   2.746582   12.536257    9.294356    6.276547\n",
      "  9.754425    3.5436594 ]:\n",
      "term policy loss 2 [ 6.962156   11.38092     0.76353025 10.404644    8.549951   11.806865\n",
      "  5.8005795   4.475003    6.8606305  10.5902195   9.69474     7.3606696\n",
      " 12.078229   11.291105   10.791421    3.754105    8.851635    6.13372\n",
      "  2.8843832   9.869055    6.5035896   7.4859643   6.3029346   2.6507158\n",
      "  5.0713305   2.0033414  14.184509   10.221784    7.486321    7.6936507\n",
      "  7.8274918   8.753361   11.329815   13.832237    0.71514946  2.4822524\n",
      " 11.965787    3.5119855   9.720411    3.9842832  10.587721   11.112684\n",
      " 13.460783    1.2666284   2.746582   12.536257    9.294356    6.276547\n",
      "  9.754425    3.5436594 ]:\n",
      "loss before mean [ 6.962156   11.38092     0.76353025 10.404644    8.549951   11.806865\n",
      "  5.8005795   4.475003    6.8606305  10.5902195   9.69474     7.3606696\n",
      " 12.078229   11.291105   10.791421    3.754105    8.851635    6.13372\n",
      "  2.8843832   9.869055    6.5035896   7.290136    6.3029346   2.6507158\n",
      "  5.0713305   2.0033414  14.184509   10.221784    7.486321    7.6936507\n",
      "  7.8274918   8.753361   10.336319   13.832237    0.71514946  2.4822524\n",
      " 11.965787    3.5119855   9.720411    3.9842832  10.587721   11.112684\n",
      " 13.460783    1.2156703   2.746582   12.536257    9.294356    6.276547\n",
      "  9.754425    3.5436594 ]:\n",
      "values value loss [0.29260832 0.03076888 0.30829197 0.12844206 0.12016966 0.27269852\n",
      " 0.03555255 0.02898271 0.2382007  0.02763806 0.13919458 0.17199688\n",
      " 0.12983257 0.09146362 0.16201645 0.3505525  0.03116986 0.1326153\n",
      " 0.37791348 0.036187   0.08946206 0.49087656 0.22638956 0.29460284\n",
      " 0.33257627 0.37063855 0.02629058 0.12340518 0.22170764 0.07356185\n",
      " 0.34970608 0.03180674 0.6135607  0.02593909 0.36840457 0.50733507\n",
      " 0.02796455 0.24154025 0.02788093 0.560786   0.03463302 0.193593\n",
      " 0.02467793 0.46541426 0.13431503 0.02589357 0.47130936 0.45933527\n",
      " 0.38214886 0.35555196]:\n",
      "returns value loss [ 6.5876937 11.331704   1.        10.980265   8.348192  10.602\n",
      "  5.9076343  4.481375   7.8201566 10.608594   9.346216   8.291905\n",
      " 12.787799  11.662191  10.981968   4.502233   8.871563   5.913425\n",
      "  2.8679993  9.795305   6.5569863  9.754009   6.604846   2.8474658\n",
      "  5.9454956  1.9597603 14.209017  10.211569   7.8038535  7.750537\n",
      "  8.347417   8.856115  14.65043   13.863257   1.         2.8522348\n",
      " 11.969214   3.6751423  9.7934     4.5157347 10.61055   11.971645\n",
      " 13.4726305  1.9548339  2.8389924 12.532835  10.978236   7.7931585\n",
      " 10.1932745  4.465848 ]:\n",
      "loss pre mean [ 39.6281     127.71113      0.47845998 117.76206     67.700356\n",
      " 106.69447     34.481342    19.8238      57.486057   111.95662\n",
      "  84.76925     65.932915   160.22412    133.88173    117.07134\n",
      "  17.23645     78.15255     33.417763     6.2005277   95.24039\n",
      "  41.82887     85.80563     40.684708     6.517109    31.504864\n",
      "   2.5253081  201.14972    101.77104     57.488934    58.935947\n",
      "  63.963375    77.868416   197.03369    191.47137      0.3989128\n",
      "   5.498554   142.59344     11.789623    95.365364    15.641619\n",
      " 111.85003    138.72252    180.84743      2.2183707    7.3152795\n",
      " 156.4236     110.39551     53.78496     96.25819     16.894535  ]:\n",
      "values value loss [0.31261533 0.03573392 0.33081487 0.13923359 0.12915859 0.29148912\n",
      " 0.04179078 0.03592997 0.2545805  0.03110817 0.15057698 0.1860289\n",
      " 0.13966677 0.09937385 0.17346999 0.37466976 0.03510253 0.14309052\n",
      " 0.40272468 0.04144912 0.09866568 0.52385205 0.24259144 0.31184262\n",
      " 0.35562387 0.39866507 0.0307156  0.1334275  0.2373662  0.08135219\n",
      " 0.3754726  0.03525646 0.65633976 0.03018025 0.39653125 0.5427278\n",
      " 0.03259468 0.2591136  0.03151269 0.5980201  0.03982568 0.20804405\n",
      " 0.02887902 0.49811465 0.14504886 0.03020635 0.49857813 0.4923627\n",
      " 0.40800142 0.3807022 ]:\n",
      "returns value loss [ 6.5876937 11.331704   1.        10.980265   8.348192  10.602\n",
      "  5.9076343  4.481375   7.8201566 10.608594   9.346216   8.291905\n",
      " 12.787799  11.662191  10.981968   4.502233   8.871563   5.913425\n",
      "  2.8679993  9.795305   6.5569863  9.754009   6.604846   2.8474658\n",
      "  5.9454956  1.9597603 14.209017  10.211569   7.8038535  7.750537\n",
      "  8.347417   8.856115  14.65043   13.863257   1.         2.8522348\n",
      " 11.969214   3.6751423  9.7934     4.5157347 10.61055   11.971645\n",
      " 13.4726305  1.9548339  2.8389924 12.532835  10.978236   7.7931585\n",
      " 10.1932745  4.465848 ]:\n",
      "loss pre mean [ 39.376606   127.59894      0.44780877 117.527954    67.552505\n",
      " 106.30663     34.40812     19.761982    57.23794    111.88321\n",
      "  84.55979     65.70524    159.97525    133.69875    116.82364\n",
      "  17.03678     78.08303     33.296757     6.0775785   95.1377\n",
      "  41.709904    85.19579     40.478283     6.429384    31.246668\n",
      "   2.4370184  201.02422    101.56893     57.25173     58.816395\n",
      "  63.551895    77.807556   195.83456    191.35403      0.36417457\n",
      "   5.3338223  142.4829      11.669252    95.29445     15.348488\n",
      " 111.74022    138.38231    180.73445      2.1220307    7.2573314\n",
      " 156.31572    109.823235    53.301624    95.75156     16.688417  ]:\n",
      "Episode: 10 Average Return: 21.3\n",
      "Ave policy loss -7.4918814 Ave value loss 72.83945\n",
      "term policy loss 1 [ 9.587469   17.851469   12.697281   16.251602    9.216943   17.312443\n",
      " 15.786728   16.09296     5.7074695  13.043573    4.384968   16.982391\n",
      " 14.625089    8.628231    3.6509879  13.696851   10.33754    16.034065\n",
      "  2.5460744  10.127275   12.745297   14.975081   13.50018     3.7840471\n",
      "  4.620895   12.122806    9.742762   15.930535    9.855373   11.017286\n",
      "  1.4233487  10.531544   15.789132   16.330898    5.8309436   6.2837386\n",
      " 15.751078   15.502851   11.967522    5.001728    7.1885033   4.3804617\n",
      "  5.196479    0.34922734 10.770617   17.73066    11.88644     1.515188\n",
      "  2.7380435  15.318079  ]:\n",
      "term policy loss 2 [ 9.587469   17.851469   12.697281   16.251602    9.216943   17.312443\n",
      " 15.786728   16.09296     5.7074695  13.043573    4.384968   16.982391\n",
      " 14.625089    8.628231    3.6509879  13.696851   11.212606   16.034065\n",
      "  2.529665   10.127275   12.745297   14.975081   13.50018     3.7840471\n",
      "  4.620895   12.122806    9.82964    15.930535    9.855373   11.017286\n",
      "  1.3122329  10.531544   15.789132   16.330898    5.8309436   6.2837386\n",
      " 15.751078   15.502851   11.967522    5.001728    7.1885033   4.3804617\n",
      "  5.196479    0.34922734 10.903726   17.73066    11.88644     1.515188\n",
      "  2.7380435  15.318079  ]:\n",
      "loss before mean [ 9.587469   17.851469   12.697281   16.251602    9.216943   17.312443\n",
      " 15.786728   16.09296     5.7074695  13.043573    4.384968   16.982391\n",
      " 14.625089    8.628231    3.6509879  13.696851   10.33754    16.034065\n",
      "  2.529665   10.127275   12.745297   14.975081   13.50018     3.7840471\n",
      "  4.620895   12.122806    9.742762   15.930535    9.855373   11.017286\n",
      "  1.3122329  10.531544   15.789132   16.330898    5.8309436   6.2837386\n",
      " 15.751078   15.502851   11.967522    5.001728    7.1885033   4.3804617\n",
      "  5.196479    0.34922734 10.770617   17.73066    11.88644     1.515188\n",
      "  2.7380435  15.318079  ]:\n",
      "term policy loss 1 [ 9.569494   17.84241    12.705259   16.243273    9.2336645  17.32607\n",
      " 15.800353   16.104618    5.7195096  13.057885    4.3945665  16.995565\n",
      " 14.644898    8.645408    3.652256   13.689842   10.347566   16.022932\n",
      "  2.5484958  10.118579   12.740391   14.977209   13.486924    3.7808926\n",
      "  4.614714   12.129394    9.747234   15.94565     9.864281   11.02556\n",
      "  1.424095   10.5404825  15.776816   16.339062    5.842498    6.2721496\n",
      " 15.740306   15.517435   11.999912    4.9972243   7.1970468   4.377703\n",
      "  5.1909804   0.35022232 10.775388   17.720652   11.903197    1.519122\n",
      "  2.7405257  15.321808  ]:\n",
      "term policy loss 2 [ 9.569494   17.84241    12.705259   16.243273    9.2336645  17.32607\n",
      " 15.800353   16.104618    5.7195096  13.057885    4.3945665  16.995565\n",
      " 14.644898    8.645408    3.652256   13.689842   11.212606   16.022932\n",
      "  2.529665   10.118579   12.740391   14.977209   13.486924    3.7808926\n",
      "  4.614714   12.129394    9.82964    15.94565     9.864281   11.02556\n",
      "  1.3122329  10.5404825  15.776816   16.339062    5.842498    6.2721496\n",
      " 15.740306   15.517435   11.999912    4.9972243   7.1970468   4.377703\n",
      "  5.1909804   0.35022232 10.903726   17.720652   11.903197    1.519122\n",
      "  2.7405257  15.321808  ]:\n",
      "loss before mean [ 9.569494   17.84241    12.705259   16.243273    9.2336645  17.32607\n",
      " 15.800353   16.104618    5.7195096  13.057885    4.3945665  16.995565\n",
      " 14.644898    8.645408    3.652256   13.689842   10.347566   16.022932\n",
      "  2.529665   10.118579   12.740391   14.977209   13.486924    3.7808926\n",
      "  4.614714   12.129394    9.747234   15.94565     9.864281   11.02556\n",
      "  1.3122329  10.5404825  15.776816   16.339062    5.842498    6.2721496\n",
      " 15.740306   15.517435   11.999912    4.9972243   7.1970468   4.377703\n",
      "  5.1909804   0.35022232 10.775388   17.720652   11.903197    1.519122\n",
      "  2.7405257  15.321808  ]:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 69\u001b[0m\n\u001b[1;32m     66\u001b[0m policy_loss \u001b[38;5;241m=\u001b[39m ppo_policy_loss(policy_params\u001b[38;5;241m=\u001b[39mpolicy_params, minibatch\u001b[38;5;241m=\u001b[39mtrain_minibatch)\n\u001b[1;32m     67\u001b[0m policy_losses\u001b[38;5;241m.\u001b[39mappend(policy_loss)\n\u001b[0;32m---> 69\u001b[0m value_params, value_optimiser_state \u001b[38;5;241m=\u001b[39m \u001b[43mupdate_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue_optimiser_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_minibatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m value_loss \u001b[38;5;241m=\u001b[39m ppo_value_loss(value_params\u001b[38;5;241m=\u001b[39mvalue_params, minibatch\u001b[38;5;241m=\u001b[39mtrain_minibatch)\n\u001b[1;32m     75\u001b[0m value_losses\u001b[38;5;241m.\u001b[39mappend(value_loss)\n",
      "Cell \u001b[0;32mIn[16], line 13\u001b[0m, in \u001b[0;36mupdate_value\u001b[0;34m(value_params, value_optimiser_state, minibatch)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_value\u001b[39m(value_params, value_optimiser_state, minibatch):\n\u001b[0;32m---> 13\u001b[0m     grads \u001b[38;5;241m=\u001b[39m \u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mppo_value_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margnums\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mminibatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     updates, new_val_optimiser_state \u001b[38;5;241m=\u001b[39m value_optimiser\u001b[38;5;241m.\u001b[39mupdate(grads, value_optimiser_state)\n\u001b[1;32m     15\u001b[0m     new_value_params \u001b[38;5;241m=\u001b[39m optax\u001b[38;5;241m.\u001b[39mapply_updates(value_params, updates)\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/centralised-agents/lib/python3.8/site-packages/jax/_src/api.py:1089\u001b[0m, in \u001b[0;36mgrad.<locals>.grad_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1086\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(fun, docstr\u001b[38;5;241m=\u001b[39mdocstr, argnums\u001b[38;5;241m=\u001b[39margnums)\n\u001b[1;32m   1087\u001b[0m \u001b[38;5;129m@api_boundary\u001b[39m\n\u001b[1;32m   1088\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgrad_f\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m-> 1089\u001b[0m   _, g \u001b[38;5;241m=\u001b[39m \u001b[43mvalue_and_grad_f\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1090\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m g\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/centralised-agents/lib/python3.8/site-packages/jax/_src/api.py:1165\u001b[0m, in \u001b[0;36mvalue_and_grad.<locals>.value_and_grad_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1163\u001b[0m   _check_input_dtype_grad(holomorphic, allow_int, leaf)\n\u001b[1;32m   1164\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_aux:\n\u001b[0;32m-> 1165\u001b[0m   ans, vjp_py \u001b[38;5;241m=\u001b[39m \u001b[43m_vjp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf_partial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdyn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreduce_axes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1166\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1167\u001b[0m   ans, vjp_py, aux \u001b[38;5;241m=\u001b[39m _vjp(\n\u001b[1;32m   1168\u001b[0m       f_partial, \u001b[38;5;241m*\u001b[39mdyn_args, has_aux\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, reduce_axes\u001b[38;5;241m=\u001b[39mreduce_axes)\n",
      "File \u001b[0;32m~/miniconda3/envs/centralised-agents/lib/python3.8/site-packages/jax/_src/api.py:2654\u001b[0m, in \u001b[0;36m_vjp\u001b[0;34m(fun, has_aux, reduce_axes, *primals)\u001b[0m\n\u001b[1;32m   2652\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_aux:\n\u001b[1;32m   2653\u001b[0m   flat_fun, out_tree \u001b[38;5;241m=\u001b[39m flatten_fun_nokwargs(fun, in_tree)\n\u001b[0;32m-> 2654\u001b[0m   out_primal, out_vjp \u001b[38;5;241m=\u001b[39m \u001b[43mad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvjp\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2655\u001b[0m \u001b[43m      \u001b[49m\u001b[43mflat_fun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprimals_flat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreduce_axes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2656\u001b[0m   out_tree \u001b[38;5;241m=\u001b[39m out_tree()\n\u001b[1;32m   2657\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/centralised-agents/lib/python3.8/site-packages/jax/interpreters/ad.py:135\u001b[0m, in \u001b[0;36mvjp\u001b[0;34m(traceable, primals, has_aux, reduce_axes)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvjp\u001b[39m(traceable, primals, has_aux\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, reduce_axes\u001b[38;5;241m=\u001b[39m()):\n\u001b[1;32m    134\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_aux:\n\u001b[0;32m--> 135\u001b[0m     out_primals, pvals, jaxpr, consts \u001b[38;5;241m=\u001b[39m \u001b[43mlinearize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraceable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprimals\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    137\u001b[0m     out_primals, pvals, jaxpr, consts, aux \u001b[38;5;241m=\u001b[39m linearize(traceable, \u001b[38;5;241m*\u001b[39mprimals, has_aux\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/centralised-agents/lib/python3.8/site-packages/jax/interpreters/ad.py:124\u001b[0m, in \u001b[0;36mlinearize\u001b[0;34m(traceable, *primals, **kwargs)\u001b[0m\n\u001b[1;32m    122\u001b[0m _, in_tree \u001b[38;5;241m=\u001b[39m tree_flatten(((primals, primals), {}))\n\u001b[1;32m    123\u001b[0m jvpfun_flat, out_tree \u001b[38;5;241m=\u001b[39m flatten_fun(jvpfun, in_tree)\n\u001b[0;32m--> 124\u001b[0m jaxpr, out_pvals, consts \u001b[38;5;241m=\u001b[39m \u001b[43mpe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace_to_jaxpr_nounits\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjvpfun_flat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_pvals\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m out_primals_pvals, out_tangents_pvals \u001b[38;5;241m=\u001b[39m tree_unflatten(out_tree(), out_pvals)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(out_primal_pval\u001b[38;5;241m.\u001b[39mis_known() \u001b[38;5;28;01mfor\u001b[39;00m out_primal_pval \u001b[38;5;129;01min\u001b[39;00m out_primals_pvals)\n",
      "File \u001b[0;32m~/miniconda3/envs/centralised-agents/lib/python3.8/site-packages/jax/_src/profiler.py:314\u001b[0m, in \u001b[0;36mannotate_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    313\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m TraceAnnotation(name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdecorator_kwargs):\n\u001b[0;32m--> 314\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    315\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m wrapper\n",
      "File \u001b[0;32m~/miniconda3/envs/centralised-agents/lib/python3.8/site-packages/jax/interpreters/partial_eval.py:767\u001b[0m, in \u001b[0;36mtrace_to_jaxpr_nounits\u001b[0;34m(fun, pvals, instantiate)\u001b[0m\n\u001b[1;32m    765\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m core\u001b[38;5;241m.\u001b[39mnew_main(JaxprTrace, name_stack\u001b[38;5;241m=\u001b[39mcurrent_name_stack) \u001b[38;5;28;01mas\u001b[39;00m main:\n\u001b[1;32m    766\u001b[0m   fun \u001b[38;5;241m=\u001b[39m trace_to_subjaxpr_nounits(fun, main, instantiate)\n\u001b[0;32m--> 767\u001b[0m   jaxpr, (out_pvals, consts, env) \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_wrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpvals\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    768\u001b[0m   \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m env\n\u001b[1;32m    769\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m main, fun, env\n",
      "File \u001b[0;32m~/miniconda3/envs/centralised-agents/lib/python3.8/site-packages/jax/linear_util.py:167\u001b[0m, in \u001b[0;36mWrappedFun.call_wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m gen \u001b[38;5;241m=\u001b[39m gen_static_args \u001b[38;5;241m=\u001b[39m out_store \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 167\u001b[0m   ans \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m    169\u001b[0m   \u001b[38;5;66;03m# Some transformations yield from inside context managers, so we have to\u001b[39;00m\n\u001b[1;32m    170\u001b[0m   \u001b[38;5;66;03m# interrupt them before reraising the exception. Otherwise they will only\u001b[39;00m\n\u001b[1;32m    171\u001b[0m   \u001b[38;5;66;03m# get garbage-collected at some later time, running their cleanup tasks\u001b[39;00m\n\u001b[1;32m    172\u001b[0m   \u001b[38;5;66;03m# only after this exception is handled, which can corrupt the global\u001b[39;00m\n\u001b[1;32m    173\u001b[0m   \u001b[38;5;66;03m# state.\u001b[39;00m\n\u001b[1;32m    174\u001b[0m   \u001b[38;5;28;01mwhile\u001b[39;00m stack:\n",
      "Cell \u001b[0;32mIn[15], line 10\u001b[0m, in \u001b[0;36mppo_value_loss\u001b[0;34m(value_params, minibatch)\u001b[0m\n\u001b[1;32m      7\u001b[0m key, train_key \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39msplit(key)\n\u001b[1;32m      8\u001b[0m batch_train_key \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39msplit(train_key, \u001b[38;5;28mlen\u001b[39m(states))\n\u001b[0;32m---> 10\u001b[0m new_values \u001b[38;5;241m=\u001b[39m \u001b[43mvmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_axes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m jax\u001b[38;5;241m.\u001b[39mdebug\u001b[38;5;241m.\u001b[39mprint(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues value loss \u001b[39m\u001b[38;5;132;01m{x}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m, x\u001b[38;5;241m=\u001b[39mnew_values)\n\u001b[1;32m     16\u001b[0m jax\u001b[38;5;241m.\u001b[39mdebug\u001b[38;5;241m.\u001b[39mprint(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturns value loss \u001b[39m\u001b[38;5;132;01m{y}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m, y\u001b[38;5;241m=\u001b[39mreturns)\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/centralised-agents/lib/python3.8/site-packages/jax/_src/api.py:1680\u001b[0m, in \u001b[0;36mvmap.<locals>.vmap_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1677\u001b[0m in_axes_flat \u001b[38;5;241m=\u001b[39m flatten_axes(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvmap in_axes\u001b[39m\u001b[38;5;124m\"\u001b[39m, in_tree, (in_axes, \u001b[38;5;241m0\u001b[39m), kws\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1678\u001b[0m axis_size_ \u001b[38;5;241m=\u001b[39m (axis_size \u001b[38;5;28;01mif\u001b[39;00m axis_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m\n\u001b[1;32m   1679\u001b[0m               _mapped_axis_size(fun, in_tree, args_flat, in_axes_flat, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvmap\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m-> 1680\u001b[0m out_flat \u001b[38;5;241m=\u001b[39m \u001b[43mbatching\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1681\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_fun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_size_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_axes_flat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1682\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mflatten_axes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvmap out_axes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_axes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1683\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspmd_axis_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspmd_axis_name\u001b[49m\n\u001b[1;32m   1684\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_wrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs_flat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1685\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tree_unflatten(out_tree(), out_flat)\n",
      "File \u001b[0;32m~/miniconda3/envs/centralised-agents/lib/python3.8/site-packages/jax/linear_util.py:167\u001b[0m, in \u001b[0;36mWrappedFun.call_wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m gen \u001b[38;5;241m=\u001b[39m gen_static_args \u001b[38;5;241m=\u001b[39m out_store \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 167\u001b[0m   ans \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m    169\u001b[0m   \u001b[38;5;66;03m# Some transformations yield from inside context managers, so we have to\u001b[39;00m\n\u001b[1;32m    170\u001b[0m   \u001b[38;5;66;03m# interrupt them before reraising the exception. Otherwise they will only\u001b[39;00m\n\u001b[1;32m    171\u001b[0m   \u001b[38;5;66;03m# get garbage-collected at some later time, running their cleanup tasks\u001b[39;00m\n\u001b[1;32m    172\u001b[0m   \u001b[38;5;66;03m# only after this exception is handled, which can corrupt the global\u001b[39;00m\n\u001b[1;32m    173\u001b[0m   \u001b[38;5;66;03m# state.\u001b[39;00m\n\u001b[1;32m    174\u001b[0m   \u001b[38;5;28;01mwhile\u001b[39;00m stack:\n",
      "Cell \u001b[0;32mIn[13], line 9\u001b[0m, in \u001b[0;36mget_value\u001b[0;34m(observation, value_params)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_value\u001b[39m(observation, value_params):\n\u001b[1;32m      5\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Given an observation, returns the action to take in the environment, \\\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03m       the related log probability, the state value, and the distribution entropy.\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;03m       \"\"\"\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[43mvalue_network\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m jnp\u001b[38;5;241m.\u001b[39msqueeze(value)\n",
      "File \u001b[0;32m~/miniconda3/envs/centralised-agents/lib/python3.8/site-packages/haiku/_src/multi_transform.py:298\u001b[0m, in \u001b[0;36mwithout_apply_rng.<locals>.apply_fn\u001b[0;34m(params, *args, **kwargs)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_fn\u001b[39m(params, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    297\u001b[0m   check_rng_kwarg(kwargs)\n\u001b[0;32m--> 298\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/centralised-agents/lib/python3.8/site-packages/haiku/_src/transform.py:128\u001b[0m, in \u001b[0;36mwithout_state.<locals>.apply_fn\u001b[0;34m(params, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstate\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[1;32m    122\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    123\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHaiku transform adds three arguments (params, state, rng) to apply. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    124\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf the functions you are transforming use the same names you must \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    125\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpass them positionally (e.g. `f.apply(.., my_state)` and not by \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    126\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname (e.g. `f.apply(.., state=my_state)`)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 128\u001b[0m out, state \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state:\n\u001b[1;32m    130\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf your transformed function uses `hk.\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124mget,set}_state` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    131\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthen use `hk.transform_with_state`.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/centralised-agents/lib/python3.8/site-packages/haiku/_src/transform.py:357\u001b[0m, in \u001b[0;36mtransform_with_state.<locals>.apply_fn\u001b[0;34m(params, state, rng, *args, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m base\u001b[38;5;241m.\u001b[39mnew_context(params\u001b[38;5;241m=\u001b[39mparams, state\u001b[38;5;241m=\u001b[39mstate, rng\u001b[38;5;241m=\u001b[39mrng) \u001b[38;5;28;01mas\u001b[39;00m ctx:\n\u001b[1;32m    356\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 357\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    358\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m jax\u001b[38;5;241m.\u001b[39merrors\u001b[38;5;241m.\u001b[39mUnexpectedTracerError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m jax\u001b[38;5;241m.\u001b[39merrors\u001b[38;5;241m.\u001b[39mUnexpectedTracerError(unexpected_tracer_hint) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[11], line 26\u001b[0m, in \u001b[0;36mvalue_fn\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     20\u001b[0m x \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mastype(jnp\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     21\u001b[0m mlp \u001b[38;5;241m=\u001b[39m hk\u001b[38;5;241m.\u001b[39mSequential([\n\u001b[1;32m     22\u001b[0m     hk\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;241m64\u001b[39m), jax\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mrelu,\n\u001b[1;32m     23\u001b[0m     hk\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;241m64\u001b[39m), jax\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mrelu,\n\u001b[1;32m     24\u001b[0m     hk\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m     25\u001b[0m ])\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/centralised-agents/lib/python3.8/site-packages/haiku/_src/module.py:426\u001b[0m, in \u001b[0;36mwrap_method.<locals>.wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    423\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m method_name \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__call__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    424\u001b[0m     f \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mnamed_call(f, name\u001b[38;5;241m=\u001b[39mmethod_name)\n\u001b[0;32m--> 426\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;66;03m# Module names are set in the constructor. If `f` is the constructor then\u001b[39;00m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;66;03m# its name will only be set **after** `f` has run. For methods other\u001b[39;00m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;66;03m# than `__init__` we need the name before running in order to wrap their\u001b[39;00m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;66;03m# execution with `named_call`.\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m module_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/centralised-agents/lib/python3.8/contextlib.py:75\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 75\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/centralised-agents/lib/python3.8/site-packages/haiku/_src/module.py:272\u001b[0m, in \u001b[0;36mrun_interceptors\u001b[0;34m(bound_method, method_name, self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Runs any method interceptors or the original method.\"\"\"\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m interceptor_stack:\n\u001b[0;32m--> 272\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbound_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    274\u001b[0m ctx \u001b[38;5;241m=\u001b[39m MethodContext(module\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    275\u001b[0m                     method_name\u001b[38;5;241m=\u001b[39mmethod_name,\n\u001b[1;32m    276\u001b[0m                     orig_method\u001b[38;5;241m=\u001b[39mbound_method)\n\u001b[1;32m    277\u001b[0m interceptor_stack_copy \u001b[38;5;241m=\u001b[39m interceptor_stack\u001b[38;5;241m.\u001b[39mclone()\n",
      "File \u001b[0;32m~/miniconda3/envs/centralised-agents/lib/python3.8/site-packages/haiku/_src/basic.py:125\u001b[0m, in \u001b[0;36mSequential.__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m     out \u001b[38;5;241m=\u001b[39m layer(out, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    124\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 125\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/centralised-agents/lib/python3.8/site-packages/jax/_src/custom_derivatives.py:238\u001b[0m, in \u001b[0;36mcustom_jvp.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    235\u001b[0m flat_fun, out_type1 \u001b[38;5;241m=\u001b[39m _flatten_fun_nokwargs(f_, in_tree)\n\u001b[1;32m    236\u001b[0m flat_jvp, out_type2 \u001b[38;5;241m=\u001b[39m _flatten_jvp(jvp, primal_name, jvp_name, in_tree,\n\u001b[1;32m    237\u001b[0m                                    out_type1)\n\u001b[0;32m--> 238\u001b[0m out_flat \u001b[38;5;241m=\u001b[39m \u001b[43mcustom_jvp_call_p\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflat_fun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflat_jvp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs_flat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m _, (out_tree, _) \u001b[38;5;241m=\u001b[39m lu\u001b[38;5;241m.\u001b[39mmerge_linear_aux(out_type1, out_type2)\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tree_unflatten(out_tree, out_flat)\n",
      "File \u001b[0;32m~/miniconda3/envs/centralised-agents/lib/python3.8/site-packages/jax/_src/custom_derivatives.py:338\u001b[0m, in \u001b[0;36mCustomJVPCallPrimitive.bind\u001b[0;34m(self, fun, jvp, *args)\u001b[0m\n\u001b[1;32m    335\u001b[0m jvp, env_trace_todo2 \u001b[38;5;241m=\u001b[39m process_env_traces(\n\u001b[1;32m    336\u001b[0m     jvp, \u001b[38;5;28mself\u001b[39m, top_trace \u001b[38;5;129;01mand\u001b[39;00m top_trace\u001b[38;5;241m.\u001b[39mlevel, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    337\u001b[0m tracers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(top_trace\u001b[38;5;241m.\u001b[39mfull_raise, args)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m--> 338\u001b[0m outs \u001b[38;5;241m=\u001b[39m \u001b[43mtop_trace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_custom_jvp_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjvp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtracers\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    339\u001b[0m _, env_trace_todo \u001b[38;5;241m=\u001b[39m lu\u001b[38;5;241m.\u001b[39mmerge_linear_aux(env_trace_todo1, env_trace_todo2)\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _apply_todos(env_trace_todo, \u001b[38;5;28mmap\u001b[39m(core\u001b[38;5;241m.\u001b[39mfull_lower, outs))\n",
      "File \u001b[0;32m~/miniconda3/envs/centralised-agents/lib/python3.8/site-packages/jax/interpreters/batching.py:452\u001b[0m, in \u001b[0;36mBatchTrace.process_custom_jvp_call\u001b[0;34m(self, prim, fun, jvp, tracers)\u001b[0m\n\u001b[1;32m    450\u001b[0m fun, out_dims1 \u001b[38;5;241m=\u001b[39m batch_subtrace(fun, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmain, in_dims)\n\u001b[1;32m    451\u001b[0m jvp, out_dims2 \u001b[38;5;241m=\u001b[39m batch_custom_jvp_subtrace(jvp, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmain, in_dims)\n\u001b[0;32m--> 452\u001b[0m out_vals \u001b[38;5;241m=\u001b[39m \u001b[43mprim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjvp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43min_vals\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m fst, out_dims \u001b[38;5;241m=\u001b[39m lu\u001b[38;5;241m.\u001b[39mmerge_linear_aux(out_dims1, out_dims2)\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fst:\n",
      "File \u001b[0;32m~/miniconda3/envs/centralised-agents/lib/python3.8/site-packages/jax/_src/custom_derivatives.py:338\u001b[0m, in \u001b[0;36mCustomJVPCallPrimitive.bind\u001b[0;34m(self, fun, jvp, *args)\u001b[0m\n\u001b[1;32m    335\u001b[0m jvp, env_trace_todo2 \u001b[38;5;241m=\u001b[39m process_env_traces(\n\u001b[1;32m    336\u001b[0m     jvp, \u001b[38;5;28mself\u001b[39m, top_trace \u001b[38;5;129;01mand\u001b[39;00m top_trace\u001b[38;5;241m.\u001b[39mlevel, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    337\u001b[0m tracers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(top_trace\u001b[38;5;241m.\u001b[39mfull_raise, args)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m--> 338\u001b[0m outs \u001b[38;5;241m=\u001b[39m \u001b[43mtop_trace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_custom_jvp_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjvp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtracers\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    339\u001b[0m _, env_trace_todo \u001b[38;5;241m=\u001b[39m lu\u001b[38;5;241m.\u001b[39mmerge_linear_aux(env_trace_todo1, env_trace_todo2)\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _apply_todos(env_trace_todo, \u001b[38;5;28mmap\u001b[39m(core\u001b[38;5;241m.\u001b[39mfull_lower, outs))\n",
      "File \u001b[0;32m~/miniconda3/envs/centralised-agents/lib/python3.8/site-packages/jax/interpreters/ad.py:375\u001b[0m, in \u001b[0;36mJVPTrace.process_custom_jvp_call\u001b[0;34m(self, _, __, f_jvp, tracers)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;66;03m# Cast float0 to zeros with the primal dtype because custom jvp rules don't\u001b[39;00m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;66;03m# currently handle float0s\u001b[39;00m\n\u001b[1;32m    374\u001b[0m tangents_in \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(replace_float0s, primals_in, tangents_in)\n\u001b[0;32m--> 375\u001b[0m outs \u001b[38;5;241m=\u001b[39m \u001b[43mf_jvp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_wrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprimals_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtangents_in\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    376\u001b[0m primals_out, tangents_out \u001b[38;5;241m=\u001b[39m split_list(outs, [\u001b[38;5;28mlen\u001b[39m(outs) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m])\n\u001b[1;32m    377\u001b[0m tangents_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(recast_to_float0, primals_out, tangents_out)\n",
      "File \u001b[0;32m~/miniconda3/envs/centralised-agents/lib/python3.8/site-packages/jax/linear_util.py:167\u001b[0m, in \u001b[0;36mWrappedFun.call_wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m gen \u001b[38;5;241m=\u001b[39m gen_static_args \u001b[38;5;241m=\u001b[39m out_store \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 167\u001b[0m   ans \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m    169\u001b[0m   \u001b[38;5;66;03m# Some transformations yield from inside context managers, so we have to\u001b[39;00m\n\u001b[1;32m    170\u001b[0m   \u001b[38;5;66;03m# interrupt them before reraising the exception. Otherwise they will only\u001b[39;00m\n\u001b[1;32m    171\u001b[0m   \u001b[38;5;66;03m# get garbage-collected at some later time, running their cleanup tasks\u001b[39;00m\n\u001b[1;32m    172\u001b[0m   \u001b[38;5;66;03m# only after this exception is handled, which can corrupt the global\u001b[39;00m\n\u001b[1;32m    173\u001b[0m   \u001b[38;5;66;03m# state.\u001b[39;00m\n\u001b[1;32m    174\u001b[0m   \u001b[38;5;28;01mwhile\u001b[39;00m stack:\n",
      "File \u001b[0;32m~/miniconda3/envs/centralised-agents/lib/python3.8/site-packages/jax/_src/custom_derivatives.py:207\u001b[0m, in \u001b[0;36mcustom_jvp.defjvps.<locals>.jvp\u001b[0;34m(primals, tangents)\u001b[0m\n\u001b[1;32m    205\u001b[0m primal_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(\u001b[38;5;241m*\u001b[39mprimals)\n\u001b[1;32m    206\u001b[0m zeros \u001b[38;5;241m=\u001b[39m _zeros_like_pytree(primal_out)\n\u001b[0;32m--> 207\u001b[0m all_tangents_out \u001b[38;5;241m=\u001b[39m [jvp(t, primal_out, \u001b[38;5;241m*\u001b[39mprimals) \u001b[38;5;28;01mif\u001b[39;00m jvp \u001b[38;5;28;01melse\u001b[39;00m zeros\n\u001b[1;32m    208\u001b[0m                     \u001b[38;5;28;01mfor\u001b[39;00m t, jvp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(tangents, jvps)]\n\u001b[1;32m    209\u001b[0m tangent_out \u001b[38;5;241m=\u001b[39m tree_map(_sum_tangents, primal_out, \u001b[38;5;241m*\u001b[39mall_tangents_out)\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m primal_out, tangent_out\n",
      "File \u001b[0;32m~/miniconda3/envs/centralised-agents/lib/python3.8/site-packages/jax/_src/custom_derivatives.py:207\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    205\u001b[0m primal_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(\u001b[38;5;241m*\u001b[39mprimals)\n\u001b[1;32m    206\u001b[0m zeros \u001b[38;5;241m=\u001b[39m _zeros_like_pytree(primal_out)\n\u001b[0;32m--> 207\u001b[0m all_tangents_out \u001b[38;5;241m=\u001b[39m [\u001b[43mjvp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprimal_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprimals\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m jvp \u001b[38;5;28;01melse\u001b[39;00m zeros\n\u001b[1;32m    208\u001b[0m                     \u001b[38;5;28;01mfor\u001b[39;00m t, jvp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(tangents, jvps)]\n\u001b[1;32m    209\u001b[0m tangent_out \u001b[38;5;241m=\u001b[39m tree_map(_sum_tangents, primal_out, \u001b[38;5;241m*\u001b[39mall_tangents_out)\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m primal_out, tangent_out\n",
      "File \u001b[0;32m~/miniconda3/envs/centralised-agents/lib/python3.8/site-packages/jax/_src/nn/functions.py:61\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(g, ans, x)\u001b[0m\n\u001b[1;32m     59\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m jnp\u001b[38;5;241m.\u001b[39mmaximum(x, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# For behavior at 0, see https://openreview.net/forum?id=urrcVI-_jRm\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m relu\u001b[38;5;241m.\u001b[39mdefjvps(\u001b[38;5;28;01mlambda\u001b[39;00m g, ans, x: lax\u001b[38;5;241m.\u001b[39mselect(x \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m, g, \u001b[43mlax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfull_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m))\n\u001b[1;32m     63\u001b[0m \u001b[38;5;129m@jax\u001b[39m\u001b[38;5;241m.\u001b[39mjit\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msoftplus\u001b[39m(x: Array) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Array:\n\u001b[1;32m     65\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Softplus activation function.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \n\u001b[1;32m     67\u001b[0m \u001b[38;5;124;03m  Computes the element-wise function\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;124;03m    x : input array\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/centralised-agents/lib/python3.8/site-packages/jax/_src/lax/lax.py:1337\u001b[0m, in \u001b[0;36mfull_like\u001b[0;34m(x, fill_value, dtype, shape)\u001b[0m\n\u001b[1;32m   1335\u001b[0m fill_shape \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mshape(x) \u001b[38;5;28;01mif\u001b[39;00m shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m canonicalize_shape(shape)\n\u001b[1;32m   1336\u001b[0m weak_type \u001b[38;5;241m=\u001b[39m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m dtypes\u001b[38;5;241m.\u001b[39mis_weakly_typed(x)\n\u001b[0;32m-> 1337\u001b[0m dtype \u001b[38;5;241m=\u001b[39m dtype \u001b[38;5;129;01mor\u001b[39;00m \u001b[43m_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1338\u001b[0m val \u001b[38;5;241m=\u001b[39m full(fill_shape, _convert_element_type(fill_value, dtype, weak_type))\n\u001b[1;32m   1339\u001b[0m \u001b[38;5;66;03m# If the sharding is SingleDeviceSharding then don't take the `if` branch\u001b[39;00m\n\u001b[1;32m   1340\u001b[0m \u001b[38;5;66;03m# because `val` is already an array with SingleDeviceSharding making this an\u001b[39;00m\n\u001b[1;32m   1341\u001b[0m \u001b[38;5;66;03m# optimization.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1344\u001b[0m \u001b[38;5;66;03m# (so it works in staged-out code as well as 'eager' code). Related to\u001b[39;00m\n\u001b[1;32m   1345\u001b[0m \u001b[38;5;66;03m# equi-sharding.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/centralised-agents/lib/python3.8/site-packages/jax/_src/dtypes.py:459\u001b[0m, in \u001b[0;36mdtype\u001b[0;34m(x, canonicalize)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(x) \u001b[38;5;129;01min\u001b[39;00m python_scalar_dtypes:\n\u001b[1;32m    458\u001b[0m   dt \u001b[38;5;241m=\u001b[39m python_scalar_dtypes[\u001b[38;5;28mtype\u001b[39m(x)]\n\u001b[0;32m--> 459\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m jax\u001b[38;5;241m.\u001b[39mcore\u001b[38;5;241m.\u001b[39mis_opaque_dtype(\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdtype\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m):\n\u001b[1;32m    460\u001b[0m   dt \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/centralised-agents/lib/python3.8/site-packages/jax/core.py:662\u001b[0m, in \u001b[0;36mTracer.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    659\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m config\u001b[38;5;241m.\u001b[39mjax_enable_checks \u001b[38;5;129;01mor\u001b[39;00m name \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maval\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    661\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 662\u001b[0m   attr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maval\u001b[49m, name)\n\u001b[1;32m    663\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    664\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m    665\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    666\u001b[0m   ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/centralised-agents/lib/python3.8/site-packages/jax/interpreters/batching.py:272\u001b[0m, in \u001b[0;36mBatchTracer.aval\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    270\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m aval\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_dim) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m--> 272\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapped_aval\u001b[49m\u001b[43m(\u001b[49m\u001b[43maval\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_dim\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_dim) \u001b[38;5;129;01mis\u001b[39;00m ConcatAxis:\n\u001b[1;32m    274\u001b[0m   shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(aval\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/miniconda3/envs/centralised-agents/lib/python3.8/site-packages/jax/core.py:2201\u001b[0m, in \u001b[0;36mmapped_aval\u001b[0;34m(size, axis, aval)\u001b[0m\n\u001b[1;32m   2199\u001b[0m handler, _ \u001b[38;5;241m=\u001b[39m aval_mapping_handlers\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mtype\u001b[39m(aval), (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m   2200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2201\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhandler\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2202\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2203\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno mapping handler for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maval\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(aval)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/centralised-agents/lib/python3.8/site-packages/jax/core.py:2219\u001b[0m, in \u001b[0;36m_map_shaped_array\u001b[0;34m(size, axis, aval)\u001b[0m\n\u001b[1;32m   2217\u001b[0m \u001b[38;5;66;03m# TODO: Extend the named shape\u001b[39;00m\n\u001b[1;32m   2218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \u001b[38;5;28;01mreturn\u001b[39;00m aval\n\u001b[0;32m-> 2219\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mShapedArray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtuple_delete\u001b[49m\u001b[43m(\u001b[49m\u001b[43maval\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maval\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2220\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mnamed_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maval\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnamed_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweak_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maval\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweak_type\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/centralised-agents/lib/python3.8/site-packages/jax/core.py:1372\u001b[0m, in \u001b[0;36mShapedArray.__init__\u001b[0;34m(self, shape, dtype, weak_type, named_shape)\u001b[0m\n\u001b[1;32m   1371\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, shape, dtype, weak_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, named_shape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m-> 1372\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m=\u001b[39m \u001b[43mcanonicalize_shape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1373\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m=\u001b[39m _dtype_object(dtype)\n\u001b[1;32m   1374\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweak_type \u001b[38;5;241m=\u001b[39m weak_type\n",
      "File \u001b[0;32m~/miniconda3/envs/centralised-agents/lib/python3.8/site-packages/jax/core.py:1901\u001b[0m, in \u001b[0;36mcanonicalize_shape\u001b[0;34m(shape, context)\u001b[0m\n\u001b[1;32m   1892\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Canonicalizes and checks for errors in a user-provided shape value.\u001b[39;00m\n\u001b[1;32m   1893\u001b[0m \n\u001b[1;32m   1894\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1898\u001b[0m \u001b[38;5;124;03m  A tuple of canonical dimension values.\u001b[39;00m\n\u001b[1;32m   1899\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1900\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1901\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43munsafe_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_canonicalize_dimension\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1902\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   1903\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/centralised-agents/lib/python3.8/site-packages/jax/core.py:1880\u001b[0m, in \u001b[0;36m_canonicalize_dimension\u001b[0;34m(dim)\u001b[0m\n\u001b[1;32m   1877\u001b[0m   handler, ds \u001b[38;5;241m=\u001b[39m _dim_handler_and_canonical(d)\n\u001b[1;32m   1878\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m handler\u001b[38;5;241m.\u001b[39mas_value(\u001b[38;5;241m*\u001b[39mds)\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_canonicalize_dimension\u001b[39m(dim: DimSize) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DimSize:\n\u001b[1;32m   1881\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dim, Tracer) \u001b[38;5;129;01mand\u001b[39;00m config\u001b[38;5;241m.\u001b[39mjax_dynamic_shapes:\n\u001b[1;32m   1882\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dim\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training loop \n",
    "\n",
    "episode_returns = []\n",
    "policy_losses = []\n",
    "value_losses = []\n",
    "\n",
    "for episode in range(1, 10000):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    episode_return = 0\n",
    "    \n",
    "    while not done: \n",
    "        # select action\n",
    "        \n",
    "        key, action_key = random.split(buffer_state.key)\n",
    "        buffer_state.key = key\n",
    "        action, log_prob, entropy = get_action_logprob(obs, action_key, policy_params)\n",
    "        value = get_value(obs, value_params)\n",
    "        \n",
    "        obs_, reward, done, _  = env.step(action.tolist())\n",
    "        \n",
    "        \n",
    "        buffer_state = jit_add(buffer_state=buffer_state, \n",
    "                                state=obs, \n",
    "                                action=action, \n",
    "                                reward=reward, \n",
    "                                done=done, \n",
    "                                log_prob=log_prob,\n",
    "                                value=value, \n",
    "                                ) \n",
    "        \n",
    "        episode_return += reward \n",
    "        obs = obs_\n",
    "        \n",
    "        # whether should train of not \n",
    "        # followed by training logic \n",
    "        if buffer.should_train(buffer_state): \n",
    "            # Compute advantages \n",
    "            buffer_state = buffer.compute_advantages(buffer_state)\n",
    "            policy_losses = []\n",
    "            value_losses = []\n",
    "            \n",
    "            for epoch in range(NUM_EPOCHS):\n",
    "                buffer_state, minibatch_idxs = buffer.get_epoch_indices(buffer_state)\n",
    "                \n",
    "                for minibatch in minibatch_idxs: \n",
    "                    \n",
    "                    # TODO only get what is really needed.\n",
    "                    train_minibatch = MiniBatch(\n",
    "                        states=buffer_state.states[minibatch],\n",
    "                        actions=buffer_state.actions[minibatch],\n",
    "                        rewards=buffer_state.rewards[minibatch],  \n",
    "                        dones=buffer_state.dones[minibatch],\n",
    "                        log_probs=buffer_state.log_probs[minibatch],\n",
    "                        values=buffer_state.values[minibatch],\n",
    "                        advantages=buffer_state.advantages[minibatch],\n",
    "                        returns=buffer_state.returns[minibatch],\n",
    "                        key=buffer_state.key, \n",
    "                    )\n",
    "                    \n",
    "                    policy_params, policy_optimiser_state = update_policy(\n",
    "                        policy_params, \n",
    "                        policy_optimiser_state, \n",
    "                        train_minibatch,\n",
    "                    )\n",
    "                    policy_loss = ppo_policy_loss(policy_params=policy_params, minibatch=train_minibatch)\n",
    "                    policy_losses.append(policy_loss)\n",
    "                    \n",
    "                    value_params, value_optimiser_state = update_value(\n",
    "                        value_params, \n",
    "                        value_optimiser_state, \n",
    "                        train_minibatch,\n",
    "                    )\n",
    "                    value_loss = ppo_value_loss(value_params=value_params, minibatch=train_minibatch)\n",
    "                    value_losses.append(value_loss)\n",
    "                    \n",
    "                    key, new_key = random.split(train_minibatch.key)\n",
    "                    buffer_state.key = new_key\n",
    "                    \n",
    "            buffer_state = buffer.reset(buffer_state)\n",
    "        \n",
    "        \n",
    "        \n",
    "    episode_returns.append(episode_return)\n",
    "    \n",
    "    if episode % 10 == 0:\n",
    "        print(\"Episode:\", episode, \"Average Return:\", np.mean(episode_returns[-100:]))\n",
    "        print(\"Ave policy loss\", np.mean(policy_losses), \"Ave value loss\", np.mean(value_losses))\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

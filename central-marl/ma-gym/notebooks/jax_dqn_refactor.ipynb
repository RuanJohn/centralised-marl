{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff30600b-c364-4f64-9bce-50bd9fe84e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda env: centralised-agents\n",
    "\n",
    "import gym \n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "import numpy as np\n",
    "import haiku as hk\n",
    "from copy import deepcopy\n",
    "from jax import jit, grad, vmap, pmap, random\n",
    "import optax\n",
    "import chex\n",
    "import rlax\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3add549-21c2-416e-bb72-1e17f67b29e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# global hypterparameters\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 100_000\n",
    "SEED = 2022\n",
    "LEARNING_RATE = 5e-4\n",
    "DISCOUNT = 0.99\n",
    "# TARGET_UPDATE_PERIOD = 100\n",
    "TARGET_UPDATE_PERIOD = 10\n",
    "EPSILON = 1.0\n",
    "# EPSILON_EXP_DECAY = 0.99999 \n",
    "EPSILON_EXP_DECAY = 0.99995\n",
    "TRAIN_EVERY = 1\n",
    "MIN_REPLAY_SIZE = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "117f7f3e-71d2-489c-aed0-d1664f06502f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "# first state\n",
    "random_state = random.PRNGKey(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e83afa5-d84c-4c4b-b116-027e8d7ed60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CentralControllerWrapper: \n",
    "    \n",
    "    def __init__(self, ma_env):\n",
    "        \n",
    "        self.env = ma_env \n",
    "        self.num_agents = ma_env.n_agents \n",
    "        self.action_mapping = self.enumerate_agent_actions()\n",
    "        self.action_space = len(self.action_mapping)\n",
    "        self.observation_space = np.sum([len(i) for i in ma_env.reset()])\n",
    "        \n",
    "    def reset(self, ):\n",
    "        \n",
    "        obs_n = self.env.reset()\n",
    "        joint_obs = self.create_joint_obs(obs_n)\n",
    "        \n",
    "        return joint_obs\n",
    "    \n",
    "    def step(self, joint_action): \n",
    "        \n",
    "        action = self.action_mapping[joint_action]\n",
    "        obs_n, reward_n, done_n, info = self.env.step(action)\n",
    "        \n",
    "        joint_obs = self.create_joint_obs(obs_n)\n",
    "        team_reward = jnp.sum(jnp.array(reward_n))\n",
    "        team_done = all(done_n)\n",
    "        \n",
    "        return joint_obs, team_reward, team_done, info\n",
    "    \n",
    "    def random_action(self,): \n",
    "        \n",
    "        action = np.random.randint(low = 0, high = self.action_space)\n",
    "        return action \n",
    "    \n",
    "    def enumerate_agent_actions(self, ):\n",
    "        \n",
    "        agent_actions = [np.arange(self.env.action_space[i].n) for i in range(len(self.env.action_space))]\n",
    "        enumerated_actions = np.array(np.meshgrid(*agent_actions)).T.reshape(-1,self.num_agents)\n",
    "        action_mapping = {int(i): list(action) for i, action in enumerate(enumerated_actions)}\n",
    "        return action_mapping\n",
    "    \n",
    "    def create_joint_obs(self, env_obs):\n",
    "        \n",
    "        array_obs = np.array(env_obs)\n",
    "        joint_obs = np.concatenate(array_obs, axis = -1)\n",
    "        \n",
    "        return joint_obs\n",
    "    \n",
    "    def unwrapped_env(self):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28943ec9-ee1b-4f94-86ac-65764419233b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For debugging\n",
    "\n",
    "class NormalGymWrapper: \n",
    "    \n",
    "    def __init__(self, env):\n",
    "        \n",
    "        self.env = env  \n",
    "        self.action_space = env.action_space.n\n",
    "        self.observation_space = env.observation_space.shape[0]\n",
    "        \n",
    "    def reset(self, ):\n",
    "        \n",
    "        obs = self.env.reset()\n",
    "        joint_obs = np.array(obs)\n",
    "        \n",
    "        return joint_obs\n",
    "    \n",
    "    def step(self, action): \n",
    "        \n",
    "        obs, reward, done, info = self.env.step(action)\n",
    "        \n",
    "        return np.array(obs), jnp.array(reward), done, info\n",
    "    \n",
    "    def random_action(self,):\n",
    "        \n",
    "        return self.env.action_space.sample()\n",
    "   \n",
    "    def unwrapped_env(self):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4441d25-5711-4b47-812d-dd5f4b9c9966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO create one hot agetn ID wrapper. \n",
    "\n",
    "class ConcatenateAgentIDs:\n",
    "    pass\n",
    "#     super().__init()\n",
    "#     def __init__(self, wrapped_env):\n",
    "        \n",
    "#         self.num_agents = wrapped_env.num_agents\n",
    "    \n",
    "#     def create_joint_obs(self): \n",
    "        \n",
    "#         env_obs = super().create_joint_obs()\n",
    "#         array_obs = np.array(env_obs)\n",
    "        \n",
    "#         for agent in self.num_agents:\n",
    "#             one_hot = np.zeros(self.num_agents)\n",
    "#             one_hot[agent] = 1.0 \n",
    "#             array_obs[agent] = np.concatenate((one_hot, array_obs[agent]), axis=-1)\n",
    "        \n",
    "#         joint_obs = np.concatenate(array_obs, axis = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ae438b1-0be2-44a9-b208-3800b6029273",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Getting environment details \n",
    "env = gym.make('ma_gym:Checkers-v0')\n",
    "# env = CentralControllerWrapper(env)\n",
    "# env = ConcatenateAgentIDs(env)\n",
    "\n",
    "# env = gym.make(\"CartPole-v0\")\n",
    "# env = NormalGymWrapper(env)\n",
    "# num_actions     = env.action_space\n",
    "# observation_dim = env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4aeaf372-17b0-4f42-9980-4e9a6f0b4fc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Box([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
       "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], (47,), float32),\n",
       " Box([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
       "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], (47,), float32)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9684d6b5-60c0-4c16-a920-a9c96019a3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@chex.dataclass\n",
    "class BufferState: \n",
    "    states: jnp.ndarray\n",
    "    actions: jnp.ndarray \n",
    "    rewards: jnp.ndarray \n",
    "    next_states: jnp.ndarray \n",
    "    dones: jnp.ndarray \n",
    "    counter: jnp.int32 \n",
    "    key: chex.PRNGKey\n",
    "    buffer_size: jnp.int32\n",
    "    batch_size: jnp.int32 \n",
    "    min_buffer_size: jnp.int32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae38bedf-0d62-4e08-a512-a452a7110818",
   "metadata": {},
   "outputs": [],
   "source": [
    "@chex.dataclass\n",
    "class ExperienceSample: \n",
    "    states: jnp.ndarray\n",
    "    actions: jnp.ndarray \n",
    "    rewards: jnp.ndarray \n",
    "    next_states: jnp.ndarray \n",
    "    dones: jnp.ndarray "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53d1db58-e095-4da9-8724-7b0ce6dfb05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Very basic jax replay buffer\n",
    "\n",
    "class JaxTransitionBuffer: \n",
    "    \n",
    "    def create_buffer(\n",
    "        self, \n",
    "        buffer_size: int, \n",
    "        observation_dim: int,\n",
    "        min_buffer_size: int = 50, \n",
    "        batch_size: int = 32, \n",
    "        buffer_key: chex.PRNGKey = random.PRNGKey(0),\n",
    "    ) -> BufferState:\n",
    "        \n",
    "        state_buffer = jnp.empty((buffer_size, observation_dim))\n",
    "        action_buffer = jnp.empty(buffer_size)\n",
    "        reward_buffer = jnp.empty(buffer_size)\n",
    "        state_buffer_ = jnp.empty((buffer_size, observation_dim))\n",
    "        done_buffer = jnp.empty(buffer_size) \n",
    "        \n",
    "        counter = 0\n",
    "        \n",
    "        buffer_state = BufferState(\n",
    "            states = state_buffer, \n",
    "            actions = action_buffer, \n",
    "            rewards = reward_buffer, \n",
    "            next_states = state_buffer_, \n",
    "            dones = done_buffer, \n",
    "            counter = jnp.array(counter, dtype=jnp.int32), \n",
    "            key = buffer_key, \n",
    "            buffer_size = jnp.array(buffer_size, dtype=jnp.int32), \n",
    "            batch_size = jnp.array(batch_size, dtype=jnp.int32), \n",
    "            min_buffer_size = jnp.array(min_buffer_size, dtype=jnp.int32), \n",
    "        )\n",
    "        \n",
    "        return buffer_state\n",
    "    \n",
    "    def add(\n",
    "        self,\n",
    "        buffer_state, \n",
    "        state, \n",
    "        action, \n",
    "        reward, \n",
    "        done, \n",
    "        state_,\n",
    "    ) -> BufferState:\n",
    "        \n",
    "        index = buffer_state.counter % buffer_state.buffer_size\n",
    "        #x = x.at[idx].set(y)\n",
    "        buffer_state.states = buffer_state.states.at[index].set(state)\n",
    "        buffer_state.actions = buffer_state.actions.at[index].set(action)\n",
    "        buffer_state.rewards = buffer_state.rewards.at[index].set(reward)\n",
    "        buffer_state.next_states = buffer_state.next_states.at[index].set(state_)\n",
    "        buffer_state.dones = buffer_state.dones.at[index].set(done)\n",
    "        \n",
    "        buffer_state.counter += 1\n",
    "        \n",
    "        return buffer_state\n",
    "    \n",
    "    def sample(\n",
    "        self, \n",
    "        buffer_state, \n",
    "    ) -> Tuple[BufferState, ExperienceSample]:\n",
    "        \n",
    "        key, sample_key = random.split(buffer_state.key)\n",
    "        indices = random.choice(\n",
    "            sample_key, \n",
    "            min(buffer_state.counter, buffer_state.buffer_size), \n",
    "            shape=(buffer_state.batch_size,), \n",
    "            replace=False)\n",
    "        \n",
    "        buffer_state.key = key \n",
    "        \n",
    "        states = jnp.stack(buffer_state.states[indices])\n",
    "        actions = buffer_state.actions[indices]\n",
    "        rewards = buffer_state.rewards[indices]\n",
    "        states_ = jnp.stack(buffer_state.next_states[indices])\n",
    "        dones = buffer_state.dones[indices]\n",
    "        \n",
    "        sampled = ExperienceSample( \n",
    "                        states=states,\n",
    "                        actions=actions, \n",
    "                        rewards=rewards, \n",
    "                        next_states=states_, \n",
    "                        dones=dones,\n",
    "        )\n",
    "        \n",
    "        return buffer_state, sampled\n",
    "    \n",
    "    def can_sample_batch(\n",
    "        self, \n",
    "        buffer_state \n",
    "    ):\n",
    "        return np.greater_equal(buffer_state.counter, buffer_state.batch_size)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d11d0c60-741d-4934-aec7-09d5ae23a092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create buffer and jit adding \n",
    "buffer = JaxTransitionBuffer()\n",
    "buffer_state = buffer.create_buffer(\n",
    "    buffer_size=BUFFER_SIZE, \n",
    "    observation_dim=observation_dim,\n",
    ")\n",
    "\n",
    "jit_add = jit(buffer.add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9268304c-d1cb-4acd-a37a-e59d3d2af96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feedforward Q_network \n",
    "\n",
    "def net_fn(batch) -> jnp.ndarray:\n",
    "    \"\"\"Standard MLP network.\"\"\"\n",
    "    x = batch.astype(jnp.float32)\n",
    "    mlp = hk.Sequential([\n",
    "        hk.Linear(64), jax.nn.relu,\n",
    "        hk.Linear(64), jax.nn.relu,\n",
    "        hk.Linear(num_actions),\n",
    "    ])\n",
    "    return mlp(x)\n",
    "\n",
    "dummy_pass_data = jnp.ones((BATCH_SIZE, observation_dim))\n",
    "\n",
    "# initialize online and target parameters  \n",
    "q_network = hk.without_apply_rng(hk.transform(net_fn))\n",
    "online_params = q_network.init(random_state, dummy_pass_data)\n",
    "target_params = deepcopy(online_params)\n",
    "\n",
    "# Intialize optimiser and optimiser state \n",
    "optimiser = optax.adam(LEARNING_RATE)\n",
    "optimiser_state = optimiser.init(online_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0082c29d-6bdc-4307-b53a-7b505190010b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dqn_loss(online_params, target_params, batch: ExperienceSample) -> jnp.ndarray:\n",
    "    \"\"\"Compute the loss of the network\"\"\"\n",
    "    states = batch.states\n",
    "    actions = batch.actions.astype(jnp.int32)\n",
    "    rewards = batch.rewards\n",
    "    dones = batch.dones\n",
    "    states_ = batch.next_states\n",
    "    \n",
    "    # TODO not sure how rlax does discounts \n",
    "    discounts = DISCOUNT * (1 - dones)\n",
    "    \n",
    "    q_values = q_network.apply(online_params, states)\n",
    "    # Do this better. \n",
    "    selected_q_values = jnp.array([q_values[i][action] for i, action in enumerate(actions)]) \n",
    "    \n",
    "    # stopping gradients \n",
    "    rewards = jax.lax.stop_gradient(rewards)\n",
    "    dones   = jax.lax.stop_gradient(dones)\n",
    "    states_ = jax.lax.stop_gradient(states_)\n",
    "    \n",
    "    next_q_values = jax.lax.stop_gradient(q_network.apply(target_params, states_))\n",
    "    max_next_q_values = jax.lax.stop_gradient(jnp.max(next_q_values, axis = 1))\n",
    "    \n",
    "    target = jax.lax.stop_gradient(rewards + discounts * max_next_q_values)\n",
    "    td_error = selected_q_values - target\n",
    "    \n",
    "    # TODO: Figure out rlax \n",
    "    # td_error = vmap(rlax.q_learning)(\n",
    "    #     q_tm1=q_values, \n",
    "    #     a_tm1=actions, \n",
    "    #     r_t=rewards, \n",
    "    #     discount_t=dones, \n",
    "    #     q_t=next_q_values, \n",
    "    # )\n",
    "\n",
    "    loss = jnp.mean(td_error **2)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2eecca75-3432-450c-a87a-5fd3f47d8821",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "@chex.assert_max_traces(n=1)\n",
    "def update(online_params, target_params, opt_state, batch):\n",
    "    grads = jax.grad(dqn_loss, argnums=0)(online_params, target_params, batch)\n",
    "    updates, new_optimiser_state = optimiser.update(grads, optimiser_state)\n",
    "    new_online_params = optax.apply_updates(online_params, updates)\n",
    "    return new_online_params, new_optimiser_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80d1450c-9b01-435a-9434-26821b33d331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 50 Average Return: 23.14 Loss: 0.8846806 Epsilon: 0.9453014512766509\n",
      "Episode: 100 Average Return: 23.32 Loss: 1.2362819 Epsilon: 0.8913635811474366\n",
      "Episode: 150 Average Return: 21.02 Loss: 2.5116284 Epsilon: 0.8509907382125926\n",
      "Episode: 200 Average Return: 18.56 Loss: 4.5197144 Epsilon: 0.8123652739752802\n",
      "Episode: 250 Average Return: 16.79 Loss: 2.599301 Epsilon: 0.7824649634575382\n",
      "Episode: 300 Average Return: 14.73 Loss: 13.8580265 Epsilon: 0.7546833388280776\n",
      "Episode: 350 Average Return: 14.98 Loss: 22.11323 Epsilon: 0.7259980099572511\n",
      "Episode: 400 Average Return: 15.13 Loss: 21.493906 Epsilon: 0.6996962772089826\n",
      "Episode: 450 Average Return: 14.48 Loss: 46.54582 Epsilon: 0.6752921848484298\n",
      "Episode: 500 Average Return: 14.54 Loss: 40.16642 Epsilon: 0.6506322182395456\n",
      "Episode: 550 Average Return: 14.49 Loss: 36.62677 Epsilon: 0.6280963965279573\n",
      "Episode: 600 Average Return: 13.64 Loss: 161.01985 Epsilon: 0.6077373682729577\n",
      "Episode: 650 Average Return: 13.39 Loss: 122.27694 Epsilon: 0.5874211231512223\n",
      "Episode: 700 Average Return: 14.12 Loss: 183.75514 Epsilon: 0.5663096783697572\n",
      "Episode: 750 Average Return: 13.82 Loss: 58.991684 Epsilon: 0.5482000367911133\n",
      "Episode: 800 Average Return: 13.92 Loss: 58.67033 Epsilon: 0.5282339768115629\n",
      "Episode: 850 Average Return: 14.12 Loss: 18.139175 Epsilon: 0.5108308337196538\n",
      "Episode: 900 Average Return: 13.5 Loss: 281.97556 Epsilon: 0.49375410785847845\n",
      "Episode: 950 Average Return: 13.06 Loss: 343.06805 Epsilon: 0.4785385889802256\n",
      "Episode: 1000 Average Return: 13.12 Loss: 97.40144 Epsilon: 0.46240262573719426\n",
      "Episode: 1050 Average Return: 13.15 Loss: 288.34302 Epsilon: 0.44808601401851317\n",
      "Episode: 1100 Average Return: 12.06 Loss: 90.74614 Epsilon: 0.43534311412984855\n",
      "Episode: 1150 Average Return: 11.58 Loss: 12.94157 Epsilon: 0.422878017187569\n",
      "Episode: 1200 Average Return: 11.84 Loss: 476.72272 Epsilon: 0.41031822133183393\n",
      "Episode: 1250 Average Return: 11.85 Loss: 176.69254 Epsilon: 0.3985497289229939\n",
      "Episode: 1300 Average Return: 11.96 Loss: 12.458487 Epsilon: 0.3864998628553526\n",
      "Episode: 1350 Average Return: 11.94 Loss: 165.39455 Epsilon: 0.37545205807986154\n",
      "Episode: 1400 Average Return: 11.32 Loss: 416.9172 Epsilon: 0.36523102484668957\n",
      "Episode: 1450 Average Return: 11.59 Loss: 40.390945 Epsilon: 0.3543125167529805\n",
      "Episode: 1500 Average Return: 12.28 Loss: 281.92722 Epsilon: 0.34347988915470373\n",
      "Episode: 1550 Average Return: 12.2 Loss: 359.4694 Epsilon: 0.333344941536363\n",
      "Episode: 1600 Average Return: 11.75 Loss: 57.694016 Epsilon: 0.32388130118236597\n",
      "Episode: 1650 Average Return: 11.34 Loss: 152.189 Epsilon: 0.3149696852111234\n",
      "Episode: 1700 Average Return: 11.05 Loss: 244.11322 Epsilon: 0.30647179030893307\n",
      "Episode: 1750 Average Return: 10.93 Loss: 143.27959 Epsilon: 0.2982180798604118\n",
      "Episode: 1800 Average Return: 11.63 Loss: 157.68474 Epsilon: 0.2891582913777433\n",
      "Episode: 1850 Average Return: 11.86 Loss: 23.583391 Epsilon: 0.28104745889130267\n",
      "Episode: 1900 Average Return: 11.1 Loss: 109.25226 Epsilon: 0.27354684082818853\n",
      "Episode: 1950 Average Return: 10.97 Loss: 224.76526 Epsilon: 0.26604678496121\n",
      "Episode: 2000 Average Return: 10.97 Loss: 58.484436 Epsilon: 0.2589465061371792\n",
      "Episode: 2050 Average Return: 10.81 Loss: 2.105003 Epsilon: 0.25204832256372794\n",
      "Episode: 2100 Average Return: 10.81 Loss: 76.74422 Epsilon: 0.24532163587366873\n",
      "Episode: 2150 Average Return: 12.19 Loss: 43.06041 Epsilon: 0.23714441499578562\n",
      "Episode: 2200 Average Return: 16.95 Loss: 21.249096 Epsilon: 0.2253867984009478\n",
      "Episode: 2250 Average Return: 31.92 Loss: 27.87498 Epsilon: 0.2021611822998507\n",
      "Episode: 2300 Average Return: 33.7 Loss: 8.735291 Epsilon: 0.19043555009268004\n",
      "Episode: 2350 Average Return: 25.49 Loss: 17.463081 Epsilon: 0.1779694889210735\n",
      "Episode: 2400 Average Return: 34.25 Loss: 1.8981448 Epsilon: 0.16046238218791747\n",
      "Episode: 2450 Average Return: 41.81 Loss: 11.41454 Epsilon: 0.144395620726446\n",
      "Episode: 2500 Average Return: 41.95 Loss: 10.970068 Epsilon: 0.1301001179700931\n",
      "Episode: 2550 Average Return: 36.92 Loss: 40.903408 Epsilon: 0.12005527760987209\n",
      "Episode: 2600 Average Return: 35.1 Loss: 13.494762 Epsilon: 0.10915838540415775\n",
      "Episode: 2650 Average Return: 41.56 Loss: 23.03083 Epsilon: 0.09752873848772244\n",
      "Episode: 2700 Average Return: 34.61 Loss: 1.1870414 Epsilon: 0.09181223597593222\n",
      "Episode: 2750 Average Return: 31.08 Loss: 35.738846 Epsilon: 0.08349136415068897\n",
      "Episode: 2800 Average Return: 36.38 Loss: 31.458305 Epsilon: 0.07654210875145542\n",
      "Episode: 2850 Average Return: 41.37 Loss: 27.812181 Epsilon: 0.0678899513518968\n",
      "Episode: 2900 Average Return: 45.78 Loss: 31.225574 Epsilon: 0.06088186261231633\n",
      "Episode: 2950 Average Return: 37.87 Loss: 0.87091535 Epsilon: 0.05617845373177914\n",
      "Episode: 3000 Average Return: 39.47 Loss: 1.1509345 Epsilon: 0.05\n",
      "Episode: 3050 Average Return: 47.83 Loss: 22.997032 Epsilon: 0.05\n",
      "Episode: 3100 Average Return: 50.64 Loss: 53.639404 Epsilon: 0.05\n",
      "Episode: 3150 Average Return: 54.01 Loss: 27.922523 Epsilon: 0.05\n",
      "Episode: 3200 Average Return: 51.95 Loss: 71.99342 Epsilon: 0.05\n",
      "Episode: 3250 Average Return: 51.6 Loss: 21.638357 Epsilon: 0.05\n",
      "Episode: 3300 Average Return: 49.58 Loss: 50.23121 Epsilon: 0.05\n",
      "Episode: 3350 Average Return: 45.84 Loss: 17.83756 Epsilon: 0.05\n",
      "Episode: 3400 Average Return: 47.07 Loss: 25.537668 Epsilon: 0.05\n",
      "Episode: 3450 Average Return: 45.48 Loss: 0.693837 Epsilon: 0.05\n",
      "Episode: 3500 Average Return: 40.65 Loss: 26.901712 Epsilon: 0.05\n",
      "Episode: 3550 Average Return: 36.44 Loss: 61.586502 Epsilon: 0.05\n",
      "Episode: 3600 Average Return: 41.83 Loss: 81.39519 Epsilon: 0.05\n",
      "Episode: 3650 Average Return: 44.64 Loss: 79.79088 Epsilon: 0.05\n",
      "Episode: 3700 Average Return: 44.02 Loss: 34.85636 Epsilon: 0.05\n",
      "Episode: 3750 Average Return: 46.98 Loss: 34.341877 Epsilon: 0.05\n",
      "Episode: 3800 Average Return: 43.48 Loss: 69.3542 Epsilon: 0.05\n",
      "Episode: 3850 Average Return: 38.06 Loss: 44.367924 Epsilon: 0.05\n",
      "Episode: 3900 Average Return: 35.12 Loss: 40.750946 Epsilon: 0.05\n",
      "Episode: 3950 Average Return: 33.02 Loss: 125.2199 Epsilon: 0.05\n",
      "Episode: 4000 Average Return: 38.47 Loss: 43.550007 Epsilon: 0.05\n",
      "Episode: 4050 Average Return: 42.63 Loss: 66.12683 Epsilon: 0.05\n",
      "Episode: 4100 Average Return: 37.4 Loss: 2.8545215 Epsilon: 0.05\n",
      "Episode: 4150 Average Return: 38.61 Loss: 0.8447113 Epsilon: 0.05\n",
      "Episode: 4200 Average Return: 39.37 Loss: 0.6229097 Epsilon: 0.05\n",
      "Episode: 4250 Average Return: 34.56 Loss: 3.6970203 Epsilon: 0.05\n",
      "Episode: 4300 Average Return: 32.39 Loss: 55.946884 Epsilon: 0.05\n",
      "Episode: 4350 Average Return: 24.55 Loss: 187.88545 Epsilon: 0.05\n",
      "Episode: 4400 Average Return: 18.68 Loss: 123.84913 Epsilon: 0.05\n",
      "Episode: 4450 Average Return: 17.17 Loss: 116.38822 Epsilon: 0.05\n",
      "Episode: 4500 Average Return: 13.12 Loss: 28.719858 Epsilon: 0.05\n",
      "Episode: 4550 Average Return: 13.79 Loss: 114.320694 Epsilon: 0.05\n",
      "Episode: 4600 Average Return: 18.57 Loss: 4.6400776 Epsilon: 0.05\n",
      "Episode: 4650 Average Return: 18.17 Loss: 35.790966 Epsilon: 0.05\n",
      "Episode: 4700 Average Return: 19.8 Loss: 33.178917 Epsilon: 0.05\n",
      "Episode: 4750 Average Return: 27.34 Loss: 3.3777936 Epsilon: 0.05\n",
      "Episode: 4800 Average Return: 21.95 Loss: 0.30665812 Epsilon: 0.05\n",
      "Episode: 4850 Average Return: 11.94 Loss: 171.55882 Epsilon: 0.05\n",
      "Episode: 4900 Average Return: 10.17 Loss: 173.01239 Epsilon: 0.05\n",
      "Episode: 4950 Average Return: 9.92 Loss: 42.199467 Epsilon: 0.05\n",
      "Episode: 5000 Average Return: 9.74 Loss: 7.035857 Epsilon: 0.05\n",
      "Episode: 5050 Average Return: 9.62 Loss: 537.1949 Epsilon: 0.05\n",
      "Episode: 5100 Average Return: 9.57 Loss: 52.08341 Epsilon: 0.05\n",
      "Episode: 5150 Average Return: 9.57 Loss: 661.27655 Epsilon: 0.05\n",
      "Episode: 5200 Average Return: 9.62 Loss: 12.829661 Epsilon: 0.05\n",
      "Episode: 5250 Average Return: 9.57 Loss: 2.8524365 Epsilon: 0.05\n",
      "Episode: 5300 Average Return: 9.59 Loss: 734.269 Epsilon: 0.05\n",
      "Episode: 5350 Average Return: 9.7 Loss: 6.9745607 Epsilon: 0.05\n",
      "Episode: 5400 Average Return: 9.67 Loss: 983.32 Epsilon: 0.05\n",
      "Episode: 5450 Average Return: 9.68 Loss: 1932.2659 Epsilon: 0.05\n",
      "Episode: 5500 Average Return: 9.59 Loss: 160.37039 Epsilon: 0.05\n",
      "Episode: 5550 Average Return: 9.55 Loss: 1112.8401 Epsilon: 0.05\n",
      "Episode: 5600 Average Return: 9.74 Loss: 64.9198 Epsilon: 0.05\n",
      "Episode: 5650 Average Return: 9.61 Loss: 1576.7455 Epsilon: 0.05\n",
      "Episode: 5700 Average Return: 9.5 Loss: 1849.696 Epsilon: 0.05\n",
      "Episode: 5750 Average Return: 9.49 Loss: 970.9236 Epsilon: 0.05\n",
      "Episode: 5800 Average Return: 9.48 Loss: 13.30772 Epsilon: 0.05\n",
      "Episode: 5850 Average Return: 9.66 Loss: 1890.2656 Epsilon: 0.05\n",
      "Episode: 5900 Average Return: 9.61 Loss: 48.30645 Epsilon: 0.05\n",
      "Episode: 5950 Average Return: 9.48 Loss: 1803.3928 Epsilon: 0.05\n",
      "Episode: 6000 Average Return: 9.52 Loss: 2277.9368 Epsilon: 0.05\n",
      "Episode: 6050 Average Return: 9.59 Loss: 232.472 Epsilon: 0.05\n",
      "Episode: 6100 Average Return: 9.52 Loss: 21.883976 Epsilon: 0.05\n",
      "Episode: 6150 Average Return: 9.49 Loss: 1713.142 Epsilon: 0.05\n",
      "Episode: 6200 Average Return: 9.55 Loss: 1140.6609 Epsilon: 0.05\n",
      "Episode: 6250 Average Return: 9.57 Loss: 232.57564 Epsilon: 0.05\n",
      "Episode: 6300 Average Return: 10.35 Loss: 985.01514 Epsilon: 0.05\n",
      "Episode: 6350 Average Return: 10.55 Loss: 290.78796 Epsilon: 0.05\n",
      "Episode: 6400 Average Return: 9.85 Loss: 600.5778 Epsilon: 0.05\n",
      "Episode: 6450 Average Return: 9.57 Loss: 499.45795 Epsilon: 0.05\n",
      "Episode: 6500 Average Return: 9.55 Loss: 156.7877 Epsilon: 0.05\n",
      "Episode: 6550 Average Return: 9.54 Loss: 61.73711 Epsilon: 0.05\n",
      "Episode: 6600 Average Return: 9.41 Loss: 2314.6038 Epsilon: 0.05\n",
      "Episode: 6650 Average Return: 9.54 Loss: 1825.036 Epsilon: 0.05\n",
      "Episode: 6700 Average Return: 9.67 Loss: 2895.9763 Epsilon: 0.05\n",
      "Episode: 6750 Average Return: 9.46 Loss: 1007.3857 Epsilon: 0.05\n",
      "Episode: 6800 Average Return: 9.52 Loss: 497.50702 Epsilon: 0.05\n",
      "Episode: 6850 Average Return: 9.75 Loss: 38.46442 Epsilon: 0.05\n",
      "Episode: 6900 Average Return: 9.75 Loss: 24.246315 Epsilon: 0.05\n",
      "Episode: 6950 Average Return: 9.83 Loss: 1804.6886 Epsilon: 0.05\n",
      "Episode: 7000 Average Return: 9.71 Loss: 537.2115 Epsilon: 0.05\n",
      "Episode: 7050 Average Return: 9.55 Loss: 1140.6498 Epsilon: 0.05\n",
      "Episode: 7100 Average Return: 9.59 Loss: 801.38196 Epsilon: 0.05\n",
      "Episode: 7150 Average Return: 9.64 Loss: 28.01054 Epsilon: 0.05\n",
      "Episode: 7200 Average Return: 9.6 Loss: 593.9885 Epsilon: 0.05\n",
      "Episode: 7250 Average Return: 9.64 Loss: 439.57962 Epsilon: 0.05\n",
      "Episode: 7300 Average Return: 9.67 Loss: 2360.9854 Epsilon: 0.05\n",
      "Episode: 7350 Average Return: 9.55 Loss: 28.083895 Epsilon: 0.05\n",
      "Episode: 7400 Average Return: 9.55 Loss: 2731.4348 Epsilon: 0.05\n",
      "Episode: 7450 Average Return: 9.61 Loss: 208.81458 Epsilon: 0.05\n",
      "Episode: 7500 Average Return: 9.72 Loss: 2225.2268 Epsilon: 0.05\n",
      "Episode: 7550 Average Return: 9.64 Loss: 368.28308 Epsilon: 0.05\n",
      "Episode: 7600 Average Return: 9.57 Loss: 554.59467 Epsilon: 0.05\n",
      "Episode: 7650 Average Return: 9.66 Loss: 402.27148 Epsilon: 0.05\n",
      "Episode: 7700 Average Return: 9.59 Loss: 1128.4742 Epsilon: 0.05\n",
      "Episode: 7750 Average Return: 9.45 Loss: 2115.944 Epsilon: 0.05\n",
      "Episode: 7800 Average Return: 9.54 Loss: 1535.9736 Epsilon: 0.05\n",
      "Episode: 7850 Average Return: 9.82 Loss: 92.47748 Epsilon: 0.05\n",
      "Episode: 7900 Average Return: 9.79 Loss: 2459.3447 Epsilon: 0.05\n",
      "Episode: 7950 Average Return: 9.74 Loss: 592.3136 Epsilon: 0.05\n",
      "Episode: 8000 Average Return: 9.68 Loss: 816.55774 Epsilon: 0.05\n",
      "Episode: 8050 Average Return: 9.48 Loss: 372.93054 Epsilon: 0.05\n",
      "Episode: 8100 Average Return: 9.47 Loss: 49.19954 Epsilon: 0.05\n",
      "Episode: 8150 Average Return: 9.63 Loss: 72.05443 Epsilon: 0.05\n",
      "Episode: 8200 Average Return: 9.66 Loss: 399.4317 Epsilon: 0.05\n",
      "Episode: 8250 Average Return: 9.58 Loss: 305.76242 Epsilon: 0.05\n",
      "Episode: 8300 Average Return: 9.62 Loss: 949.7567 Epsilon: 0.05\n",
      "Episode: 8350 Average Return: 9.63 Loss: 1979.0217 Epsilon: 0.05\n",
      "Episode: 8400 Average Return: 9.65 Loss: 2392.9915 Epsilon: 0.05\n",
      "Episode: 8450 Average Return: 9.59 Loss: 1501.0308 Epsilon: 0.05\n",
      "Episode: 8500 Average Return: 9.6 Loss: 2502.291 Epsilon: 0.05\n",
      "Episode: 8550 Average Return: 9.63 Loss: 1123.9177 Epsilon: 0.05\n",
      "Episode: 8600 Average Return: 9.46 Loss: 1993.788 Epsilon: 0.05\n",
      "Episode: 8650 Average Return: 9.51 Loss: 2131.1482 Epsilon: 0.05\n",
      "Episode: 8700 Average Return: 9.73 Loss: 1484.8036 Epsilon: 0.05\n",
      "Episode: 8750 Average Return: 9.69 Loss: 31.790728 Epsilon: 0.05\n",
      "Episode: 8800 Average Return: 9.61 Loss: 2128.4211 Epsilon: 0.05\n",
      "Episode: 8850 Average Return: 9.61 Loss: 886.6523 Epsilon: 0.05\n",
      "Episode: 8900 Average Return: 9.49 Loss: 543.4927 Epsilon: 0.05\n",
      "Episode: 8950 Average Return: 9.54 Loss: 15.788944 Epsilon: 0.05\n",
      "Episode: 9000 Average Return: 9.66 Loss: 17.450857 Epsilon: 0.05\n",
      "Episode: 9050 Average Return: 9.68 Loss: 453.81573 Epsilon: 0.05\n",
      "Episode: 9100 Average Return: 9.71 Loss: 836.5386 Epsilon: 0.05\n",
      "Episode: 9150 Average Return: 9.67 Loss: 625.2304 Epsilon: 0.05\n",
      "Episode: 9200 Average Return: 9.52 Loss: 770.27625 Epsilon: 0.05\n",
      "Episode: 9250 Average Return: 9.47 Loss: 613.1312 Epsilon: 0.05\n",
      "Episode: 9300 Average Return: 9.51 Loss: 90.015594 Epsilon: 0.05\n",
      "Episode: 9350 Average Return: 9.37 Loss: 16.037899 Epsilon: 0.05\n",
      "Episode: 9400 Average Return: 9.41 Loss: 259.3157 Epsilon: 0.05\n",
      "Episode: 9450 Average Return: 9.68 Loss: 446.83112 Epsilon: 0.05\n",
      "Episode: 9500 Average Return: 9.77 Loss: 20.520983 Epsilon: 0.05\n",
      "Episode: 9550 Average Return: 9.72 Loss: 240.99048 Epsilon: 0.05\n",
      "Episode: 9600 Average Return: 9.7 Loss: 286.88123 Epsilon: 0.05\n",
      "Episode: 9650 Average Return: 9.8 Loss: 155.1704 Epsilon: 0.05\n",
      "Episode: 9700 Average Return: 9.88 Loss: 377.31577 Epsilon: 0.05\n",
      "Episode: 9750 Average Return: 9.91 Loss: 90.86497 Epsilon: 0.05\n",
      "Episode: 9800 Average Return: 9.7 Loss: 193.06223 Epsilon: 0.05\n",
      "Episode: 9850 Average Return: 9.57 Loss: 548.2092 Epsilon: 0.05\n",
      "Episode: 9900 Average Return: 9.59 Loss: 156.79039 Epsilon: 0.05\n",
      "Episode: 9950 Average Return: 9.59 Loss: 260.23547 Epsilon: 0.05\n"
     ]
    }
   ],
   "source": [
    "episode_returns = []\n",
    "losses = []\n",
    "for episode in range(1, 10000):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    episode_return = 0\n",
    "    \n",
    "    while not done: \n",
    "        if np.random.random() < EPSILON:\n",
    "            action = env.random_action()\n",
    "            action = jnp.array(action)\n",
    "        else:\n",
    "            action = jnp.argmax(q_network.apply(online_params, jnp.array(obs)))\n",
    "            \n",
    "        if buffer.can_sample_batch(buffer_state): \n",
    "            EPSILON = max(0.05, EPSILON*EPSILON_EXP_DECAY)\n",
    "        \n",
    "        obs_, reward, done, _  = env.step(action.tolist())\n",
    "        \n",
    "        buffer_state = jit_add(buffer_state=buffer_state, \n",
    "                                state=obs, \n",
    "                                action=action, \n",
    "                                reward=reward, \n",
    "                                done=done, \n",
    "                                state_=obs_,\n",
    "                               ) \n",
    "        episode_return += reward \n",
    "        obs = obs_\n",
    "        \n",
    "        \n",
    "    if (episode % TRAIN_EVERY == 0) and buffer.can_sample_batch(buffer_state):\n",
    "        \n",
    "        buffer_state, sampled_data = buffer.sample(buffer_state)\n",
    "        \n",
    "        # Computing loss can be done by returning aux from the update. \n",
    "        loss=dqn_loss(online_params, target_params, sampled_data)\n",
    "        \n",
    "        losses.append(loss)\n",
    "        online_params, optimiser_state = update(online_params, target_params, optimiser_state, sampled_data)\n",
    "\n",
    "    if episode % TARGET_UPDATE_PERIOD == 0:\n",
    "        \n",
    "        target_params = deepcopy(online_params)\n",
    "        \n",
    "    episode_returns.append(episode_return)\n",
    "    \n",
    "    if episode % 50 == 0:\n",
    "        print(\"Episode:\", episode, \"Average Return:\", np.mean(episode_returns[-100:]), \"Loss:\", loss, \"Epsilon:\", EPSILON)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

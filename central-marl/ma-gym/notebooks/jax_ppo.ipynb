{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1dcd3561-ac42-47ba-9cff-cac22b2be978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda env: centralised-agents\n",
    "\n",
    "import gym \n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "import numpy as np\n",
    "import haiku as hk\n",
    "from copy import deepcopy\n",
    "from jax import jit, grad, vmap, pmap, random\n",
    "import optax\n",
    "import chex\n",
    "import rlax\n",
    "from typing import Tuple\n",
    "import distrax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fa18407-3ddb-42bd-90ca-c12ce615de37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# global hypterparameters\n",
    "HORIZON = 100\n",
    "NUM_EPOCHS = 2\n",
    "NUM_MINIBATCHES = 2\n",
    "SEED = 2022\n",
    "LEARNING_RATE = 5e-4\n",
    "DISCOUNT = 0.99\n",
    "GAE_LAMBDA = 0.95 \n",
    "CLIPPING_EPSILON = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5382ac13-7a8b-49f2-863b-0b61fa97c9e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "# first state\n",
    "random_state = random.PRNGKey(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05e9a311-fe12-446e-b699-24d27dd31483",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CentralControllerWrapper: \n",
    "    \n",
    "    def __init__(self, ma_env):\n",
    "        \n",
    "        self.env = ma_env \n",
    "        self.num_agents = ma_env.n_agents \n",
    "        self.action_mapping = self.enumerate_agent_actions()\n",
    "        self.action_space = len(self.action_mapping)\n",
    "        self.observation_space = np.sum([len(i) for i in ma_env.reset()])\n",
    "        \n",
    "    def reset(self, ):\n",
    "        \n",
    "        obs_n = self.env.reset()\n",
    "        joint_obs = self.create_joint_obs(obs_n)\n",
    "        \n",
    "        return joint_obs\n",
    "    \n",
    "    def step(self, joint_action): \n",
    "        \n",
    "        action = self.action_mapping[joint_action]\n",
    "        obs_n, reward_n, done_n, info = self.env.step(action)\n",
    "        \n",
    "        joint_obs = self.create_joint_obs(obs_n)\n",
    "        team_reward = jnp.sum(jnp.array(reward_n))\n",
    "        team_done = all(done_n)\n",
    "        \n",
    "        return joint_obs, team_reward, team_done, info\n",
    "    \n",
    "    def random_action(self,): \n",
    "        \n",
    "        action = np.random.randint(low = 0, high = self.action_space)\n",
    "        return action \n",
    "    \n",
    "    def enumerate_agent_actions(self, ):\n",
    "        \n",
    "        agent_actions = [np.arange(self.env.action_space[i].n) for i in range(len(self.env.action_space))]\n",
    "        enumerated_actions = np.array(np.meshgrid(*agent_actions)).T.reshape(-1,self.num_agents)\n",
    "        action_mapping = {int(i): list(action) for i, action in enumerate(enumerated_actions)}\n",
    "        return action_mapping\n",
    "    \n",
    "    def create_joint_obs(self, env_obs):\n",
    "        \n",
    "        array_obs = np.array(env_obs)\n",
    "        joint_obs = np.concatenate(array_obs, axis = -1)\n",
    "        \n",
    "        return joint_obs\n",
    "    \n",
    "    def unwrapped_env(self):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e62e6420-096e-462c-84ef-7f2b858bf7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For debugging\n",
    "\n",
    "class NormalGymWrapper: \n",
    "    \n",
    "    def __init__(self, env):\n",
    "        \n",
    "        self.env = env  \n",
    "        self.action_space = env.action_space.n\n",
    "        self.observation_space = env.observation_space.shape[0]\n",
    "        \n",
    "    def reset(self, ):\n",
    "        \n",
    "        obs = self.env.reset()\n",
    "        joint_obs = np.array(obs)\n",
    "        \n",
    "        return joint_obs\n",
    "    \n",
    "    def step(self, action): \n",
    "        \n",
    "        obs, reward, done, info = self.env.step(action)\n",
    "        \n",
    "        return np.array(obs), jnp.array(reward), done, info\n",
    "    \n",
    "    def unwrapped_env(self):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a2ecc2d-11dd-48fa-a000-46f011fbff2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Getting environment details \n",
    "# env = gym.make('ma_gym:Switch2-v0')\n",
    "# env = CentralControllerWrapper(env)\n",
    "\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "env = NormalGymWrapper(env)\n",
    "num_actions     = env.action_space\n",
    "observation_dim = env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76347b0b-bb5d-460d-a426-4afdb816a617",
   "metadata": {},
   "outputs": [],
   "source": [
    "@chex.dataclass\n",
    "class BufferState: \n",
    "    states: jnp.ndarray\n",
    "    actions: jnp.ndarray \n",
    "    rewards: jnp.ndarray \n",
    "    dones: jnp.ndarray\n",
    "    log_probs: jnp.ndarray\n",
    "    values: jnp.ndarray\n",
    "    advantages: jnp.ndarray\n",
    "    returns: jnp.ndarray\n",
    "    counter: jnp.int32 \n",
    "    key: chex.PRNGKey\n",
    "    buffer_size: jnp.int32\n",
    "    gae_lambda: jnp.float32 \n",
    "    discount: jnp.float32\n",
    "    num_minibatches: jnp.int32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b030c215-c6a9-4452-914b-7589760aa794",
   "metadata": {},
   "outputs": [],
   "source": [
    "@chex.dataclass\n",
    "class MiniBatch:\n",
    "    states: jnp.ndarray\n",
    "    actions: jnp.ndarray \n",
    "    rewards: jnp.ndarray \n",
    "    dones: jnp.ndarray\n",
    "    log_probs: jnp.ndarray\n",
    "    values: jnp.ndarray\n",
    "    advantages: jnp.ndarray\n",
    "    returns: jnp.ndarray\n",
    "    key: chex.PRNGKey\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b23fc9f-85d7-4ced-b52b-19ef92071d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Very basic jax replay buffer\n",
    "\n",
    "class JaxTrajectoryBuffer: \n",
    "    \n",
    "    def create_buffer(\n",
    "        self, \n",
    "        buffer_size: int, \n",
    "        observation_dim: int,\n",
    "        gae_lambda: float, \n",
    "        discount: float, \n",
    "        num_minibatches: int,\n",
    "        buffer_key: chex.PRNGKey = random.PRNGKey(0),\n",
    "    ) -> BufferState:\n",
    "        \n",
    "        state_buffer = jnp.empty((buffer_size + 1, observation_dim), dtype=jnp.float32)\n",
    "        action_buffer = jnp.empty(buffer_size + 1, dtype=jnp.int32)\n",
    "        reward_buffer = jnp.empty(buffer_size + 1, dtype=jnp.float32)\n",
    "        done_buffer = jnp.empty(buffer_size + 1, dtype=bool) \n",
    "        log_probs_buffer = jnp.empty(buffer_size + 1, dtype=jnp.float32)\n",
    "        values_buffer = jnp.empty(buffer_size + 1, dtype=jnp.float32)\n",
    "        advantages_buffer = jnp.empty(buffer_size, dtype=jnp.float32)\n",
    "        returns_buffer = jnp.empty(buffer_size, dtype=jnp.float32)\n",
    "        \n",
    "        buffer_state = BufferState(\n",
    "            states = state_buffer, \n",
    "            actions = action_buffer, \n",
    "            rewards = reward_buffer, \n",
    "            dones = done_buffer, \n",
    "            log_probs=log_probs_buffer, \n",
    "            values=values_buffer,\n",
    "            advantages=advantages_buffer,\n",
    "            returns=returns_buffer, \n",
    "            counter = jnp.array(0, dtype=jnp.int32), \n",
    "            key = buffer_key, \n",
    "            buffer_size = jnp.array(buffer_size, dtype=jnp.int32), \n",
    "            gae_lambda = jnp.array(gae_lambda, dtype=jnp.float32), \n",
    "            discount = jnp.array(discount, dtype=jnp.float32),\n",
    "            num_minibatches = jnp.array(num_minibatches, dtype=jnp.int32),\n",
    "        )\n",
    "        \n",
    "        return buffer_state\n",
    "    \n",
    "    def add(\n",
    "        self,\n",
    "        buffer_state, \n",
    "        state, \n",
    "        action, \n",
    "        reward, \n",
    "        done, \n",
    "        log_prob,\n",
    "        value, \n",
    "    ) -> BufferState:\n",
    "        \n",
    "        index = buffer_state.counter\n",
    "        #x = x.at[idx].set(y)\n",
    "        buffer_state.states = buffer_state.states.at[index].set(state)\n",
    "        buffer_state.actions = buffer_state.actions.at[index].set(action)\n",
    "        buffer_state.rewards = buffer_state.rewards.at[index].set(reward)\n",
    "        buffer_state.dones = buffer_state.dones.at[index].set(done)\n",
    "        buffer_state.log_probs = buffer_state.log_probs.at[index].set(log_prob)\n",
    "        buffer_state.values = buffer_state.values.at[index].set(value)\n",
    "        \n",
    "        buffer_state.counter += 1\n",
    "        \n",
    "        return buffer_state\n",
    "    \n",
    "    def compute_advantages(\n",
    "        self, \n",
    "        buffer_state, \n",
    "    ) -> BufferState:\n",
    "        \n",
    "        # Returns array of length [0:k-1]\n",
    "        \n",
    "        advantages = rlax.truncated_generalized_advantage_estimation(\n",
    "            r_t = buffer_state.rewards[1: ], \n",
    "            discount_t = (1 - buffer_state.dones[1: ]) * buffer_state.discount, \n",
    "            lambda_ = buffer_state.gae_lambda, \n",
    "            values = buffer_state.values,\n",
    "        )\n",
    "        \n",
    "        # Don't have to add a zero just make other arrays shorter during training. \n",
    "        \n",
    "        buffer_state.advantages = advantages\n",
    "        \n",
    "        # can now get the returns by saying \n",
    "        # returns = advantages - values[0:k-1]  -> essentially Adv - V_{t}\n",
    "        returns = advantages + buffer_state.values[:-1]\n",
    "        buffer_state.returns = returns \n",
    "        \n",
    "        return buffer_state\n",
    "        \n",
    "        \n",
    "    def get_epoch_indices(\n",
    "        self, \n",
    "        buffer_state, \n",
    "    ) -> jnp.ndarray:\n",
    "        \n",
    "        key, sample_key = random.split(buffer_state.key)\n",
    "        \n",
    "        shuffled_idx = random.permutation(sample_key, buffer_state.buffer_size)\n",
    "        \n",
    "        buffer_state.key = key \n",
    "        \n",
    "        # Split indices into minibatches \n",
    "        # right now the type here is a list. might have to be cast into \n",
    "        # something else. \n",
    "        minibatch_idxs = jnp.split(shuffled_idx, buffer_state.num_minibatches)\n",
    "        \n",
    "        return buffer_state, minibatch_idxs\n",
    "    \n",
    "    def should_train(\n",
    "        self, \n",
    "        buffer_state \n",
    "    ) -> bool:\n",
    "        \n",
    "        return jnp.equal(buffer_state.counter, buffer_state.buffer_size + 1)\n",
    "    \n",
    "    def reset(\n",
    "        self, \n",
    "        buffer_state: BufferState\n",
    "    ) -> BufferState:\n",
    "        \n",
    "        state_buffer = jnp.empty((buffer_state.buffer_size + 1, observation_dim), dtype=jnp.float32)\n",
    "        action_buffer = jnp.empty(buffer_state.buffer_size + 1, dtype=jnp.int32)\n",
    "        reward_buffer = jnp.empty(buffer_state.buffer_size + 1, dtype=jnp.float32)\n",
    "        done_buffer = jnp.empty(buffer_state.buffer_size + 1, dtype=bool) \n",
    "        log_probs_buffer = jnp.empty(buffer_state.buffer_size + 1, dtype=jnp.float32)\n",
    "        values_buffer = jnp.empty(buffer_state.buffer_size + 1, dtype=jnp.float32)\n",
    "        advantages_buffer = jnp.empty(buffer_state.buffer_size, dtype=jnp.float32)\n",
    "        returns_buffer = jnp.empty(buffer_state.buffer_size, dtype=jnp.float32)\n",
    "        \n",
    "        buffer_state.states = state_buffer\n",
    "        buffer_state.actions = action_buffer\n",
    "        buffer_state.rewards = reward_buffer\n",
    "        buffer_state.dones = done_buffer\n",
    "        buffer_state.log_probs = log_probs_buffer\n",
    "        buffer_state.values = values_buffer\n",
    "        buffer_state.advantages = advantages_buffer\n",
    "        buffer_state.returns = returns_buffer\n",
    "        buffer_state.counter = jnp.array(0, dtype=jnp.int32)\n",
    "        \n",
    "        return buffer_state\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1953436b-895d-4171-adb5-46973eb9f7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create replay buffer \n",
    "buffer = JaxTrajectoryBuffer()\n",
    "buffer_state = buffer.create_buffer(\n",
    "        buffer_size = HORIZON,  \n",
    "        observation_dim = observation_dim,\n",
    "        gae_lambda = GAE_LAMBDA, \n",
    "        discount = DISCOUNT, \n",
    "        num_minibatches = NUM_MINIBATCHES,\n",
    ")\n",
    "jit_add = jax.jit(buffer.add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31608b73-86c1-434e-8efc-640d777d90b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create networks \n",
    "\n",
    "network_state, policy_init_state = random.split(random_state)\n",
    "network_state, value_init_state  = random.split(network_state)\n",
    "\n",
    "# Create feedforward policy and value network  \n",
    "\n",
    "def policy_fn(batch) -> jnp.ndarray:\n",
    "    \"\"\"Standard MLP network.\"\"\"\n",
    "    x = batch.astype(jnp.float32)\n",
    "    mlp = hk.Sequential([\n",
    "        hk.Linear(64), jax.nn.relu,\n",
    "        hk.Linear(64), jax.nn.relu,\n",
    "        hk.Linear(num_actions),\n",
    "    ])\n",
    "    return mlp(x)\n",
    "\n",
    "def value_fn(batch) -> jnp.ndarray:\n",
    "    \"\"\"Standard MLP network.\"\"\"\n",
    "    x = batch.astype(jnp.float32)\n",
    "    mlp = hk.Sequential([\n",
    "        hk.Linear(64), jax.nn.relu,\n",
    "        hk.Linear(64), jax.nn.relu,\n",
    "        hk.Linear(1),\n",
    "    ])\n",
    "    return mlp(x)\n",
    "\n",
    "dummy_pass_data = jnp.ones((1, observation_dim))\n",
    "\n",
    "# initialize policy and value parameters  \n",
    "policy_network = hk.without_apply_rng(hk.transform(policy_fn))\n",
    "policy_params  = policy_network.init(policy_init_state, dummy_pass_data)\n",
    "\n",
    "value_network = hk.without_apply_rng(hk.transform(value_fn))\n",
    "value_params  = value_network.init(value_init_state, dummy_pass_data)\n",
    "\n",
    "\n",
    "# Intialize optimisers and optimiser states \n",
    "policy_optimiser = optax.adam(LEARNING_RATE)\n",
    "policy_optimiser_state = policy_optimiser.init(policy_params)\n",
    "\n",
    "value_optimiser = optax.adam(LEARNING_RATE)\n",
    "value_optimiser_state = value_optimiser.init(value_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "683b04f1-7c78-4dd4-9fe9-969c2ef18208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @jit\n",
    "# @chex.assert_max_traces(n=1)\n",
    "def get_action_logprob(observation, action_key, policy_params):\n",
    "    \n",
    "    \"\"\"Given an observation, returns the action to take in the environment, \\\n",
    "       the related log probability, the state value, and the distribution entropy.\n",
    "       \"\"\"\n",
    "    \n",
    "    logits = policy_network.apply(policy_params, observation)\n",
    "    distribution = distrax.Categorical(logits=logits)\n",
    "    \n",
    "    action, logprob = distribution.sample_and_log_prob(\n",
    "        seed = action_key, \n",
    "    )\n",
    "    \n",
    "    entropy = distribution.entropy()\n",
    "    \n",
    "    return jnp.squeeze(action), jnp.squeeze(logprob), jnp.squeeze(entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34f8c50c-20bb-426b-8dcd-dc2445e44ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @jit\n",
    "# @chex.assert_max_traces(n=1)\n",
    "def get_value(observation, value_params):\n",
    "    \n",
    "    \"\"\"Given an observation, returns the action to take in the environment, \\\n",
    "       the related log probability, the state value, and the distribution entropy.\n",
    "       \"\"\"\n",
    "    \n",
    "    value = value_network.apply(value_params, observation)\n",
    "    \n",
    "    return jnp.squeeze(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4922ac0d-2379-489d-85d2-d2bbb1fc7a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_policy_loss(policy_params, minibatch: MiniBatch):\n",
    "    \n",
    "    # TODO: pass in somehow \n",
    "    CLIP_EPSILON = 0.2\n",
    "    \n",
    "    states = minibatch.states \n",
    "    actions = minibatch.actions\n",
    "    old_log_probs = minibatch.log_probs\n",
    "    old_values = minibatch.values \n",
    "    key = minibatch.key\n",
    "    advantages = minibatch.advantages\n",
    "    \n",
    "    key, train_key = random.split(key)\n",
    "    batch_train_keys = random.split(train_key, len(states))\n",
    "    \n",
    "    _, new_log_probs, entropy = vmap(get_action_logprob, in_axes = (0, 0, None))(\n",
    "        states, \n",
    "        batch_train_keys, \n",
    "        policy_params)\n",
    "    \n",
    "    ratio = jnp.exp(new_log_probs - old_log_probs)\n",
    "    \n",
    "    term_1 = ratio * advantages \n",
    "    term_2 = jnp.clip(a = ratio, a_min= 1- CLIP_EPSILON, a_max = 1 + CLIP_EPSILON) * advantages\n",
    "    \n",
    "    jax.debug.print(\"term policy loss 1 {x}:\", x=term_1.shape)\n",
    "    jax.debug.print(\"term policy loss 2 {y}:\", y=term_2.shape)\n",
    "    jax.debug.print(\"policy loss before mean {z}:\", z=jnp.minimum(term_1, term_2).shape)\n",
    "    \n",
    "    \n",
    "    # negative loss for gradient ascent \n",
    "    loss = -jnp.mean(jnp.minimum(term_1, term_2))\n",
    "    \n",
    "    # TODO maybe stop gradient for all value stuff? \n",
    "    \n",
    "    return loss \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60879c28-1b0b-4c64-aa12-68c56326eb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_value_loss(value_params, minibatch: MiniBatch):\n",
    "    \n",
    "    states = minibatch.states \n",
    "    returns = minibatch.returns \n",
    "    key = minibatch.key\n",
    "    \n",
    "    key, train_key = random.split(key)\n",
    "    batch_train_key = random.split(train_key, len(states))\n",
    "    \n",
    "    new_values = vmap(get_value, in_axes = (0, None))(\n",
    "        states,\n",
    "        value_params, \n",
    "    )\n",
    "    \n",
    "    jax.debug.print(\"values value loss {x}:\", x=new_values.shape)\n",
    "    jax.debug.print(\"returns value loss {y}:\", y=returns.shape)\n",
    "    jax.debug.print(\"loss pre mean {y}:\", y=((returns - new_values)**2).shape)\n",
    "    \n",
    "    loss = jnp.mean((returns - new_values)**2)\n",
    "    \n",
    "    return loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f756e13b-6ed9-4fd6-a34c-43b9ea6728f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @jit\n",
    "# @chex.assert_max_traces(n=1)\n",
    "def update_policy(policy_params, policy_optimiser_state, minibatch):\n",
    "    grads = jax.grad(ppo_policy_loss, argnums=0)(policy_params, minibatch)\n",
    "    updates, new_pol_optimiser_state = policy_optimiser.update(grads, policy_optimiser_state)\n",
    "    new_policy_params = optax.apply_updates(policy_params, updates)\n",
    "    return new_policy_params, new_pol_optimiser_state\n",
    "\n",
    "\n",
    "# @jit\n",
    "# @chex.assert_max_traces(n=1)\n",
    "def update_value(value_params, value_optimiser_state, minibatch):\n",
    "    grads = jax.grad(ppo_value_loss, argnums=0)(value_params, minibatch)\n",
    "    updates, new_val_optimiser_state = value_optimiser.update(grads, value_optimiser_state)\n",
    "    new_value_params = optax.apply_updates(value_params, updates)\n",
    "    return new_value_params, new_val_optimiser_state\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05416a22-d2b7-4ec8-bde3-e6eaf9582443",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d91e18e2-900f-44fe-b430-e4266b9b066f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "term policy loss 1 (50,):\n",
      "term policy loss 2 (50,):\n",
      "policy loss before mean (50,):\n",
      "term policy loss 1 (50,):\n",
      "term policy loss 2 (50,):\n",
      "policy loss before mean (50,):\n",
      "values value loss (50,):\n",
      "returns value loss (50,):\n",
      "loss pre mean (50,):\n",
      "values value loss (50,):\n",
      "returns value loss (50,):\n",
      "loss pre mean (50,):\n",
      "term policy loss 1 (50,):\n",
      "term policy loss 2 (50,):\n",
      "policy loss before mean (50,):\n",
      "term policy loss 1 (50,):\n",
      "term policy loss 2 (50,):\n",
      "policy loss before mean (50,):\n",
      "values value loss (50,):\n",
      "returns value loss (50,):\n",
      "loss pre mean (50,):\n",
      "values value loss (50,):\n",
      "returns value loss (50,):\n",
      "loss pre mean (50,):\n",
      "term policy loss 1 (50,):\n",
      "term policy loss 2 (50,):\n",
      "policy loss before mean (50,):\n",
      "term policy loss 1 (50,):\n",
      "term policy loss 2 (50,):\n",
      "policy loss before mean (50,):\n",
      "values value loss (50,):\n",
      "returns value loss (50,):\n",
      "loss pre mean (50,):\n",
      "values value loss (50,):\n",
      "returns value loss (50,):\n",
      "loss pre mean (50,):\n",
      "term policy loss 1 (50,):\n",
      "term policy loss 2 (50,):\n",
      "policy loss before mean (50,):\n",
      "term policy loss 1 (50,):\n",
      "term policy loss 2 (50,):\n",
      "policy loss before mean (50,):\n",
      "values value loss (50,):\n",
      "returns value loss (50,):\n",
      "loss pre mean (50,):\n",
      "values value loss (50,):\n",
      "returns value loss (50,):\n",
      "loss pre mean (50,):\n",
      "term policy loss 1 (50,):\n",
      "term policy loss 2 (50,):\n",
      "policy loss before mean (50,):\n",
      "term policy loss 1 (50,):\n",
      "term policy loss 2 (50,):\n",
      "policy loss before mean (50,):\n",
      "values value loss (50,):\n",
      "returns value loss (50,):\n",
      "loss pre mean (50,):\n",
      "values value loss (50,):\n",
      "returns value loss (50,):\n",
      "loss pre mean (50,):\n",
      "term policy loss 1 (50,):\n",
      "term policy loss 2 (50,):\n",
      "policy loss before mean (50,):\n",
      "term policy loss 1 (50,):\n",
      "term policy loss 2 (50,):\n",
      "policy loss before mean (50,):\n",
      "values value loss (50,):\n",
      "returns value loss (50,):\n",
      "loss pre mean (50,):\n",
      "values value loss (50,):\n",
      "returns value loss (50,):\n",
      "loss pre mean (50,):\n",
      "term policy loss 1 (50,):\n",
      "term policy loss 2 (50,):\n",
      "policy loss before mean (50,):\n",
      "term policy loss 1 (50,):\n",
      "term policy loss 2 (50,):\n",
      "policy loss before mean (50,):\n",
      "values value loss (50,):\n",
      "returns value loss (50,):\n",
      "loss pre mean (50,):\n",
      "values value loss (50,):\n",
      "returns value loss (50,):\n",
      "loss pre mean (50,):\n",
      "term policy loss 1 (50,):\n",
      "term policy loss 2 (50,):\n",
      "policy loss before mean (50,):\n",
      "term policy loss 1 (50,):\n",
      "term policy loss 2 (50,):\n",
      "policy loss before mean (50,):\n",
      "values value loss (50,):\n",
      "returns value loss (50,):\n",
      "loss pre mean (50,):\n",
      "values value loss (50,):\n",
      "returns value loss (50,):\n",
      "loss pre mean (50,):\n",
      "Episode: 10 Average Return: 23.7\n",
      "Ave policy loss -8.955875 Ave value loss 98.71536\n",
      "term policy loss 1 (50,):\n",
      "term policy loss 2 (50,):\n",
      "policy loss before mean (50,):\n",
      "term policy loss 1 (50,):\n",
      "term policy loss 2 (50,):\n",
      "policy loss before mean (50,):\n",
      "values value loss (50,):\n",
      "returns value loss (50,):\n",
      "loss pre mean (50,):\n",
      "values value loss (50,):\n",
      "returns value loss (50,):\n",
      "loss pre mean (50,):\n",
      "term policy loss 1 (50,):\n",
      "term policy loss 2 (50,):\n",
      "policy loss before mean (50,):\n",
      "term policy loss 1 (50,):\n",
      "term policy loss 2 (50,):\n",
      "policy loss before mean (50,):\n",
      "values value loss (50,):\n",
      "returns value loss (50,):\n",
      "loss pre mean (50,):\n",
      "values value loss (50,):\n",
      "returns value loss (50,):\n",
      "loss pre mean (50,):\n",
      "term policy loss 1 (50,):\n",
      "term policy loss 2 (50,):\n",
      "policy loss before mean (50,):\n",
      "term policy loss 1 (50,):\n",
      "term policy loss 2 (50,):\n",
      "policy loss before mean (50,):\n",
      "values value loss (50,):\n",
      "returns value loss (50,):\n",
      "loss pre mean (50,):\n",
      "values value loss (50,):\n",
      "returns value loss (50,):\n",
      "loss pre mean (50,):\n",
      "term policy loss 1 (50,):\n",
      "term policy loss 2 (50,):\n",
      "policy loss before mean (50,):\n",
      "term policy loss 1 (50,):\n",
      "term policy loss 2 (50,):\n",
      "policy loss before mean (50,):\n",
      "values value loss (50,):\n",
      "returns value loss (50,):\n",
      "loss pre mean (50,):\n",
      "values value loss (50,):\n",
      "returns value loss (50,):\n",
      "loss pre mean (50,):\n",
      "term policy loss 1 (50,):\n",
      "term policy loss 2 (50,):\n",
      "policy loss before mean (50,):\n",
      "term policy loss 1 (50,):\n",
      "term policy loss 2 (50,):\n",
      "policy loss before mean (50,):\n",
      "values value loss (50,):\n",
      "returns value loss (50,):\n",
      "loss pre mean (50,):\n",
      "values value loss (50,):\n",
      "returns value loss (50,):\n",
      "loss pre mean (50,):\n",
      "term policy loss 1 (50,):\n",
      "term policy loss 2 (50,):\n",
      "policy loss before mean (50,):\n",
      "term policy loss 1 (50,):\n",
      "term policy loss 2 (50,):\n",
      "policy loss before mean (50,):\n",
      "values value loss (50,):\n",
      "returns value loss (50,):\n",
      "loss pre mean (50,):\n",
      "values value loss (50,):\n",
      "returns value loss (50,):\n",
      "loss pre mean (50,):\n",
      "term policy loss 1 (50,):\n",
      "term policy loss 2 (50,):\n",
      "policy loss before mean (50,):\n",
      "term policy loss 1 (50,):\n",
      "term policy loss 2 (50,):\n",
      "policy loss before mean (50,):\n",
      "values value loss (50,):\n",
      "returns value loss (50,):\n",
      "loss pre mean (50,):\n",
      "values value loss (50,):\n",
      "returns value loss (50,):\n",
      "loss pre mean (50,):\n",
      "term policy loss 1 (50,):\n",
      "term policy loss 2 (50,):\n",
      "policy loss before mean (50,):\n",
      "term policy loss 1 (50,):\n",
      "term policy loss 2 (50,):\n",
      "policy loss before mean (50,):\n",
      "values value loss (50,):\n",
      "returns value loss (50,):\n",
      "loss pre mean (50,):\n",
      "values value loss (50,):\n",
      "returns value loss (50,):\n",
      "loss pre mean (50,):\n",
      "Episode: 20 Average Return: 21.15\n",
      "Ave policy loss -6.841623 Ave value loss 60.138145\n",
      "term policy loss 1 (50,):\n",
      "term policy loss 2 (50,):\n",
      "policy loss before mean (50,):\n",
      "term policy loss 1 (50,):\n",
      "term policy loss 2 (50,):\n",
      "policy loss before mean (50,):\n",
      "values value loss (50,):\n",
      "returns value loss (50,):\n",
      "loss pre mean (50,):\n",
      "values value loss (50,):\n",
      "returns value loss (50,):\n",
      "loss pre mean (50,):\n",
      "term policy loss 1 (50,):\n",
      "term policy loss 2 (50,):\n",
      "policy loss before mean (50,):\n",
      "term policy loss 1 (50,):\n",
      "term policy loss 2 (50,):\n",
      "policy loss before mean (50,):\n",
      "values value loss (50,):\n",
      "returns value loss (50,):\n",
      "loss pre mean (50,):\n",
      "values value loss (50,):\n",
      "returns value loss (50,):\n",
      "loss pre mean (50,):\n",
      "term policy loss 1 (50,):\n",
      "term policy loss 2 (50,):\n",
      "policy loss before mean (50,):\n",
      "term policy loss 1 (50,):\n",
      "term policy loss 2 (50,):\n",
      "policy loss before mean (50,):\n",
      "values value loss (50,):\n",
      "returns value loss (50,):\n",
      "loss pre mean (50,):\n",
      "values value loss (50,):\n",
      "returns value loss (50,):\n",
      "loss pre mean (50,):\n",
      "term policy loss 1 (50,):\n",
      "term policy loss 2 (50,):\n",
      "policy loss before mean (50,):\n",
      "term policy loss 1 (50,):\n",
      "term policy loss 2 (50,):\n",
      "policy loss before mean (50,):\n",
      "values value loss (50,):\n",
      "returns value loss (50,):\n",
      "loss pre mean (50,):\n",
      "values value loss (50,):\n",
      "returns value loss (50,):\n",
      "loss pre mean (50,):\n",
      "term policy loss 1 (50,):\n",
      "term policy loss 2 (50,):\n",
      "policy loss before mean (50,):\n",
      "term policy loss 1 (50,):\n",
      "term policy loss 2 (50,):\n",
      "policy loss before mean (50,):\n",
      "values value loss (50,):\n",
      "returns value loss (50,):\n",
      "loss pre mean (50,):\n",
      "values value loss (50,):\n",
      "returns value loss (50,):\n",
      "loss pre mean (50,):\n",
      "term policy loss 1 (50,):\n",
      "term policy loss 2 (50,):\n",
      "policy loss before mean (50,):\n",
      "term policy loss 1 (50,):\n",
      "term policy loss 2 (50,):\n",
      "policy loss before mean (50,):\n",
      "values value loss (50,):\n",
      "returns value loss (50,):\n",
      "loss pre mean (50,):\n",
      "values value loss (50,):\n",
      "returns value loss (50,):\n",
      "loss pre mean (50,):\n",
      "term policy loss 1 (50,):\n",
      "term policy loss 2 (50,):\n",
      "policy loss before mean (50,):\n",
      "term policy loss 1 (50,):\n",
      "term policy loss 2 (50,):\n",
      "policy loss before mean (50,):\n",
      "values value loss (50,):\n",
      "returns value loss (50,):\n",
      "loss pre mean (50,):\n",
      "values value loss (50,):\n",
      "returns value loss (50,):\n",
      "loss pre mean (50,):\n",
      "term policy loss 1 (50,):\n",
      "term policy loss 2 (50,):\n",
      "policy loss before mean (50,):\n",
      "term policy loss 1 (50,):\n",
      "term policy loss 2 (50,):\n",
      "policy loss before mean (50,):\n",
      "values value loss (50,):\n",
      "returns value loss (50,):\n",
      "loss pre mean (50,):\n",
      "values value loss (50,):\n",
      "returns value loss (50,):\n",
      "loss pre mean (50,):\n",
      "Episode: 30 Average Return: 22.766666\n",
      "Ave policy loss -6.92479 Ave value loss 61.14006\n",
      "term policy loss 1 (50,):\n",
      "term policy loss 2 (50,):\n",
      "policy loss before mean (50,):\n",
      "term policy loss 1 (50,):\n",
      "term policy loss 2 (50,):\n",
      "policy loss before mean (50,):\n",
      "values value loss (50,):\n",
      "returns value loss (50,):\n",
      "loss pre mean (50,):\n",
      "values value loss (50,):\n",
      "returns value loss (50,):\n",
      "loss pre mean (50,):\n",
      "term policy loss 1 (50,):\n",
      "term policy loss 2 (50,):\n",
      "policy loss before mean (50,):\n",
      "term policy loss 1 (50,):\n",
      "term policy loss 2 (50,):\n",
      "policy loss before mean (50,):\n",
      "values value loss (50,):\n",
      "returns value loss (50,):\n",
      "loss pre mean (50,):\n",
      "values value loss (50,):\n",
      "returns value loss (50,):\n",
      "loss pre mean (50,):\n",
      "term policy loss 1 (50,):\n",
      "term policy loss 2 (50,):\n",
      "policy loss before mean (50,):\n",
      "term policy loss 1 (50,):\n",
      "term policy loss 2 (50,):\n",
      "policy loss before mean (50,):\n",
      "values value loss (50,):\n",
      "returns value loss (50,):\n",
      "loss pre mean (50,):\n",
      "values value loss (50,):\n",
      "returns value loss (50,):\n",
      "loss pre mean (50,):\n",
      "term policy loss 1 (50,):\n",
      "term policy loss 2 (50,):\n",
      "policy loss before mean (50,):\n",
      "term policy loss 1 (50,):\n",
      "term policy loss 2 (50,):\n",
      "policy loss before mean (50,):\n",
      "values value loss (50,):\n",
      "returns value loss (50,):\n",
      "loss pre mean (50,):\n",
      "values value loss (50,):\n",
      "returns value loss (50,):\n",
      "loss pre mean (50,):\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m key, action_key \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39msplit(buffer_state\u001b[38;5;241m.\u001b[39mkey)\n\u001b[1;32m     16\u001b[0m buffer_state\u001b[38;5;241m.\u001b[39mkey \u001b[38;5;241m=\u001b[39m key\n\u001b[0;32m---> 17\u001b[0m action, log_prob, entropy \u001b[38;5;241m=\u001b[39m \u001b[43mget_action_logprob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m value \u001b[38;5;241m=\u001b[39m get_value(obs, value_params)\n\u001b[1;32m     20\u001b[0m obs_, reward, done, _  \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action\u001b[38;5;241m.\u001b[39mtolist())\n",
      "Cell \u001b[0;32mIn[12], line 12\u001b[0m, in \u001b[0;36mget_action_logprob\u001b[0;34m(observation, action_key, policy_params)\u001b[0m\n\u001b[1;32m      9\u001b[0m logits \u001b[38;5;241m=\u001b[39m policy_network\u001b[38;5;241m.\u001b[39mapply(policy_params, observation)\n\u001b[1;32m     10\u001b[0m distribution \u001b[38;5;241m=\u001b[39m distrax\u001b[38;5;241m.\u001b[39mCategorical(logits\u001b[38;5;241m=\u001b[39mlogits)\n\u001b[0;32m---> 12\u001b[0m action, logprob \u001b[38;5;241m=\u001b[39m \u001b[43mdistribution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_and_log_prob\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43maction_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m entropy \u001b[38;5;241m=\u001b[39m distribution\u001b[38;5;241m.\u001b[39mentropy()\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m jnp\u001b[38;5;241m.\u001b[39msqueeze(action), jnp\u001b[38;5;241m.\u001b[39msqueeze(logprob), jnp\u001b[38;5;241m.\u001b[39msqueeze(entropy)\n",
      "File \u001b[0;32m~/miniconda3/envs/centralised-agents/lib/python3.8/site-packages/distrax/_src/distributions/distribution.py:145\u001b[0m, in \u001b[0;36mDistribution.sample_and_log_prob\u001b[0;34m(self, seed, sample_shape)\u001b[0m\n\u001b[1;32m    142\u001b[0m rng, sample_shape \u001b[38;5;241m=\u001b[39m convert_seed_and_sample_shape(seed, sample_shape)\n\u001b[1;32m    143\u001b[0m num_samples \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mreduce(operator\u001b[38;5;241m.\u001b[39mmul, sample_shape, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# product\u001b[39;00m\n\u001b[0;32m--> 145\u001b[0m samples, log_prob \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample_n_and_log_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrng\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m samples \u001b[38;5;241m=\u001b[39m samples\u001b[38;5;241m.\u001b[39mreshape(sample_shape \u001b[38;5;241m+\u001b[39m samples\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:])\n\u001b[1;32m    148\u001b[0m log_prob \u001b[38;5;241m=\u001b[39m log_prob\u001b[38;5;241m.\u001b[39mreshape(sample_shape \u001b[38;5;241m+\u001b[39m log_prob\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:])\n",
      "File \u001b[0;32m~/miniconda3/envs/centralised-agents/lib/python3.8/site-packages/distrax/_src/distributions/distribution.py:64\u001b[0m, in \u001b[0;36mDistribution._sample_n_and_log_prob\u001b[0;34m(self, key, n)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sample_n_and_log_prob\u001b[39m(\u001b[38;5;28mself\u001b[39m, key: PRNGKey, n: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Array, Array]:\n\u001b[1;32m     49\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Returns `n` samples and their log probs.\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \n\u001b[1;32m     51\u001b[0m \u001b[38;5;124;03m  By default, it just calls `log_prob` on the generated samples. However, for\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;124;03m    A tuple of `n` samples and their log probs.\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m   samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample_n\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m   log_prob \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_prob(samples)\n\u001b[1;32m     66\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m samples, log_prob\n",
      "File \u001b[0;32m~/miniconda3/envs/centralised-agents/lib/python3.8/site-packages/distrax/_src/distributions/categorical.py:94\u001b[0m, in \u001b[0;36mCategorical._sample_n\u001b[0;34m(self, key, n)\u001b[0m\n\u001b[1;32m     91\u001b[0m new_shape \u001b[38;5;241m=\u001b[39m (n,) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogits\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     92\u001b[0m is_valid \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mlogical_and(jnp\u001b[38;5;241m.\u001b[39mall(jnp\u001b[38;5;241m.\u001b[39misfinite(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprobs), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m     93\u001b[0m                            jnp\u001b[38;5;241m.\u001b[39mall(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprobs \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m---> 94\u001b[0m draws \u001b[38;5;241m=\u001b[39m \u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcategorical\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_shape\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dtype)\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m jnp\u001b[38;5;241m.\u001b[39mwhere(is_valid, draws, jnp\u001b[38;5;241m.\u001b[39mones_like(draws) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/centralised-agents/lib/python3.8/site-packages/jax/_src/random.py:1324\u001b[0m, in \u001b[0;36mcategorical\u001b[0;34m(key, logits, axis, shape)\u001b[0m\n\u001b[1;32m   1322\u001b[0m key, _ \u001b[38;5;241m=\u001b[39m _check_prng_key(key)\n\u001b[1;32m   1323\u001b[0m _check_arraylike(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategorical\u001b[39m\u001b[38;5;124m\"\u001b[39m, logits)\n\u001b[0;32m-> 1324\u001b[0m logits_arr \u001b[38;5;241m=\u001b[39m \u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1327\u001b[0m   axis \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(logits_arr\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/miniconda3/envs/centralised-agents/lib/python3.8/site-packages/jax/_src/numpy/lax_numpy.py:2036\u001b[0m, in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m   2034\u001b[0m lax_internal\u001b[38;5;241m.\u001b[39m_check_user_dtype_supported(dtype, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masarray\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2035\u001b[0m dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mcanonicalize_dtype(dtype) \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m dtype\n\u001b[0;32m-> 2036\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/centralised-agents/lib/python3.8/site-packages/jax/_src/numpy/lax_numpy.py:2017\u001b[0m, in \u001b[0;36marray\u001b[0;34m(object, dtype, copy, order, ndmin)\u001b[0m\n\u001b[1;32m   2013\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m array(np\u001b[38;5;241m.\u001b[39masarray(view), dtype, copy, ndmin\u001b[38;5;241m=\u001b[39mndmin)\n\u001b[1;32m   2015\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected input type for array: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2017\u001b[0m out_array: Array \u001b[38;5;241m=\u001b[39m \u001b[43mlax_internal\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_element_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweak_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweak_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2018\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ndmin \u001b[38;5;241m>\u001b[39m ndim(out_array):\n\u001b[1;32m   2019\u001b[0m   out_array \u001b[38;5;241m=\u001b[39m lax\u001b[38;5;241m.\u001b[39mexpand_dims(out_array, \u001b[38;5;28mrange\u001b[39m(ndmin \u001b[38;5;241m-\u001b[39m ndim(out_array)))\n",
      "File \u001b[0;32m~/miniconda3/envs/centralised-agents/lib/python3.8/site-packages/jax/_src/lax/lax.py:569\u001b[0m, in \u001b[0;36m_convert_element_type\u001b[0;34m(operand, new_dtype, weak_type)\u001b[0m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    568\u001b[0m   new_dtype \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdtype(new_dtype)\n\u001b[0;32m--> 569\u001b[0m new_dtype \u001b[38;5;241m=\u001b[39m \u001b[43mdtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcanonicalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (dtypes\u001b[38;5;241m.\u001b[39missubdtype(old_dtype, np\u001b[38;5;241m.\u001b[39mcomplexfloating) \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m dtypes\u001b[38;5;241m.\u001b[39missubdtype(new_dtype, np\u001b[38;5;241m.\u001b[39mcomplexfloating)):\n\u001b[1;32m    573\u001b[0m   msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCasting complex values to real discards the imaginary part\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/centralised-agents/lib/python3.8/site-packages/jax/_src/dtypes.py:459\u001b[0m, in \u001b[0;36mdtype\u001b[0;34m(x, canonicalize)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(x) \u001b[38;5;129;01min\u001b[39;00m python_scalar_dtypes:\n\u001b[1;32m    458\u001b[0m   dt \u001b[38;5;241m=\u001b[39m python_scalar_dtypes[\u001b[38;5;28mtype\u001b[39m(x)]\n\u001b[0;32m--> 459\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_opaque_dtype\u001b[49m(\u001b[38;5;28mgetattr\u001b[39m(x, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)):\n\u001b[1;32m    460\u001b[0m   dt \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training loop \n",
    "\n",
    "episode_returns = []\n",
    "policy_losses = []\n",
    "value_losses = []\n",
    "\n",
    "for episode in range(1, 10000):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    episode_return = 0\n",
    "    \n",
    "    while not done: \n",
    "        # select action\n",
    "        \n",
    "        key, action_key = random.split(buffer_state.key)\n",
    "        buffer_state.key = key\n",
    "        action, log_prob, entropy = get_action_logprob(obs, action_key, policy_params)\n",
    "        value = get_value(obs, value_params)\n",
    "        \n",
    "        obs_, reward, done, _  = env.step(action.tolist())\n",
    "        \n",
    "        \n",
    "        buffer_state = jit_add(buffer_state=buffer_state, \n",
    "                                state=obs, \n",
    "                                action=action, \n",
    "                                reward=reward, \n",
    "                                done=done, \n",
    "                                log_prob=log_prob,\n",
    "                                value=value, \n",
    "                                ) \n",
    "        \n",
    "        episode_return += reward \n",
    "        obs = obs_\n",
    "        \n",
    "        # whether should train of not \n",
    "        # followed by training logic \n",
    "        if buffer.should_train(buffer_state): \n",
    "            # Compute advantages \n",
    "            buffer_state = buffer.compute_advantages(buffer_state)\n",
    "            policy_losses = []\n",
    "            value_losses = []\n",
    "            \n",
    "            for epoch in range(NUM_EPOCHS):\n",
    "                buffer_state, minibatch_idxs = buffer.get_epoch_indices(buffer_state)\n",
    "                \n",
    "                for minibatch in minibatch_idxs: \n",
    "                    \n",
    "                    # TODO only get what is really needed.\n",
    "                    train_minibatch = MiniBatch(\n",
    "                        states=buffer_state.states[minibatch],\n",
    "                        actions=buffer_state.actions[minibatch],\n",
    "                        rewards=buffer_state.rewards[minibatch],  \n",
    "                        dones=buffer_state.dones[minibatch],\n",
    "                        log_probs=buffer_state.log_probs[minibatch],\n",
    "                        values=buffer_state.values[minibatch],\n",
    "                        advantages=buffer_state.advantages[minibatch],\n",
    "                        returns=buffer_state.returns[minibatch],\n",
    "                        key=buffer_state.key, \n",
    "                    )\n",
    "                    \n",
    "                    policy_params, policy_optimiser_state = update_policy(\n",
    "                        policy_params, \n",
    "                        policy_optimiser_state, \n",
    "                        train_minibatch,\n",
    "                    )\n",
    "                    policy_loss = ppo_policy_loss(policy_params=policy_params, minibatch=train_minibatch)\n",
    "                    policy_losses.append(policy_loss)\n",
    "                    \n",
    "                    value_params, value_optimiser_state = update_value(\n",
    "                        value_params, \n",
    "                        value_optimiser_state, \n",
    "                        train_minibatch,\n",
    "                    )\n",
    "                    value_loss = ppo_value_loss(value_params=value_params, minibatch=train_minibatch)\n",
    "                    value_losses.append(value_loss)\n",
    "                    \n",
    "                    key, new_key = random.split(train_minibatch.key)\n",
    "                    buffer_state.key = new_key\n",
    "                    \n",
    "            buffer_state = buffer.reset(buffer_state)\n",
    "        \n",
    "        \n",
    "        \n",
    "    episode_returns.append(episode_return)\n",
    "    \n",
    "    if episode % 10 == 0:\n",
    "        print(\"Episode:\", episode, \"Average Return:\", np.mean(episode_returns[-100:]))\n",
    "        print(\"Ave policy loss\", np.mean(policy_losses), \"Ave value loss\", np.mean(value_losses))\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "centralised-agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "323e2cf05b266ccb7652569eb368379b6b4c8dc9189d03e9f7b7cf8cf7a73966"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
